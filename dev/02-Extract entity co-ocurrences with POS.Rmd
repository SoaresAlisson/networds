---
title: "02-Extract entity co-ocurrences with POS"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{entities_and_relation_extraction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
Loading the library

```{r setup library}
library(txtnet)
library(spacyr)
# library(igraph) # library/4.5/igraph/libs/igraph.so': libxml2.so.2: não é possível abrir arquivo compartilhado: Arquivo ou diretório inexistente
```

Extracting proper names or entities with regex are cool, but has its limitations. 
In this section, we will use the Part of Speech (POS) tags to extract the entities, proper names, noun phrases and its co-occurrences to generate graphs.

## Installing SpacyR
First of all, we need to install the {[spacyR](https://spacyr.quanteda.io/)} package.
The SpacyR is a wrap around the Spacy package in Python, and SpacyR deals with the boring parts of creating an exclusive python virtual environment.
This package will extract the NER (named entities) and POS (part of speech tagging).

```{r install spacyR, eval = FALSE}
install.packages("spacyR")
# if you prefer, or if the CRAN version is buggy, install the github version:
pak::pkg_install("quanteda/spacyr")

# to Install spaCy and requirements (python env). With empty parameters, it will
# install the default “en_core_web_sm” model.
spacyr::spacy_install()
```

To install and use other languages than English, see below "Using another languages"

## Using txtnet package

```{r text, eval=T, echo=T}
# text from https://www.bbc.com/news/articles/c7ve36zg0e5o
text <- r"(The manhunt for a suspect who gunned down a healthcare chief executive in New York is now in its third day, with police chasing several different leads.

UnitedHealthcare boss Brian Thompson, 50, was fatally shot in the back on Wednesday morning outside the Hilton hotel in Midtown Manhattan.

Police say Thompson was targeted in a pre-planned killing, for which they do not yet have a motive.

Investigators are using surveillance photos, bullet casings with cryptic messages written on them, and the suspect's movements to track him down. They are also working with the FBI and authorities in other states as the search expands beyond New York.)"
```
```{r POS}
POS <- text |>
  spacyr::spacy_parse(dependency = T)

head(POS, 20)
```


## Extracting entities

The package {spacyr} has two functions useful in this section.
Both of them conflate compound nouns, like "New" and "York" into "New_York".
The first one, extract only the entities

```{r spacyr::entity_extract}
POS |> spacyr::entity_extract()
```

The second one, conflates the compound nouns and preserve the other POS tags.

```{r spacyr::entity_consolidate}
POS |> spacyr::entity_consolidate()
```

This functions is used inside {txtnet}.
Lets use the package text example. You can use whatever you want.

```{r sample_txt head}
data(package = "txtnet")

# text_sample
# an example of text. this comes with the package.
txt_wiki[2:3]
```

With the function in {txtnet}, is possible to give the whole text as input, search for a term/query. The package will tokenize in sentences (or in paragraphs, if specified in parameters of the function), perform the POS tagging. Is possible to run the whole process or go step by step, to understand what is going on. 

```{r}
# tokenizing by sentence and filtering those that contains "police"
txt_wiki[2:6] |> filter_by_query("Police")
```

Is possible to return a vector instead of a list object

```{r}
txt_wiki[1:12] |> filter_by_query("Police", unlist = TRUE)
```

The next step is the POS tagging using `parsePOS()`

```{r}
txt_wiki[1:12] |>
  filter_by_query("Police", unlist = TRUE) |>
  parsePOS()
```



```{r wiki get_pairs, eval=FALSE, echo=FALSE}
txt_wiki |>
  filter_by_query("Police") |>
  parsePOS() |>
  get_pairs()

txt_wiki |>
  filter_by_query("Police") |>
  parsePOS() |>
  get_pairs(loop = TRUE)
```

```{r, eval=FALSE, echo=FALSE}
graph <- text |>
  filter_by_query(query = "Brian") |>
  parsePOS() |>
  get_pairs()

options(browser = "firefox")
graph |> net_wordcloud(text, df = _)
graph |>
  dplyr::rename(from = n1, to = n2) |>
  viz_graph()
```
```{r, eval=FALSE, echo=FALSE}
graph <- sotu_text[237:240] |>
  filter_by_query(query = "China") |>
  parsePOS() |>
  get_pairs()

graph |>
  dplyr::rename(from = n1, to = n2) |>
  viz_graph()
```
```{r, eval=FALSE, echo=FALSE}
sotu_text[237:240] |> get_graph_from_txt("people")
sotu_text[237:240] |> get_graph_from_txt("China", by_sentence = FALSE)
sotu_text[237:240] |> get_graph_from_txt("Russia", by_sentence = FALSE)
```

To get the graph of entities co-occurrences:

```{r entities cooc}
graph <- filter_by_query(txt_wiki, "Police") |>
  parsePOS(only_entities = TRUE) |>
  dplyr::filter(entity_type != "CARDINAL") |> # to clean the graph
  get_cooc_entities()

graph
```

```{r, echo=FALSE, eval=FALSE}
net_wordcloud(txt_wiki, graph)
```

```{r}
graph$graphs |>
  dplyr::rename(from = n1, to = n2) |>
  net_wordcloud(txt_wiki, df=_)
```

To get the graph of entities and nouns:

```{r}
graph <- filter_by_query(txt_wiki, "Police") |>
  parsePOS(only_entities = FALSE) |>
  dplyr::filter(entity_type != "CARDINAL") |> # to clean the graph
  get_pairs2()

graph
```

```{r}
viz_graph(graph)
```



## Sotu example

Using data from [{SOTU} package](https://cran.r-project.org/web/packages/sotu/index.html),  United States Presidential State of the Union Addresses.

```{r, eval=FALSE, echo=FALSE}
library(sotu)
# looking at availabe datasets in the package
data(package = "sotu")
sotu_text[237:240] |> filter_by_query("China")
filter_by_query(sotu_text[237:240], "\\bXi\\b", unlist = TRUE)
filter_by_query(sotu_text[237:240], "bob") # test

# Trump sentences from SOTU talking about "Xi"
filter_by_query(sotu_text[237:240], "\\Xi\\b") |> parsePOS()
# Trump sentences from SOTU talking about "China"
filter_by_query(sotu_text[237:240], "China")[2] |> parsePOS()
# Trump sentences from SOTU talking about "China"
filter_by_query(sotu_text[237:240], "China") |> parsePOS()
filter_by_query(sotu_text[237:240], "China") |> parsePOS(entities = FALSE)

filter_by_query(sotu_text[237:240], "China") |>
  parsePOS() |>
  get_pairs()

filter_by_query(sotu_text[237:240], "China") |>
  parsePOS() |>
  get_pairs(loop = TRUE)
```

## Using another languages

How {txtnet} uses spacy to tag the words, the user must initialize the model for the language.

To download other language model, check models available at [spacy.io/usage/models](https://spacy.io/usage/models)
Ex.: installing Portuguese, the models available are:

```{r install spacy models PT, eval = FALSE}
modelsPT <- c("pt_core_news_sm", "pt_core_news_md", "pt_core_news_lg")

# installing the bigger model
spacyr::spacy_download_langmodel(modelsPT[3])
```

```{r, eval=FALSE, echo=FALSE}
# if you initialized spacyR with another model, first finalize it
sapcyr::spacy_finalize()

# load the model of another language
spacy_initialize(model = "pt_core_news_lg")

"Maria Jana ama John Smith e Maria é amada por Joaquim de Souza" |>
  spacyr::spacy_parse(dependency = T) |>
  group_ppn()
```
