# Dev de extract relations

## LL, devtools
```{r ll}
sto::ll("dplyr spacyr udpipe sto rsyntax tibble tidygraph ggraph")
options(browser = "firefox")
detach("package:txtnet", unload = T)
# library("txtnet")
```
```{r available}
pak::pkg_install("available")
available::available("textnetwork")
available::available("nettext")
available::available("txtnet")
```
```{r devtools vignette}
list.files("vignettes")
devtools::build_vignettes("vignettes")
devtools::build_vignettes("vignettes/entities_and_relation_extraction.Rmd")
# devtools::build_vignettes("First_steps")
# devtools::build_vignettes("entities_and_relation_extraction.Rmd")
# devtools::build_vignettes("01-Extract entity coocurrences with regex")
# devtools::build_vignettes("02-Extract entity co-ocurrences with POS")
devtools::build_vignettes()
#devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
vignette(pkgdown::metadata)
usethis::use_readme_rmd()
usethis::use_github()
usethis::use_github_links() #  If a package is already connected to a remote GitHub repository, usethis::use_github_links() can be called to just add the relevant links to DESCRIPTION.

usethis::use_data_table() # creates all the boilerplate. https://usethis.r-lib.org/reference/use_data_table.html
devtools::build_readme() # locates your README.Rmd and builds it into a README.md
devtools::build_manual() # Create package pdf manual
usethis::use_pkgdown() # is a function you run once and it does the initial, minimal setup necessary to start using pkgdown:
pkgdown::build_site_github_pages()
pkgdown::build_site() # is a function you’ll call repeatedly, to re-render your site locally. In an extremely barebones package, https://r-pkgs.org/website.html
devtools::build_site() # Execute pkgdown build_site in a package
# build_site() is a shortcut for pkgdown::build_site()

# to create a data object:
usethis::use_data()
```
## txt
```{r txt}
txt <- c(
  "Maria Jana ama John Smith e Maria é amada por Joaquim de Souza.",
  "Os Estados Unidos da América emitiram um novo alerta para a Justiça do Trabalho do Estado do Mato Grosso do Sul, a título de indenização.",
  "O iogurte acabou",
  "O rato Roberto Ratatouille roeu a roupa do rei de Roma.",
  "Maria de Jesus ama John Smith, Cláudio é amado por Amanda, mas Cláudio não ama ninguém.",
  "O novo plano foi anunciado esta semana por Biden. A fala do presidente recebeu críticas."
)
# txtPT_trt_full <- readLines("../narrative_argument/texto_trt.txt")
# txtPT_trt_full <- readLines("../../narrative_argument/data/texto_trt.txt")
txtPT_trt_full <- readLines("../narrative_argument/data/texto_trt.txt")
txtPT_trt_full <- "../narrative_argument/data/txts.R" |> source()
```
```{r package data txt}
library(rvest)
URL <- "https://en.wikipedia.org/wiki/Killing_of_Brian_Thompson"

page <- rvest::read_html(URL)

txt_wiki <- page |>
  rvest::html_elements("p") |> 
  html_text() |> 
  gsub("\\n+", " ", x=_)

title <- page |> html_elements("title") |> html_text() 

txt_wiki <- c(title, txt_wiki, paste0("Font: ", URL))

# attr(txt_wiki, "Font") <- URL

# use the data in package
# https://r-pkgs.org/data.html
# txt_wiki <- "./data/txt_wiki.rda"
# save(txt, file=txt_wiki)
# usethis::use_data_raw("txt_wiki")
usethis::use_data(txt_wiki, overwrite = T)
```
```{r read wikipedia}
# list.files("data", full.names = T, pattern = "txt_wiki") |> load()
txt_wiki |> filter_by_query("Brian", unlist=T)

```

# UD pipe
```{r Udpipe}
UDmodels <- list.files("~/Documentos/Programação/R/UDpipe/", pattern="udpipe$", full.names=TRUE)
tokens =udpipe(txt$pt[2], UDmodels[2]) |> as_tokenindex()
tokens = udpipe(txt$en[1], UDmodels[1]) |> as_tokenindex()

txt[1]  |> 
```
# Spacy 
```{r spacy init}
spacy_finalize()
spacyr::spacy_initialize(model = "pt_core_news_lg")
```
```{r spacy pos}
pos <- txt[1] |> spacyr::spacy_parse(, dependency = T)
pos
select(pos, token, pos, dep_rel, entity)
```
```{r fun conflate_ppn conflate_adp extract_nouns_ppnN}
conflate_ppn <- function(pos) {
  # "doc_id"        "sentence_id"   "token_id"      "token"         "lemma"         "pos"           "head_token_id" "dep_rel"       "entity"
  # naLine <-rep(NA, names(pos) |> length())
  # pos |>
  # pos <- txt[1] |> spacyr::spacy_parse(dependency = T)
  rbind(NA, pos) |>
    rbind(NA) |>
    select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
    mutate(
      nome = ifelse(pos == lead(pos),
        # paste(token, lead(token)),
        paste(token2, lead(token2)),
        token
      ),
      nome = ifelse(pos == lag(pos), NA, nome)
    ) |>
    filter(!(is.na(nome) & grepl("^PER_", entity))) |>
    mutate(nome = ifelse(is.na(nome), token, nome)) |>
    filter(!(is.na(nome) & is.na(token))) |>
    nothing()
}

txt[1] |>
  spacyr::spacy_parse(dependency = T) |>
  conflate_ppn()

# collapse ADP
# Collapse upos ADP, if the previous and the next pos are NOUN or PROPN. It is used in the pipeline to extract proper names like "United States of America"
collapse_adp <- function(pos_df) {
  pos_df |>
    # se adp é rodeado por ppn e noun = TRUE
    dplyr::mutate(c_adp = 
      (pos == "ADP" & 
      lag(pos) %in% s2v("NOUN PROPN") &
      lead(pos) %in% s2v("NOUN PROPN")
    ),
    token2 = ifelse(lead(c_adp), 
        paste(lemma, lead(token), collpase = " ") |> trimws(), 
        lemma),
    token2 = ifelse(is.na(token2), token, token2 ) ) |>
    dplyr::rename(token0 = token, token = token2) |> 
    dplyr::filter(!c_adp) |>
    dplyr::select(-c_adp)
    # select(token, pos, entity, c_adp, lemma, lemma2)
}

#'' extract_entity for each sentence
#' @return a list of all entities in each sentence
extract_entities2 <- function(pos_df) {
  sentence <- unique(pos_df$sentence_id)

  lapply(sentence, \(x) {
    pos_df |> dplyr::filter(sentence_id == x) |> 
      extract_entities()
         })
    
}


txt[2] |> spacyr::spacy_parse(dependency = T) |> extract_entities()

txt[1:3] |> paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T) |>
  group_by(sentence_id) |>
  extract_entities()

txt[1:3] |> paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T) |>
  extract_entities2()
```
```{r fun entity_list_2_graph }
entity_list_2_graph <- function(entities_list) {
    # entidades <- txt[1:3] |> paste(collapse = " ") |>
    #   spacyr::spacy_parse(dependency = T) |>
    #   extract_entities2()

    comb <- entities_list |>
      combn( 2, simplify = F) |>
      unlist()

    tibble::tibble(
      n1 = comb[seq(1, length(comb), by = 2)],
      n2 = comb[seq(2, length(comb), by = 2)]
    )

}

txt[1] |> paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T) |>
  extract_entities2() |> unlist() |>
  entity_list_2_graph()


#' from list of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs
entity_list_2_graph2 <- function(entities_list, count = TRUE) {
# entities_list <- txt[1:3] |> paste(collapse = " ") |>
#   spacyr::spacy_parse(dependency = T) |>
#   extract_entities2() 

  more_than1_element_in_list <- lapply(entities_list, length)  |> unlist() > 1
  entities_list <- entities_list[more_than1_element_in_list]

  tib <- lapply(entities_list, entity_list_2_graph) |>
    dplyr::bind_rows()

  if (count) {
    tib |> dplyr::count(n1, n2, sort = TRUE)
  } else {
    tib
  }
}

# teste, chamando a função
txt[1:3] |> paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T) |>
  extract_entities2() |> 
  entity_list_2_graph2()

txtPT_trt_full  |>
  spacyr::spacy_parse(dependency = T) |>
  extract_entities2() |> head(6) |>
  entity_list_2_graph2()
```

```{r pos group_by}
# NAO DEU CERTO
pos |> select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  # mutate(entity2 = gsub("(PER)(_.*)", "\\1", entity) ) |>
  # group_by(entity2) |>
  # summarise(token2 = paste(token, collapse = " "), .groups = 'drop')
  nothing()
# summarise(token2 = ifelse(entity2 == lead(entity2), paste(token, collapse = " ", token)))
```
```{r pos}
pos2 <- txt[1] |> spacyr::spacy_parse(dependency = T)

rbind(NA, pos2) |>
  rbind(NA) |>
  select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  mutate(entity2 = gsub("(PER)(_.*)", "\\1", entity)) |>
  mutate(
    nome = ifelse(entity2 == lead(entity2),
      paste(token, lead(token)),
      token
    )
    # nome = ifelse(pos == lag(pos), NA, nome )
  ) |>
  # filter(! (is.na(nome) & grepl("^PER_",entity))  )|>
  nothing()
```
```{r while}
while (nrow(filter(df, grepl("^##", word))) > 0) {
  df <- df |>
    mutate(
      word2 =
        case_when(
          !grepl("^##", word) & !grepl("^##", lead(word)) ~ word,
          grepl("^##", word) & !grepl("^##", lag(word)) ~ paste0(
            lag(word),
            substr(word, 3, length(word))
          ),
          grepl("^##", word) & grepl("^##", lag(word)) ~ word
        )
    ) %>%
    filter(!is.na(word2)) %>%
    mutate(word = word2, word2 = NULL)
}
```
```{r rbind mutate}
rbind(NA, pos) |>
  rbind(NA) |>
  select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  mutate(
    entity2 = gsub("(PER)(_.*)", "\\1", entity),
    # nome=(pos == lead(pos)
    # nome = ifelse(entity2 == lead(entity2), paste(token, lead(token)), token),
    # nome = ifelse(pos == lag(pos), NA, nome)
  ) |>
  # rowwise() |>
  group_by(entity2) %>%
  mutate(collapsed_values = paste(token, lead(token))) %>%
  # mutate(collapsed_values = paste(token, lead(token), collapse = " ")) %>%
  # mutate(collapsed_values = paste(token, collapse = " "), .by = "entity2") %>%
  # summarise(collapsed_values = paste(token, collapse = " "), .groups = 'rowwise') %>%
  # ungroup() %>%
  # distinct(entity2, collapsed_values)
  nothing()
```
```{r teste}
s2v("foo bar bla fulano bla lorem ipsum dolor", print = T)

df1 <- tibble(
  v1 = c(T, T, F, T, F, T, T, T),
  v2 = c("foo", "bar", "bla", "fulano", "bla", "lorem", "ipsum", "dolor")
)

tibble(c("foo bar", "bla", "fulano", "bla", "lorem ipsum dolor"))

# select(pos, token, entity) |>
pos |>
  mutate(name = grepl("^PER_", entity)) |>
  # DF |>
  # group_by(name= cumsum(0 + !(lag(name, default = grepl("^PER_", entity)) & name)), ) #|>
  group_by(name = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
  # summarise(name = token |> unique() |> paste(collapse = " "))
  mutate(name = token |> unique() |> paste(collapse = " ")) |>
  filter(!entity == "PER_I")

group_names <- function(DF, v1, v2) {
  # select(pos, token, entity) |>
  # DF |>
  group_by(v1 = cumsum(0 + !(lag(v1, default = TRUE) & v1)), ) |>
    summarise(v2 = v2 |> unique() |> paste(collapse = " "))
}

group_names(df1, v1, v2)

select(pos, token, entity) |>
  mutate(name = grepl("^PER_", entity)) |>
  group_names("name", "token")
group_names(name, token)
```
```{r fun group_names}
group_names <- function(DF) {
  DF |>
    mutate(name = grepl("^PER_", entity)) |>
    # DF |>
    # group_by(name= cumsum(0 + !(lag(name, default = grepl("^PER_", entity)) & name)), ) #|>
    group_by(name = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
    # summarise(name = token |> unique() |> paste(collapse = " "))
    mutate(name = token |> unique() |> paste(collapse = " ")) |>
    filter(!entity == "PER_I")
}
pos_ <- txt[3] |>  spacyr::spacy_parse( dependency = T) |> group_names()
rsyntax::plot_tree(|> as_tokenindex(pos_), token, lemma, pos)
```
```{r ex DF}
library(dplyr)
library(tidyr)

# Sample data
# df <- data.frame(
#   A = c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j"),
#   B = c(FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE)
# )
df <- data.frame(B= c(F,T, T, T, F, F, F,T,T,F)) |> mutate(A=letters[1:length(B)])

# Create a grouping variable based on consecutive TRUE values in column B
df <- df |> mutate(group = cumsum(!B))
```
```{r ex gemini = group sequence}

# Group by the grouping variable and concatenate the text in column A
df %>%
  group_by(group) %>%
  filter(any(B)) %>%
  summarize(A = paste(A, collapse = " "))

## opção 2 - nao funcionou
# Use rle to identify runs of TRUE
# Run Length Encoding: Compute the lengths and values of runs of equal values in a vector -- or the reverse operation
rle_result <- rle(df$B)
# Create a grouping variable based on runs of TRUE
df <- df %>% mutate(group = cumsum(rle_result$lengths * rle_result$values))
# Group and concatenate
df %>%
  group_by(group) %>%
  filter(any(B)) %>%
  summarize(A = paste(A, collapse = " "))


## opção 3
df %>%
  # group_by(run = data.table::rleid(B)) #%>%
  dplyr::group_by(run = data.table::rleidv(cols = B)) # %>%
# summarise(count = paste(A[1], n(), sep = ":"))
```
```{r ex data table rleid}
library(data.table)
DT <- data.table::data.table(grp = rep(c("A", "B", "C", "A", "B"), c(2, 2, 3, 1, 2)), value = 1:10)
data.table::rleid(DT$grp) # get run-length ids
data.table::rleidv(DT, "grp") # same as above

data.table::rleid(DT$grp, prefix = "grp") # prefix with 'grp'

# get sum of value over run-length groups
DT[, sum(value), by = .(grp, data.table::rleid(grp))]
DT[, sum(value), by = .(grp, data.table::rleid(grp, prefix = "grp"))]
DT[, paste(value, collapse = " "), by = .(grp, data.table::rleid(grp, prefix = "grp"))]
DT[, paste(value, collapse = " "), by = .(grp, data.table::rleid(grp))]

DT$grp |> rle()
```
```{r fun group_entities}
# agrupa sequencia das mesmas entites se subsequentes
t <- "UnitedHealthcare boss Brian Thompson, 5f, was fatally shot in the back on Wednesday morning outside the Hilton hotel in Midtown Manhattan... Investigators are using surveillance photos, bullet casings with cryptic messages written on them, and the suspect's movements to track him down. They are also working with the FBI and authorities in other states as the search expands beyond New York"

spacyr::spacy_initialize(model = "en_core_web_lg")
#' spacyr::spacy_parse(t, dependency = T) |> group_entities()
#'
#' # example in Portuguese language
#' spacyr::spacy_initialize(model = "pt_core_news_lg")
#' "Maria Jana ama John Smith e Maria é amada por Joaquim de Souza" |>
#'   spacyr::spacy_parse(dependency = T) |>
#'   group_entities()
# group_entities <- function(DF) {
# group_entity <- function(DF) {
DF <- spacyr::spacy_parse(t, dependency = T) #|> group_ppn()
entities <- "^(PER(SON)?|ORG|GPE)_"

DF |>
  dplyr::mutate(name = grepl(entities, entity)) |>
  # dplyr::mutate(name = (entity == "(PER|PERSON|ORG)_.*")) #|>
  dplyr::group_by(name2 = data.table::rleid(name)) %>%
  # dplyr::group_by(name2 = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
  dplyr::mutate(name3 = token |>
f   # unique() |>
    paste(collapse = " ")) |>
  # dplyr::filter(entity != paste0(entities, "I")) |>
  dplyr::filter(!grepl("_I", entity), entity != "", name)
```
```{r teste datatable}
DT_pos <- DF |> dplyr::mutate(name = grepl(entities, entity))# |> data.table::setDT()
  # as.character())

# DT_pos[, paste(token, collapse = " "), 
# by(DT_pos$token, DT_pos$name fun = data.table::rleid ]

DT_pos[, paste(token, collapse = " "), by = .(name, data.table::rleid(name))] 
DT_pos[, paste(token, collapse = " "), by = .(name, data.table::rleid(name))]  |> dplyr::pull(V1)
DT_pos[, paste(token, collapse = " "), by = .(name, data.table::rleid(name))]  |> dplyr::pull(V1) |> dplyr::filter(name)
# DT_pos[, paste(token, collapse = " "), by = .(name, data.table::rleid(name))]

# DT_pos |> dplyr::select(DT_pos, doc_id, sentence_id, token, pos, entity, name)[, paste(token, collapse = " "), by = .(name, data.table::rleid(name))]
dplyr::select(DT_pos, token,  name)[, paste(token, collapse = " "), by = .(name, data.table::rleid(name))]
```

```



###

```{r txtPT_trt_full_pos }
# texto de plataformas gig economy
txtPT_trt_full_pos <- txtPT_trt_full |>
  # txtPT_trt_full[63:70] |>
  gsub2("^ +$", "") |>
  stringi::stri_remove_empty() |>
  # adicionando ponto final nas linhas sem
  lapply(gsub2, "(.*[^\\.,:;!?])$", "\\1.") |>
  unlist() |>
  paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T)
# lapply(\(x) spacyr::spacy_parse(x,dependency = T))

count(txtPT_trt_full_pos, sentence_id)
count(txtPT_trt_full_pos, dep_rel, sort = T) |> filter(grepl2(dep_rel, "obj"))

txtPT_trt_full_pos |> semgram::extract_motifs()

# TODO ver como fazer com iobj
lista_grafo_trt <- txtPT_trt_full_pos |>
  mutate(dep_rel = gsub(x = dep_rel, ".*obj", "dobj"))

count(lista_grafo_trt, dep_rel, sort = T) |> filter(grepl2(dep_rel, "obj"))

semgram_trt <- lista_grafo_trt |> semgram::extract_motifs()
triplets <- semgram_trt |> extract_triplets()

# funcoes abaixo foram incorporadas ao pacote
# triplet1 <- select(lista_grafo_trt$agent_treatments, Agent, treatment, Entity) |>
#   rename(from = Agent, label = treatment, to = Entity)
# triplet2 <- select(lista_grafo_trt$action_patients, Entity, action, Patient) |>
#   rename(from = Entity, label = action, to = Patient)
# triplets <- bind_rows(triplet1, triplet2)
triplets
# triplets |> count_graph2()
plot_graph2(text = txtPT_trt_full, triplets)
triplets |> viz_graph()
```
```{r fun count_graph }
count_graph <- function(graph, from, label, to, sort = F) {
  if (missing(from)) {
    from <- names(graph)[1]
  }
  if (missing(label)) {
    label <- names(graph)[2]
  }
  if (missing(to)) {
    to <- names(graph)[3]
  }
  #  message("f: ", from, "| L:", label, "|T:", to)
  # , l = names(graph)[2], t = names(graph)[3]

  graph |>
    # triplets  |>
    # count({{ from }}, {{ label }}, {{ to }}) |>
    count(from, label, to, sort = sort) |>
    # rename(from = {{ from }}, label = {{ label }}, to = {{ to }}, value = n) #|> head(100)
    rename(value = n) #|> head(100)
}

triplets |> count_graph()
# triplets[, from]
```
```{r ex deepseek}
In R language, I have this dataframe
# df <- data.frame(
#   A = c("a", "b", "c", "d", "e", "f", "g", "h", "i"),
#   B = c(FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE)
# )

When column B is TRUE and follows another TRUE, conflate/collapse the values of column A.

df <- df %>%
  mutate(
    A_new = ifelse(B & lag(B, default = FALSE), paste0(lag(A), A), A)
  ) %>%
  mutate(
    A = ifelse(B & lag(B, default = FALSE), NA, A)
  ) %>%
  fill(A_new, .direction = "down") %>%
  mutate(
    A = ifelse(is.na(A), A_new, A)
  ) %>%
  select(-A_new)
```
```{r help grupo R - melhor opcao}

Olá Pessoal,
Tenho o seguinte DF 

df <- data.frame(B= c(F,T, T, T, F, F, F,T,T,F)) |> mutate(A=letters[1:length(B)])


Quero rodar um `paste(collapse= " ")` na coluna A, apenas quando houver sequencias de TRUE. Assim, quero obter:
c("a", "b c d", "e","f","g", "h i", "j")

Tentei com 
df |> group_by(B = cumsum(0 + !(lag(B, default = TRUE) & B)), ) |>
  mutate(name = A |> paste(collapse = " "))  |>  
  cbind(L = df$B) |> 
  mutate(L2 = L==T & lag(L)==T,
  L3 = is.na(L2)) |> 
  filter(L3) |> select(-L2, -L3, -L)
  # mutate(L2 = ifelse(L && lead(L),T,F ))
 
data.table::rleid( data.table::setDT(df)$B) # get run-length ids


DT_pos |> 
  group_by(name = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
  mutate(name = token |> paste(collapse = " "))  |>  
  cbind(L = DT_pos$name ) |> 
  mutate(L2 = L==T & lag(L)==T,
  L3 = is.na(L2)) |> 
  filter(L3)|> 
  # select(-L2, -L3, -L) |>
  pull(name)
```
```{r}
extract_entities <- function(DF) {
  DF |>
    dplyr::mutate(name = grepl(entities, entity)) |> # data.table::setDT()
    group_by(name = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
    mutate(name = token |> paste(collapse = " ")) |>
    cbind(L = DF$name) |>
    mutate(
      L2 = L == T & lag(L) == T,
      L3 = is.na(L2)
    ) |>
    filter(L3) |>
    # select(-L2, -L3, -L) |>
    pull(name)
}

DT_pos |> extract_entities()
```
```{r fun lapply}
#' Extract entities in list format, by sentence
extract_entities_l <- function(DF) {
  doc_ids <- unique(DF$doc_id)
  sentenc_id <- unique(DF$sentence_id)
  # message(doc_ids)
  # ID = 1
  lapply(sentenc_id, \(id_sent) {
    message("sentenc: ", id_sent)

    DF |>
      dplyr::filter(sentence_id == id_sent) |>
      extract_entities()
  })
}

DT_pos |> extract_entities_l()
```

# dataviz
```{r ex ggraph autograph}
library(tidygraph)
library(ggraph)

gr <- create_notable('herschel') %>%
  mutate(class = sample(letters[1:3], n(), TRUE)) %E>%
  mutate(weight = runif(n()))

# Adding node labels will cap edges
autograph(gr, node_label = class)

letters[1:10] |> create_notable()

```

```{r ex ggraph autograph }
x <- txt_wiki[2:44] |> 
  filter_by_query("Police") |>
  parsePOS()

x

gr <- get_cooc_entities(x)

  graph <- df |> head(head_n)
  vert <- unique(c(graph$n1, graph$n2))

  # frequency of nodes/terms
  freqPPN <- lapply(vert, \(v) {
    text |> stringr::str_extract_all(v)
  }) |>
    unlist() |>
    count_vec()

color="lightblue"
gr$edges |> 
  tidygraph::as_tbl_graph() |>
  ggraph::autograph(
    node_label= name, 
    node_size = gr$nodes$freq,
    edge_colour = color, 
    edge_width = freq)

```
```{r ex ggraph}
cv <- s2v("black blue yellow red dargoldenrod4 mediumblue midnightblue firebrick4 maroon4 forestgreen seagreen4 lightpink4 mediumpurple4")
length(cv)

edge_color = s2v("lightpink4 forestgreen")[2]
edge_alpha = 0.1

# color="lightblue"
point_color = cv[2]
point_fill = cv[3]
point_alpha = 0.3
# font_color = s2v("dargoldenrod4 mediumblue firebrick4")[2]
font_color = cv[2]
font_size = 2

graph_layout = s2v("nicely graphopt stress kk mds fr drl lgl")

# gr$edges |> as_tbl_graph() 
# gr

node_size <- gr$nodes |> 
  dplyr::filter(!node %in% gr$isolated_nodes$node)

gr$edges |>
  tidygraph::as_tbl_graph() |>
    # igraph::graph_from_data_frame(directed = FALSE, vertices = gr$nodes ) |>
    ggraph::ggraph(layout = graph_layout[6]) +
    ggraph::geom_edge_link(ggplot2::aes(
    # edge_width = 1, 
      edge_width = gr$edges$freq, 
      edge_alpha = edge_alpha),
      angle_calc = "along",
      label_dodge = grid::unit(4.5, "mm"),
      color = edge_color,
      # c("lightblue", "blue", "royalblue")[1],
      end_cap = ggraph::circle(6, "mm")  # afastamento do nó
    ) + 
    ggraph::geom_node_point(ggplot2::aes(
      size = node_size$freq, 
      alpha = point_alpha, 
      # alpha = 0.3, 
      fill = point_fill ,
      color = point_color )) +
    ggraph::geom_node_text(ggplot2::aes(
      label = name,
      # alpha = 0,
      size = font_size,
      # fill = "mediumpurple4",
      color = font_color), 
      # size = node_size$freq), 
      repel = TRUE) + 
  
  ggplot2::theme_void() +
    ggplot2::theme(legend.position = "none")
```
```{r list data to test fun graph_ppn}
gr$edges  |> plot_pos_graph()
txt_wiki[2:44] |> 
  filter_by_query("Police") |>
  parsePOS() |> 
  get_cooc_entities() |> 
  plot_pos_graph()

graph_ppn <- filter_by_query(txt_wiki[2:44], "Police") |> 
  parsePOS(only_entities=FALSE)  |> 
  dplyr::filter(entity_type != "CARDINAL") |> # to clean the graph
  dplyr::mutate(token = gsub("Police", "police", token)) |> # normalize the term "police"
  get_cooc()
```
```{r fun plot_pos_graph}
plot_pos_graph <- function(pos_list,
                           edge_color = "lightblue",
                           edge_alpha = 0.1,
                           font_size=2, 
                           font_color = "black",
                           point_fill = "firebrick4",
                           point_alpha = 0.3,
                           point_color = "firebrick4",
                           graph_layout = "graphopt") {
# === Updated upstream
    # pos_list <- graph_ppn
  # 
  # node_size <- pos_list$nodes |> 
  #   dplyr::filter(!node %in% pos_list$isolated_nodes$node)
  #
  # pos_list$edges |>
# =======
# pos_list <- graph_ppn
  # pos_list <- uber_g 

  head_edges <- pos_list$edges |> head(n_head)
  head_nodes <- c(head_edges$n1, head_edges$n2) |> unique()

  node_size <- pos_list$nodes |> dplyr::filter(node %in% head_nodes)
  # node_size <- head_edges  #|> dplyr::filter(!node %in% pos_list$isolated_nodes$node)
  # nrow(pos_list$nodes)

 head_edges |> 
## ===== Stashed changes
    tidygraph::as_tbl_graph() |>
      ggraph::ggraph(layout = graph_layout) +
      ggraph::geom_edge_link(ggplot2::aes(
        edge_width = pos_list$edges$freq, 
        edge_alpha = 0.5),
        angle_calc = "along",
        label_dodge = grid::unit(4.5, "mm"),
        color = color,
        end_cap = ggraph::circle(6, "mm")
        ) + # afastamento do nó
      ggraph::geom_node_point(ggplot2::aes(
        size = node_size$freq, 
        alpha = point_alpha, 
        # alpha = 0.3, 
        fill = point_fill ,
        color = point_color )) +
      ggraph::geom_node_text(ggplot2::aes(
        label = name,
        # alpha = 0,
      # size = node_size$freq), 
        size = font_size,
        # fill = "mediumpurple4",
        color = font_color), 
        repel = TRUE) + 
      ggplot2::theme_void() +
      ggplot2::theme(legend.position = "none")
 }

graph_ppn |> plot_pos_graph(font_size = 5)
# ggsave("www/graph_pos_wikiText.png", width = 8, height = 8)
```
```{r testing}
# outros dados para testar a funcao plot_pos_graph
arq <- "~/Documentos/Programação/R/cebrap/analise_tribunais/data/uber-agregado-acordaos.Rds"; file.exists(arq)
# file.exists("arq")
uber_acor <- readRDS(arq)
dplyr::glimpse(uber_acor)
try(spacyr::spacy_finalize() )
spacyr::spacy_initialize(model = "pt_core_news_lg")
tictoc::tic()
uber_g  <- 
  uber_acor$textoAcordao_txt |>
    filter_by_query("onerosidade") |>
    parsePOS() |>
    get_cooc_entities()
tictoc::toc() # uber:342.445 sec = 5,7 min
# uber_g
uber_g |> plot_pos_graph(font_size=75, n_head = n_head ) + labs(title = f('Os {n_head} termos coocorrentes ao termo "{termo}"'))
ggsave(f("~/Documentos/Programação/R/cebrap/analise_tribunais/www/uber_coocorrencias_{termo}_{n_head}.png"), width = 8, height = 8)
```
```{r}
# gr$edges |> 
#   tidygraph::as_tbl_graph() |>
#   ggraph::autograph(node_label= name)
#
# ```
# ```{r ex ggraph}
# color="lightblue"
#
# gr$edges |> as_tbl_graph() 
#
# gr$edges |>
#   tidygraph::as_tbl_graph() |>
#     # igraph::graph_from_data_frame(directed = FALSE, vertices = gr$nodes ) |>
#     ggraph::ggraph(layout = "graphopt") +
#     ggraph::geom_edge_link(ggplot2::aes(edge_width = 1, edge_alpha = 0.5),
#       angle_calc = "along",
#       label_dodge = grid::unit(4.5, "mm"),
#       color = color,
#       # c("lightblue", "blue", "royalblue")[1],
#       end_cap = ggraph::circle(6, "mm")
#     ) + # afastamento do nó
#     ggraph::geom_node_text(ggplot2::aes(label = name, size = 2), repel = TRUE) + # TODO ajustar tamanho minimo e máximo
#     # ggraph::geom_node_text(ggplot2::aes(label = name, size = freq), repel = TRUE) + # TODO ajustar tamanho minimo e máximo
#     # ggraph::geom_node_label(ggplot2::aes(label = name), repel=TRUE,  point.padding = unit(0.2, "lines")) +
#     ggplot2::theme_void() +
#     ggplot2::theme(legend.position = "none")

```




summarise(name = token |> unique() |> paste(collapse = " "))




summarise(name = token |> unique() |> paste(collapse = " "))




# selecionando a partir de x nos filhos
```{r test}
df <- tibble::tribble(
  ~from, ~c2, ~to,
  "a", "l", "b",
  "a", "l", "b",
  "a", "l", "c",
  "b", "l", "c",
  "e", "l", "f"
)
df
nodes <- tibble(ids = c(df$from, df$to) |> unique())

g <- tbl_graph(nodes = nodes, edges = df)

extract_children <- function(variables) {
  # nem comecei. mudei para ego_graph
}

g |> to_subgraph(ids %in% c("a"), subset_by = "nodes")
```
```{r ex filtra gafico por n nos filhos/ vizinhos}
# funciona

# Sample graph data
nodes <- data.frame(id = 1:5, name = LETTERS[1:5])
nomes <- s2v("joao joaquim jojo joana jovem Janus Jambo")
nodes <- data.frame(id = 1:length(nomes), name = nomes)
edges <- data.frame(from = c(1, 1, 2, 3, 4, 1, 6, 7), to = c(2, 3, 4, 5, 5, 4, 7, 5))
# plot graph
g <- igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes)
plot(g)

g2 <- g |> igraph::make_ego_graph(order = 2, nodes = "joaquim", mode = c("all"))
plot(g2[[1]])
as_tbl_graph(g2[[1]])

filter_ego <- function(edges, nodes = NULL, filter_by, n_neighbours = 1) {
  if (is.null(nodes)) {
    message("Nodes are empty. Extracting it from edge dataframe")
    nodes <- unique(c(edges[["from"]], edges[["to"]]))
    nodes <- data.frame(id = 1:length(nodes), name = nodes)
  }

  g <- igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes) |>
    igraph::make_ego_graph(order = n_neighbours, nodes = filter_by, mode = c("all"))

  as_tbl_graph(g[[1]])
}

filter_ego(df, n_neighbours = 2)
filter_ego(edges, nodes, filter_by = "jojo", n_neighbours = 2)
```
```{r ex tidygraphs}
graph <- tbl_graph(edges = tibble(from = c(1, 1, 2, 2, 3), to = c(2, 3, 4, 5, 6)), nodes = tibble(node_id = 1:6))

# Extract the ego-network of node 1, including neighbors up to 2 hops
ego_network <- graph %>%
  focus(node_id == 1) %>%
  # Here, you can use more complex filtering based on edge weights, node attributes, or other criteria
  # For example, to limit to 2 hops:
  mutate(distance = shortest_path(to = node_id)) %>%
  filter(distance <= 2) %>%
  unfocus()
```
### tidygraph ego subgraph
```{r graph df}
# Create a sample graph
# graph <- create_notable("bull")
# graph <- create_kautz(3, 5)
# create_bipartite(3, 5, directed = FALSE, mode = "out")
# create_lattice(5)
# graph <- create_citation(5)
# graph

# Node data frame
nodes <- data.frame(
  name = c("Alice", "Bob", "Charlie", "David", "John", "Mary")
)
# Edge data frame
edges <- data.frame(
  from = c(1, 1, 2, 3, 2, 6),
  to = c(2, 3, 4, 4, 5, 5)
)
# from = c("Alice", "Alice", "Bob", "Charlie", "Bob"),
# to = c("Bob", "Charlie", "David", "David", "John")
graph <- tidygraph::tbl_graph(nodes = nodes, edges = edges)
graph
```
```{r fun get_node_id}
get_node_index(graph, stringr::str_detect(name, "A|B|C"))

name <- "Bob"

g <- graph |>
  tidygraph::activate(nodes) |>
  data.frame() |>
  dplyr::mutate(id = dplyr::row_number())
g |> dplyr::filter(name == name)
g |> dplyr::filter(name == "Bob")

get_node_id <- function(graph, querie) {
  graph |>
    tidygraph::activate(nodes) |>
    data.frame() |>
    dplyr::mutate(id = dplyr::row_number()) |>
    dplyr::filter(name == querie) |>
    pull(id)
}
get_node_id(graph, "Bob")
```
```{r test gemini}
# Select the node with the label "A" and its immediate neighbors
library(tidygraph)

query <- "Bob"

subgraph <- graph |>
  tidygraph::activate(nodes) |>
  # filter(label == "A") %>%
  # filter(name == "Alice") %>%
  # convert(to_local_neighborhood, node = 6, order = 1, mode = "all")
  tidygraph::convert(to_local_neighborhood,
    # node = 1,
    node = get_node_id(graph, query),
    order = 1, mode = "all"
  )

get_neighbors <- function(graph, query, n = 1) {
  graph |>
    tidygraph::activate(nodes) |>
    tidygraph::convert(to_local_neighborhood,
      # node = 1,
      node = get_node_id(graph, query),
      order = n, mode = "all"
    )
}

get_neighbors(g, "Bob", 2)

# Visualize the subgraph
# graph |>
subgraph |>
  ggraph(layout = "kk") +
  geom_edge_link() +
  geom_node_point() +
  # geom_node_label(aes(label = label))
  geom_node_label(aes(label = name))
```

```{r subgraph}
library(tidygraph)
# Create a tidygraph object
graph <- tbl_graph(nodes = nodes, edges = edges)
plot(graph)
# Identify the target node (e.g., node with id 1)
target_node_id <- 1

# Extract child nodes
child_nodes <- graph %>%
  activate(edges) %>%
  filter(from == target_node_id) %>%
  pull(to)

# Filter the graph to include only the target node and its children
subgraph <- graph %>%
  filter(id %in% c(target_node_id, child_nodes))
# Visualize the subgraph (optional)
library(ggraph)
ggraph(subgraph, layout = "tree") +
  geom_node_point() +
  geom_edge_link() +
  geom_node_label(aes(label = name))
```
```{r teste}
# Create a simple graph
my_graph <- tbl_graph(edges = tibble(from = c(1, 1, 2, 2, 3), to = c(2, 3, 4, 5, 6)))

# Focus on node 1 and select its first 2 children
graph %>%
  focus(id == 1) %>%
  filter(igraph::edge_index() <= 2) %>%
  unfocus()
```
## Transform labels into 2 dataframes: indexes a
```{r exemplo split df graph into 2 dataframes}
# Assuming your dataframe is named "graph_data"
# DF_graph
graph_data <- data.frame(
  from = c("Amanda", "Bruno", "Carlos", "Daniel"),
  to = c("Bruno", "Carlos", "Daniel", "Amanda")
)

# Create a unique list of all nodes (vertices)
unique_nodes <- unique(c(graph_data$from, graph_data$to))

# Create a mapping between node names and indices
node_mapping <- data.frame(
  id = seq_along(unique_nodes),
  label = unique_nodes
)

# Replace node names with indices in the graph data
graph_data <- graph_data |>
  dplyr::mutate(
    from = purrr::map_chr(from, ~ node_mapping$id[node_mapping$label == .x]),
    to = purrr::map_chr(to, ~ node_mapping$id[node_mapping$label == .x])
  )

# Print the transformed graph data and node mapping
graph_data
node_mapping
```
```{r fun split_graph }
#' split a tidy graph (each line is grah with at least 2 nodes)

split_graph <- function(DF_graph) {
  # Create a unique list of all nodes (vertices)
  unique_nodes <- unique(c(DF_graph$from, DF_graph$to))

  # Create a mapping between node names and indices
  node_mapping <- data.frame(
    id = seq_along(unique_nodes),
    label = unique_nodes
  )

  # Replace node names with indices in the graph data
  graph_data <- DF_graph |>
    dplyr::mutate(
      from = purrr::map_chr(from, ~ node_mapping$id[node_mapping$label == .x] |> as.character()),
      to = purrr::map_chr(to, ~ node_mapping$id[node_mapping$label == .x] |> as.character()) |> as.character()
    )

  list_ <- list(node_mapping, graph_data)
  names(list_) <- s2v("node_mapping graph_data")
  list_
}


data.frame(
  from = c("Amanda", "Bruno", "Carlos", "Daniel"),
  to = c("Bruno", "Carlos", "Daniel", "Amanda")
) |> split_graph()
```
```{r ex gemini}
# Create the dataframe
df <- data.frame(
  col1 = c(1, 1, 1, 2, 2, 2),
  col2 = c(TRUE, FALSE, TRUE, TRUE, FALSE, FALSE),
  col3 = c(1, 2, 3, 4, 5, 6)
)

# Use `split` and `subset` to group and filter
result <- split(df$col3, df$col1)
result <- lapply(result, function(x) x[df$col2[match(x, df$col3)]])
result
```
```{r fun filtrando ppn em cada frase}
pos <- paste(txt[1:2], collapse = " ") |> spacyr::spacy_parse(, dependency = T)
pos |>
  group_seq_pos() |>
  pull(collapsed)
pos |>
  group_seq_pos() |>
  dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos)) |>
  select(pos, collapsed, noun_ppn)

filter_ppn <- function(pos_df, POS = c("NOUN", "PROPN")) {
  pos_df |>
    group_seq_pos() |>
    # dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos )) |>
    dplyr::mutate(noun_ppn = pos %in% POS) |>
    # select(sentence_id, pos, collapsed, noun_ppn) |>
    filter(noun_ppn) |>
    select(-noun_ppn) |>
    pull(collapsed) |>
    nothing()
}

#' extract proper name and nouns from POS DF
#' @params POS the POS to be extracted
filter_ppn <- function(pos_df, POS = c("NOUN", "PROPN")) {
  pos_df <- pos_df |>
    group_seq_pos() |>
    # dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos )) |>
    dplyr::mutate(noun_ppn = pos %in% POS) |>
    # select(sentence_id, pos, collapsed, noun_ppn) |>
    filter(noun_ppn) |>
    select(-noun_ppn) |>
    nothing()

  result <- split(pos_df$collapsed, pos_df$sentence_id)
  # result <-
  # lapply(result, function(x) x[pos_df$col2[match(x, pos_df$col3)]])
  result
}

pos |> filter_ppn()
txtPT_trt_full_pos |> filter_ppn()

g_v <- filter_ppn(pos)[[1]] |>
  combn(2, simplify = F) |>
  unlist()
# separando os elementos pares dos impares do vetor em um DF
tibble(
  n1 = g_v[seq(1, length(g_v), by = 2)],
  n2 = g_v[seq(2, length(g_v), by = 2)]
)
# list_g  <-

lapply(filter_ppn(pos), \(x) {
  g_v <- combn(x, 2, simplify = F) |> unlist()

  tibble::tibble(
    n1 = g_v[seq(1, length(g_v), by = 2)],
    n2 = g_v[seq(2, length(g_v), by = 2)]
  )
}) |> bind_rows()
# lapply(list_g, \(x) { .[[x]][1] })
```
```{r fun extract_graph_pos}
extract_graph_pos <- function(pos_df, count = FALSE) {
  extract_graph
  graph <- lapply(filter_ppn(pos_df), \(x) {
    g_v <- combn(x, 2, simplify = F) |> unlist()

    tibble::tibble( # id = x,
      n1 = g_v[seq(1, length(g_v), by = 2)],
      n2 = g_v[seq(2, length(g_v), by = 2)]
    )
  }) |>
    # mutate(id_sentence = x["sentence_id"],)
    bind_rows()

  if (count) {
    graph |> dplyr::count(n1, n2, sort = T)
  } else {
    graph
  }
}

extract_graph_pos(pos)
extract_graph_pos(pos, T)
as_tibble(txtPT_trt_full_pos) |> extract_graph_pos(F)
```
















I have a dataframe in R. One column is `entity = c(PER_B, NA ,NA, NA, PER_B, PER_I, PER_I )`. How to paste/conflate all the lines of the other column `tokens = c("Amanda","é", "amada", "por", "Joaquim", "de", "Souza")` so the new column became `new = c("Amanda","é", "amada","por", "Joaquim de Souza")`

## rollama
```{r rollama}
library(rollama)
"tinyllama"
```












