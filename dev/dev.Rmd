# Dev de extract relations

```{r ll}
sto::ll("dplyr spacyr udpipe sto rsyntax tibble tidygraph ggraph")
options(browser = "firefox")
# library("txtnet")
```
```{r available}
pak::pkg_install("available")
available::available("textnetwork")
available::available("nettext")
available::available("txtnet")
```
```{r devtools vignette}
devtools::build_vignettes("First_steps")
devtools::build_vignettes()
devtools::build_vignettes("01-Extract entity coocurrences with regex")
#devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
usethis::use_readme_rmd()
usethis::use_github()

devtools::build_readme() # locates your README.Rmd and builds it into a README.md
pkgdown::build_site_github_pages()
devtools::build_manual() # Create package pdf manual
devtools::build_site() # Execute pkgdown build_site in a package
# build_site() is a shortcut for pkgdown::build_site()
pkgdown::build_site()
```
```{r txt}
txt <- c(
  "Maria Jana ama John Smith e Maria é amada por Joaquim de Souza.",
  "Os Estados Unidos da América emitiram um novo alerta para a Justiça do Trabalho do Estado do Mato Grosso do Sul, a título de indenização.",
  "O iogurte acabou",
  "O rato Roberto Ratatouille roeu a roupa do rei de Roma.",
  "Maria de Jesus ama John Smith, Cláudio é amado por Amanda, mas Cláudio não ama ninguém.",
  "O novo plano foi anunciado esta semana por Biden. A fala do presidente recebeu críticas."
)
# txtPT_trt_full <- readLines("../narrative_argument/texto_trt.txt")
# txtPT_trt_full <- readLines("../../narrative_argument/data/texto_trt.txt")
txtPT_trt_full <- readLines("../narrative_argument/data/texto_trt.txt")
txtPT_trt_full <- "../narrative_argument/data/txts.R") |> source()

```
# UD pipe
```{r Udpipe}
UDmodels <- list.files("~/Documentos/Programação/R/UDpipe/", pattern="udpipe$", full.names=TRUE)
tokens =udpipe(txt$pt[2], UDmodels[2]) |> as_tokenindex()
tokens = udpipe(txt$en[1], UDmodels[1]) |> as_tokenindex()

txt[1]  |> 
```
# Spacy 
```{r spacy init}
spacy_finalize()
spacyr::spacy_initialize(model = "pt_core_news_lg")
```
```{r spacy pos}
pos <- txt[1] |> spacyr::spacy_parse(, dependency = T)
pos
select(pos, token, pos, dep_rel, entity)
```
```{r fun conflate_ppn conflate_adp extract_nouns_ppnN}
conflate_ppn <- function(pos) {
  # "doc_id"        "sentence_id"   "token_id"      "token"         "lemma"         "pos"           "head_token_id" "dep_rel"       "entity"
  # naLine <-rep(NA, names(pos) |> length())
  # pos |>
  # pos <- txt[1] |> spacyr::spacy_parse(dependency = T)
  rbind(NA, pos) |>
    rbind(NA) |>
    select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
    mutate(
      nome = ifelse(pos == lead(pos),
        # paste(token, lead(token)),
        paste(token2, lead(token2)),
        token
      ),
      nome = ifelse(pos == lag(pos), NA, nome)
    ) |>
    filter(!(is.na(nome) & grepl("^PER_", entity))) |>
    mutate(nome = ifelse(is.na(nome), token, nome)) |>
    filter(!(is.na(nome) & is.na(token))) |>
    nothing()
}

txt[1] |>
  spacyr::spacy_parse(dependency = T) |>
  conflate_ppn()

# collapse ADP
# Collapse upos ADP, if the previous and the next pos are NOUN or PROPN. It is used in the pipeline to extract proper names like "United States of America"
collapse_adp <- function(pos_df) {
  pos_df |>
    # se adp é rodeado por ppn e noun = TRUE
    dplyr::mutate(c_adp = 
      (pos == "ADP" & 
      lag(pos) %in% s2v("NOUN PROPN") &
      lead(pos) %in% s2v("NOUN PROPN")
    ),
    token2 = ifelse(lead(c_adp), 
        paste(lemma, lead(token), collpase = " ") |> trimws(), 
        lemma),
    token2 = ifelse(is.na(token2), token, token2 ) ) |>
    dplyr::rename(token0 = token, token = token2) |> 
    dplyr::filter(!c_adp) |>
    dplyr::select(-c_adp)
    # select(token, pos, entity, c_adp, lemma, lemma2)
}

#' extract entities from POS
extract_entities <- function(pos_df) {
  pos_df  |>
    collapse_adp() |>
    # conflate_ppn()
    group_seq_pos() |>
    dplyr::filter(pos %in% s2v("NOUN PROPN")) |>
    dplyr::pull(collapsed)
}

#' extract_entity for each sentence
#' @return a list of all entities in each sentence
extract_entities2 <- function(pos_df) {
  sentence <- unique(pos_df$sentence_id)

  lapply(sentence, \(x) {
    pos_df |> dplyr::filter(sentence_id == x) |> 
      extract_entities()
         })
    
}


txt[2] |> spacyr::spacy_parse(dependency = T) |> extract_entities()

txt[1:3] |> paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T) |>
  group_by(sentence_id) |>
  extract_entities()

txt[1:3] |> paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T) |>
  extract_entities2()
```
```{r fun entity_list_2_graph }
entity_list_2_graph <- function(entities_list) {
    # entidades <- txt[1:3] |> paste(collapse = " ") |>
    #   spacyr::spacy_parse(dependency = T) |>
    #   extract_entities2()

    comb <- entities_list |>
      combn( 2, simplify = F) |>
      unlist()

    tibble::tibble(
      n1 = comb[seq(1, length(comb), by = 2)],
      n2 = comb[seq(2, length(comb), by = 2)]
    )

}

txt[1] |> paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T) |>
  extract_entities2() |> unlist() |>
  entity_list_2_graph()


#' from list of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs
entity_list_2_graph2 <- function(entities_list, count = TRUE) {
# entities_list <- txt[1:3] |> paste(collapse = " ") |>
#   spacyr::spacy_parse(dependency = T) |>
#   extract_entities2() 

  more_than1_element_in_list <- lapply(entities_list, length)  |> unlist() > 1
  entities_list <- entities_list[more_than1_element_in_list]

  tib <- lapply(entities_list, entity_list_2_graph) |>
    dplyr::bind_rows()

  if (count) {
    tib |> dplyr::count(n1, n2, sort = TRUE)
  } else {
    tib
  }
}

# teste, chamando a função
txt[1:3] |> paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T) |>
  extract_entities2() |> 
  entity_list_2_graph2()

txtPT_trt_full  |>
  spacyr::spacy_parse(dependency = T) |>
  extract_entities2() |> head(6) |>
  entity_list_2_graph2()
```


```{r pos group_by}
# NAO DEU CERTO
pos |> select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  # mutate(entity2 = gsub("(PER)(_.*)", "\\1", entity) ) |>
  # group_by(entity2) |>
  # summarise(token2 = paste(token, collapse = " "), .groups = 'drop')
  nothing()
# summarise(token2 = ifelse(entity2 == lead(entity2), paste(token, collapse = " ", token)))
```
```{r pos}
pos2 <- txt[1] |> spacyr::spacy_parse(dependency = T)

rbind(NA, pos2) |>
  rbind(NA) |>
  select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  mutate(entity2 = gsub("(PER)(_.*)", "\\1", entity)) |>
  mutate(
    nome = ifelse(entity2 == lead(entity2),
      paste(token, lead(token)),
      token
    )
    # nome = ifelse(pos == lag(pos), NA, nome )
  ) |>
  # filter(! (is.na(nome) & grepl("^PER_",entity))  )|>
  nothing()
```
```{r while}
while (nrow(filter(df, grepl("^##", word))) > 0) {
  df <- df |>
    mutate(
      word2 =
        case_when(
          !grepl("^##", word) & !grepl("^##", lead(word)) ~ word,
          grepl("^##", word) & !grepl("^##", lag(word)) ~ paste0(
            lag(word),
            substr(word, 3, length(word))
          ),
          grepl("^##", word) & grepl("^##", lag(word)) ~ word
        )
    ) %>%
    filter(!is.na(word2)) %>%
    mutate(word = word2, word2 = NULL)
}
```
```{r rbind mutate}
rbind(NA, pos) |>
  rbind(NA) |>
  select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  mutate(
    entity2 = gsub("(PER)(_.*)", "\\1", entity),
    # nome=(pos == lead(pos)
    # nome = ifelse(entity2 == lead(entity2), paste(token, lead(token)), token),
    # nome = ifelse(pos == lag(pos), NA, nome)
  ) |>
  # rowwise() |>
  group_by(entity2) %>%
  mutate(collapsed_values = paste(token, lead(token))) %>%
  # mutate(collapsed_values = paste(token, lead(token), collapse = " ")) %>%
  # mutate(collapsed_values = paste(token, collapse = " "), .by = "entity2") %>%
  # summarise(collapsed_values = paste(token, collapse = " "), .groups = 'rowwise') %>%
  # ungroup() %>%
  # distinct(entity2, collapsed_values)
  nothing()
```
```{r teste}
s2v("foo bar bla fulano bla lorem ipsum dolor", print = T)

df1 <- tibble(
  v1 = c(T, T, F, T, F, T, T, T),
  v2 = c("foo", "bar", "bla", "fulano", "bla", "lorem", "ipsum", "dolor")
)

tibble(c("foo bar", "bla", "fulano", "bla", "lorem ipsum dolor"))

# select(pos, token, entity) |>
pos |>
  mutate(name = grepl("^PER_", entity)) |>
  # DF |>
  # group_by(name= cumsum(0 + !(lag(name, default = grepl("^PER_", entity)) & name)), ) #|>
  group_by(name = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
  # summarise(name = token |> unique() |> paste(collapse = " "))
  mutate(name = token |> unique() |> paste(collapse = " ")) |>
  filter(!entity == "PER_I")

group_names <- function(DF, v1, v2) {
  # select(pos, token, entity) |>
  # DF |>
  group_by(v1 = cumsum(0 + !(lag(v1, default = TRUE) & v1)), ) |>
    summarise(v2 = v2 |> unique() |> paste(collapse = " "))
}

group_names(df1, v1, v2)

select(pos, token, entity) |>
  mutate(name = grepl("^PER_", entity)) |>
  group_names("name", "token")
group_names(name, token)
```
```{r fun group_names}
group_names <- function(DF) {
  DF |>
    mutate(name = grepl("^PER_", entity)) |>
    # DF |>
    # group_by(name= cumsum(0 + !(lag(name, default = grepl("^PER_", entity)) & name)), ) #|>
    group_by(name = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
    # summarise(name = token |> unique() |> paste(collapse = " "))
    mutate(name = token |> unique() |> paste(collapse = " ")) |>
    filter(!entity == "PER_I")
}
pos_ <- txt[3] |>  spacyr::spacy_parse( dependency = T) |> group_names()
rsyntax::plot_tree(|> as_tokenindex(pos_), token, lemma, pos)
```

###

```{r txtPT_trt_full_pos }
# texto de plataformas gig economy
txtPT_trt_full_pos <- txtPT_trt_full |>
  # txtPT_trt_full[63:70] |>
  gsub2("^ +$", "") |>
  stringi::stri_remove_empty() |>
  # adicionando ponto final nas linhas sem
  lapply(gsub2, "(.*[^\\.,:;!?])$", "\\1.") |>
  unlist() |>
  paste(collapse = " ") |>
  spacyr::spacy_parse(dependency = T)
# lapply(\(x) spacyr::spacy_parse(x,dependency = T))

count(txtPT_trt_full_pos, sentence_id)
count(txtPT_trt_full_pos, dep_rel, sort = T) |> filter(grepl2(dep_rel, "obj"))

txtPT_trt_full_pos |> semgram::extract_motifs()

# TODO ver como fazer com iobj
lista_grafo_trt <- txtPT_trt_full_pos |>
  mutate(dep_rel = gsub(x = dep_rel, ".*obj", "dobj"))

count(lista_grafo_trt, dep_rel, sort = T) |> filter(grepl2(dep_rel, "obj"))

semgram_trt <- lista_grafo_trt |> semgram::extract_motifs()
triplets <- semgram_trt |> extract_triplets()

# funcoes abaixo foram incorporadas ao pacote
# triplet1 <- select(lista_grafo_trt$agent_treatments, Agent, treatment, Entity) |>
#   rename(from = Agent, label = treatment, to = Entity)
# triplet2 <- select(lista_grafo_trt$action_patients, Entity, action, Patient) |>
#   rename(from = Entity, label = action, to = Patient)
# triplets <- bind_rows(triplet1, triplet2)
triplets
# triplets |> count_graph2()
plot_graph2(text = txtPT_trt_full, triplets)
triplets |> viz_graph()
```
```{r fun count_graph }
count_graph <- function(graph, from, label, to, sort = F) {
  if (missing(from)) {
    from <- names(graph)[1]
  }
  if (missing(label)) {
    label <- names(graph)[2]
  }
  if (missing(to)) {
    to <- names(graph)[3]
  }
  #  message("f: ", from, "| L:", label, "|T:", to)
  # , l = names(graph)[2], t = names(graph)[3]

  graph |>
    # triplets  |>
    # count({{ from }}, {{ label }}, {{ to }}) |>
    count(from, label, to, sort = sort) |>
    # rename(from = {{ from }}, label = {{ label }}, to = {{ to }}, value = n) #|> head(100)
    rename(value = n) #|> head(100)
}

triplets |> count_graph()
# triplets[, from]
```

# selecionando a partir de x nos filhos
```{r test}
df <- tibble::tribble(
  ~from, ~c2, ~to,
  "a", "l", "b",
  "a", "l", "b",
  "a", "l", "c",
  "b", "l", "c",
  "e", "l", "f"
)
df
nodes <- tibble(ids = c(df$from, df$to) |> unique())

g <- tbl_graph(nodes = nodes, edges = df)

extract_children <- function(variables) {
  # nem comecei. mudei para ego_graph
}

g |> to_subgraph(ids %in% c("a"), subset_by = "nodes")
```
```{r ex filtra gafico por n nos filhos/ vizinhos}
# funciona

# Sample graph data
nodes <- data.frame(id = 1:5, name = LETTERS[1:5])
nomes <- s2v("joao joaquim jojo joana jovem Janus Jambo")
nodes <- data.frame(id = 1:length(nomes), name = nomes)
edges <- data.frame(from = c(1, 1, 2, 3, 4, 1, 6, 7), to = c(2, 3, 4, 5, 5, 4, 7, 5))
# plot graph
g <- igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes)
plot(g)

g2 <- g |> igraph::make_ego_graph(order = 2, nodes = "joaquim", mode = c("all"))
plot(g2[[1]])
as_tbl_graph(g2[[1]])

filter_ego <- function(edges, nodes = NULL, filter_by, n_neighbours = 1) {
  if (is.null(nodes)) {
    message("Nodes are empty. Extracting it from edge dataframe")
    nodes <- unique(c(edges[["from"]], edges[["to"]]))
    nodes <- data.frame(id = 1:length(nodes), name = nodes)
  }

  g <- igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes) |>
    igraph::make_ego_graph(order = n_neighbours, nodes = filter_by, mode = c("all"))

  as_tbl_graph(g[[1]])
}

filter_ego(df, n_neighbours = 2)
filter_ego(edges, nodes, filter_by = "jojo", n_neighbours = 2)
```
```{r ex tidygraphs}
graph <- tbl_graph(edges = tibble(from = c(1, 1, 2, 2, 3), to = c(2, 3, 4, 5, 6)), nodes = tibble(node_id = 1:6))

# Extract the ego-network of node 1, including neighbors up to 2 hops
ego_network <- graph %>%
  focus(node_id == 1) %>%
  # Here, you can use more complex filtering based on edge weights, node attributes, or other criteria
  # For example, to limit to 2 hops:
  mutate(distance = shortest_path(to = node_id)) %>%
  filter(distance <= 2) %>%
  unfocus()
```
### tidygraph ego subgraph
```{r graph df}
# Create a sample graph
# graph <- create_notable("bull")
# graph <- create_kautz(3, 5)
# create_bipartite(3, 5, directed = FALSE, mode = "out")
# create_lattice(5)
# graph <- create_citation(5)
# graph

# Node data frame
nodes <- data.frame(
  name = c("Alice", "Bob", "Charlie", "David", "John", "Mary")
)
# Edge data frame
edges <- data.frame(
  from = c(1, 1, 2, 3, 2, 6),
  to = c(2, 3, 4, 4, 5, 5)
)
# from = c("Alice", "Alice", "Bob", "Charlie", "Bob"),
# to = c("Bob", "Charlie", "David", "David", "John")
graph <- tidygraph::tbl_graph(nodes = nodes, edges = edges)
graph
```
```{r fun get_node_id}
get_node_index(graph, stringr::str_detect(name, "A|B|C"))

name <- "Bob"

g <- graph |>
  tidygraph::activate(nodes) |>
  data.frame() |>
  dplyr::mutate(id = dplyr::row_number())
g |> dplyr::filter(name == name)
g |> dplyr::filter(name == "Bob")

get_node_id <- function(graph, querie) {
  graph |>
    tidygraph::activate(nodes) |>
    data.frame() |>
    dplyr::mutate(id = dplyr::row_number()) |>
    dplyr::filter(name == querie) |>
    pull(id)
}
get_node_id(graph, "Bob")
```
```{r test gemini}
# Select the node with the label "A" and its immediate neighbors
library(tidygraph)

query <- "Bob"

subgraph <- graph |>
  tidygraph::activate(nodes) |>
  # filter(label == "A") %>%
  # filter(name == "Alice") %>%
  # convert(to_local_neighborhood, node = 6, order = 1, mode = "all")
  tidygraph::convert(to_local_neighborhood,
    # node = 1,
    node = get_node_id(graph, query),
    order = 1, mode = "all"
  )

get_neighbors <- function(graph, query, n = 1) {
  graph |>
    tidygraph::activate(nodes) |>
    tidygraph::convert(to_local_neighborhood,
      # node = 1,
      node = get_node_id(graph, query),
      order = n, mode = "all"
    )
}

get_neighbors(g, "Bob", 2)

# Visualize the subgraph
# graph |>
subgraph |>
  ggraph(layout = "kk") +
  geom_edge_link() +
  geom_node_point() +
  # geom_node_label(aes(label = label))
  geom_node_label(aes(label = name))
```

```{r subgraph}
library(tidygraph)
# Create a tidygraph object
graph <- tbl_graph(nodes = nodes, edges = edges)
plot(graph)
# Identify the target node (e.g., node with id 1)
target_node_id <- 1

# Extract child nodes
child_nodes <- graph %>%
  activate(edges) %>%
  filter(from == target_node_id) %>%
  pull(to)

# Filter the graph to include only the target node and its children
subgraph <- graph %>%
  filter(id %in% c(target_node_id, child_nodes))
# Visualize the subgraph (optional)
library(ggraph)
ggraph(subgraph, layout = "tree") +
  geom_node_point() +
  geom_edge_link() +
  geom_node_label(aes(label = name))
```
```{r teste}
# Create a simple graph
my_graph <- tbl_graph(edges = tibble(from = c(1, 1, 2, 2, 3), to = c(2, 3, 4, 5, 6)))

# Focus on node 1 and select its first 2 children
graph %>%
  focus(id == 1) %>%
  filter(igraph::edge_index() <= 2) %>%
  unfocus()
```
## Transform labels into 2 dataframes: indexes a
```{r exemplo split df graph into 2 dataframes}
# Assuming your dataframe is named "graph_data"
# DF_graph
graph_data <- data.frame(
  from = c("Amanda", "Bruno", "Carlos", "Daniel"),
  to = c("Bruno", "Carlos", "Daniel", "Amanda")
)

# Create a unique list of all nodes (vertices)
unique_nodes <- unique(c(graph_data$from, graph_data$to))

# Create a mapping between node names and indices
node_mapping <- data.frame(
  id = seq_along(unique_nodes),
  label = unique_nodes
)

# Replace node names with indices in the graph data
graph_data <- graph_data |>
  dplyr::mutate(
    from = purrr::map_chr(from, ~ node_mapping$id[node_mapping$label == .x]),
    to = purrr::map_chr(to, ~ node_mapping$id[node_mapping$label == .x])
  )

# Print the transformed graph data and node mapping
graph_data
node_mapping
```
```{r fun split_graph }
#' split a tidy graph (each line is grah with at least 2 nodes)

split_graph <- function(DF_graph) {
  # Create a unique list of all nodes (vertices)
  unique_nodes <- unique(c(DF_graph$from, DF_graph$to))

  # Create a mapping between node names and indices
  node_mapping <- data.frame(
    id = seq_along(unique_nodes),
    label = unique_nodes
  )

  # Replace node names with indices in the graph data
  graph_data <- DF_graph |>
    dplyr::mutate(
      from = purrr::map_chr(from, ~ node_mapping$id[node_mapping$label == .x] |> as.character()),
      to = purrr::map_chr(to, ~ node_mapping$id[node_mapping$label == .x] |> as.character()) |> as.character()
    )

  list_ <- list(node_mapping, graph_data)
  names(list_) <- s2v("node_mapping graph_data")
  list_
}


data.frame(
  from = c("Amanda", "Bruno", "Carlos", "Daniel"),
  to = c("Bruno", "Carlos", "Daniel", "Amanda")
) |> split_graph()
```
```{r ex gemini}
# Create the dataframe
df <- data.frame(
  col1 = c(1, 1, 1, 2, 2, 2),
  col2 = c(TRUE, FALSE, TRUE, TRUE, FALSE, FALSE),
  col3 = c(1, 2, 3, 4, 5, 6)
)

# Use `split` and `subset` to group and filter
result <- split(df$col3, df$col1)
result <- lapply(result, function(x) x[df$col2[match(x, df$col3)]])
result
```
```{r fun filtrando ppn em cada frase}
pos <- paste(txt[1:2], collapse = " ") |> spacyr::spacy_parse(, dependency = T)
pos |>
  group_seq_pos() |>
  pull(collapsed)
pos |>
  group_seq_pos() |>
  dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos)) |>
  select(pos, collapsed, noun_ppn)

filter_ppn <- function(pos_df, POS = c("NOUN", "PROPN")) {
  pos_df |>
    group_seq_pos() |>
    # dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos )) |>
    dplyr::mutate(noun_ppn = pos %in% POS) |>
    # select(sentence_id, pos, collapsed, noun_ppn) |>
    filter(noun_ppn) |>
    select(-noun_ppn) |>
    pull(collapsed) |>
    nothing()
}

#' extract proper name and nouns from POS DF
#' @params POS the POS to be extracted
filter_ppn <- function(pos_df, POS = c("NOUN", "PROPN")) {
  pos_df <- pos_df |>
    group_seq_pos() |>
    # dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos )) |>
    dplyr::mutate(noun_ppn = pos %in% POS) |>
    # select(sentence_id, pos, collapsed, noun_ppn) |>
    filter(noun_ppn) |>
    select(-noun_ppn) |>
    nothing()

  result <- split(pos_df$collapsed, pos_df$sentence_id)
  # result <-
  # lapply(result, function(x) x[pos_df$col2[match(x, pos_df$col3)]])
  result
}

pos |> filter_ppn()
txtPT_trt_full_pos |> filter_ppn()

g_v <- filter_ppn(pos)[[1]] |>
  combn(2, simplify = F) |>
  unlist()
# separando os elementos pares dos impares do vetor em um DF
tibble(
  n1 = g_v[seq(1, length(g_v), by = 2)],
  n2 = g_v[seq(2, length(g_v), by = 2)]
)
# list_g  <-

lapply(filter_ppn(pos), \(x) {
  g_v <- combn(x, 2, simplify = F) |> unlist()

  tibble::tibble(
    n1 = g_v[seq(1, length(g_v), by = 2)],
    n2 = g_v[seq(2, length(g_v), by = 2)]
  )
}) |> bind_rows()
# lapply(list_g, \(x) { .[[x]][1] })
```
```{r fun extract_graph_pos}
extract_graph_pos <- function(pos_df, count = FALSE) {
  graph <- lapply(filter_ppn(pos_df), \(x) {
    g_v <- combn(x, 2, simplify = F) |> unlist()

    tibble::tibble( # id = x,
      n1 = g_v[seq(1, length(g_v), by = 2)],
      n2 = g_v[seq(2, length(g_v), by = 2)]
    )
  }) |>
    # mutate(id_sentence = x["sentence_id"],)
    bind_rows()

  if (count) {
    graph |> dplyr::count(n1, n2, sort = T)
  } else {
    graph
  }
}

extract_graph_pos(pos)
extract_graph_pos(pos, T)
as_tibble(txtPT_trt_full_pos) |> extract_graph_pos(F)
```
















I have a dataframe in R. One column is `entity = c(PER_B, NA ,NA, NA, PER_B, PER_I, PER_I )`. How to paste/conflate all the lines of the other column `tokens = c("Amanda","é", "amada", "por", "Joaquim", "de", "Souza")` so the new column became `new = c("Amanda","é", "amada","por", "Joaquim de Souza")`

## rollama
```{r rollama}
library(rollama)
"tinyllama"
```












