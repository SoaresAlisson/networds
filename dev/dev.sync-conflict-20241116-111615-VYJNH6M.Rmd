# Dev de extract relations

```{r ll}
sto::ll("dplyr spacyr udpipe sto rsyntax tibble tidygraph")
options(browser = "firefox")
```
```{r available}
pak::pkg_install("available")
available::available("textnetwork")
available::available("nettext")
available::available("txtnet")
```
```{r devtools vignette}
devtools::build_vignettes("First_steps")
devtools::build_vignettes()
devtools::build_vignettes("01-Extract entity coocurrences with regex")
#devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
usethis::use_readme_rmd()
devtools::build_readme()
usethis::use_github()
```

```{r txt}
txt <- c(
  "Maria Jana ama John Smith e Maria é amada por Joaquim de Souza",
  "O rato Roberto Ratatouille roeu a roupa do rei de Roma",
  "Maria de Jesus ama John Smith, Cláudio é amado por Amanda, mas Cláudio não ama ninguém",
  "O novo plano foi anunciado esta semana por Biden. A fala do presidente recebeu críticas."
)
# txtPT_trt_full <- readLines("../narrative_argument/texto_trt.txt")
# txtPT_trt_full <- readLines("../../narrative_argument/data/texto_trt.txt")
txtPT_trt_full <- readLines("../narrative_argument/data/texto_trt.txt")

```
## UD pipe
```{r Udpipe}
UDmodels <- list.files("~/Documentos/Programação/R/UDpipe/", pattern="udpipe$", full.names=TRUE)
tokens =udpipe(txt$pt[2], UDmodels[2]) |> as_tokenindex()
tokens = udpipe(txt$en[1], UDmodels[1]) |> as_tokenindex()

txt[1]  |> 
```
## Spacy 
```{r spacy init}
spacy_finalize()
spacyr::spacy_initialize(model = "pt_core_news_lg")
```
```{r spacy pos}
pos <- txt[1] |> spacyr::spacy_parse(, dependency = T)
pos
select(pos, token, pos, dep_rel, entity)
```
```{r fun conflate PropN}
conflate_ppn <- function(pos) {
  # "doc_id"        "sentence_id"   "token_id"      "token"         "lemma"         "pos"           "head_token_id" "dep_rel"       "entity"
  # naLine <-rep(NA, names(pos) |> length())
  # pos |>
  # pos <- txt[1] |> spacyr::spacy_parse(dependency = T)
  rbind(NA, pos) |>
    rbind(NA) |>
    select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
    mutate(
      nome = ifelse(pos == lead(pos),
        paste(token, lead(token)),
        token
      ),
      nome = ifelse(pos == lag(pos), NA, nome)
    ) |>
    filter(!(is.na(nome) & grepl("^PER_", entity))) |>
    mutate(nome = ifelse(is.na(nome), token, nome)) |>
    filter(!(is.na(nome) & is.na(token))) |>
    nothing()
}
txt[1] |>
  spacyr::spacy_parse(dependency = T) |>
  conflate_ppn()
```
```{r pos group_by}
# NAO DEU CERTO
pos |> select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  # mutate(entity2 = gsub("(PER)(_.*)", "\\1", entity) ) |>
  # group_by(entity2) |>
  # summarise(token2 = paste(token, collapse = " "), .groups = 'drop')
  nothing()
# summarise(token2 = ifelse(entity2 == lead(entity2), paste(token, collapse = " ", token)))
```
```{r pos}
pos2 <- txt[1] |> spacyr::spacy_parse(dependency = T)

rbind(NA, pos2) |>
  rbind(NA) |>
  select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  mutate(entity2 = gsub("(PER)(_.*)", "\\1", entity)) |>
  mutate(
    nome = ifelse(entity2 == lead(entity2),
      paste(token, lead(token)),
      token
    )
    # nome = ifelse(pos == lag(pos), NA, nome )
  ) |>
  # filter(! (is.na(nome) & grepl("^PER_",entity))  )|>
  nothing()
```
```{r while}
while (nrow(filter(df, grepl("^##", word))) > 0) {
  df <- df |>
    mutate(
      word2 =
        case_when(
          !grepl("^##", word) & !grepl("^##", lead(word)) ~ word,
          grepl("^##", word) & !grepl("^##", lag(word)) ~ paste0(
            lag(word),
            substr(word, 3, length(word))
          ),
          grepl("^##", word) & grepl("^##", lag(word)) ~ word
        )
    ) %>%
    filter(!is.na(word2)) %>%
    mutate(word = word2, word2 = NULL)
}
```
```{r}
rbind(NA, pos) |>
  rbind(NA) |>
  select(-doc_id, -sentence_id, -token_id, -head_token_id) |>
  mutate(
    entity2 = gsub("(PER)(_.*)", "\\1", entity),
    # nome=(pos == lead(pos)
    # nome = ifelse(entity2 == lead(entity2), paste(token, lead(token)), token),
    # nome = ifelse(pos == lag(pos), NA, nome)
  ) |>
  # rowwise() |>
  group_by(entity2) %>%
  mutate(collapsed_values = paste(token, lead(token))) %>%
  # mutate(collapsed_values = paste(token, lead(token), collapse = " ")) %>%
  # mutate(collapsed_values = paste(token, collapse = " "), .by = "entity2") %>%
  # summarise(collapsed_values = paste(token, collapse = " "), .groups = 'rowwise') %>%
  # ungroup() %>%
  # distinct(entity2, collapsed_values)
  nothing()
```
```{r teste}
s2v("foo bar bla fulano bla lorem ipsum dolor", print = T)

df1 <- tibble(
  v1 = c(T, T, F, T, F, T, T, T),
  v2 = c("foo", "bar", "bla", "fulano", "bla", "lorem", "ipsum", "dolor")
)

tibble(c("foo bar", "bla", "fulano", "bla", "lorem ipsum dolor"))

# select(pos, token, entity) |>
pos |>
  mutate(name = grepl("^PER_", entity)) |>
  # DF |>
  # group_by(name= cumsum(0 + !(lag(name, default = grepl("^PER_", entity)) & name)), ) #|>
  group_by(name = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
  # summarise(name = token |> unique() |> paste(collapse = " "))
  mutate(name = token |> unique() |> paste(collapse = " ")) |>
  filter(!entity == "PER_I")

group_names <- function(DF, v1, v2) {
  # select(pos, token, entity) |>
  # DF |>
  group_by(v1 = cumsum(0 + !(lag(v1, default = TRUE) & v1)), ) |>
    summarise(v2 = v2 |> unique() |> paste(collapse = " "))
}

group_names(df1, v1, v2)

select(pos, token, entity) |>
  mutate(name = grepl("^PER_", entity)) |>
  group_names("name", "token")
group_names(name, token)
```
```{r fun group_names}
group_names <- function(DF) {
  DF |>
    mutate(name = grepl("^PER_", entity)) |>
    # DF |>
    # group_by(name= cumsum(0 + !(lag(name, default = grepl("^PER_", entity)) & name)), ) #|>
    group_by(name = cumsum(0 + !(lag(name, default = TRUE) & name)), ) |>
    # summarise(name = token |> unique() |> paste(collapse = " "))
    mutate(name = token |> unique() |> paste(collapse = " ")) |>
    filter(!entity == "PER_I")
}
pos_ <- txt[3] |>  spacyr::spacy_parse( dependency = T) |> group_names()
rsyntax::plot_tree(|> as_tokenindex(pos_), token, lemma, pos)
```
```{r}

```

###

```{r txtPT_trt_full_pos }
# texto de plataformas gig economy
txtPT_trt_full_pos <- txtPT_trt_full |>
# txtPT_trt_full[63:70] |>
    gsub2("^ +$", "") |> 
    stringi::stri_remove_empty() |>
    # adicionando ponto final nas linhas sem
    lapply(gsub2, "(.*[^\\.,:;!?])$", "\\1.") |>
    unlist() |>
    paste(collapse = " ") |>  
    spacyr::spacy_parse(dependency = T)
# lapply(\(x) spacyr::spacy_parse(x,dependency = T))

count(txtPT_trt_full_pos, sentence_id)
count(txtPT_trt_full_pos, dep_rel, sort = T) |> filter(grepl2(dep_rel, "obj"))

txtPT_trt_full_pos |> semgram::extract_motifs()

# TODO ver como fazer com iobj
lista_grafo_trt <- txtPT_trt_full_pos |>
  mutate(dep_rel = gsub(x = dep_rel, ".*obj", "dobj"))

count(lista_grafo_trt, dep_rel, sort = T) |> filter(grepl2(dep_rel, "obj"))

semgram_trt <- lista_grafo_trt |> semgram::extract_motifs()
triplets <- semgram_trt  |> extract_triplets() 

# funcoes abaixo foram incorporadas ao pacote 
# triplet1 <- select(lista_grafo_trt$agent_treatments, Agent, treatment, Entity) |>
#   rename(from = Agent, label = treatment, to = Entity)
# triplet2 <- select(lista_grafo_trt$action_patients, Entity, action, Patient) |>
#   rename(from = Entity, label = action, to = Patient)
# triplets <- bind_rows(triplet1, triplet2)
triplets
#triplets |> count_graph2() 
plot_graph2(text=txtPT_trt_full , triplets)
triplets |> viz_graph() 

```
```{r fun count_graph }
count_graph <- function(graph, from, label, to, sort = F) {
  if (missing(from)) {
    from <- names(graph)[1]
  }
  if (missing(label)) {
    label <- names(graph)[2]
  }
  if (missing(to)) {
    to <- names(graph)[3]
  }
  #  message("f: ", from, "| L:", label, "|T:", to)
  # , l = names(graph)[2], t = names(graph)[3]

  graph |>
    # triplets  |>
    # count({{ from }}, {{ label }}, {{ to }}) |>
    count(from, label, to, sort = sort) |>
    # rename(from = {{ from }}, label = {{ label }}, to = {{ to }}, value = n) #|> head(100)
    rename(value = n) #|> head(100)
}

triplets |> count_graph()
# triplets[, from]
```
## selecionando a partir de x nos filhos
```{r test}
df <- tibble::tribble(
  ~from, ~c2, ~to,
  "a", "l", "b",
  "a", "l", "b",
  "a", "l", "c",
  "b", "l", "c",
  "e", "l", "f"
)
df
nodes <- tibble(ids = c(df$from, df$to) |> unique())

g <- tbl_graph(nodes = nodes, edges = df)

extract_children <- function(variables) {
  # nem comecei. mudei para ego_graph
}

g |> to_subgraph(ids %in% c("a"), subset_by = "nodes")
```
```{r ex filtra gafico por n nos filhos/ vizinhos}
# funciona

# Sample graph data
nodes <- data.frame(id = 1:5, name = LETTERS[1:5])
nomes <- s2v("joao joaquim jojo joana jovem Janus Jambo")
nodes <- data.frame(id = 1:length(nomes), name = nomes)
edges <- data.frame(from = c(1, 1, 2, 3, 4, 1, 6, 7), to = c(2, 3, 4, 5, 5, 4, 7, 5))
# plot graph
g <- igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes)
plot(g)

g2 <- g |> igraph::make_ego_graph(order = 2, nodes = "joaquim", mode = c("all"))
plot(g2[[1]])
as_tbl_graph(g2[[1]])

filter_ego <- function(edges, nodes = NULL, filter_by, n_neighbours = 1) {
  if (is.null(nodes)) {
    message("Nodes are empty. Extracting it from edge dataframe")
    nodes <- unique(c(edges[["from"]], edges[["to"]]))
    nodes <- data.frame(id = 1:length(nodes), name = nodes)
  }

  g <- igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes) |>
    igraph::make_ego_graph(order = n_neighbours, nodes = filter_by, mode = c("all"))

  as_tbl_graph(g[[1]])
}

filter_ego(df, n_neighbours = 2)
filter_ego(edges, nodes, filter_by = "jojo", n_neighbours = 2)
```
```{r ex tidygraphs}
graph <- tbl_graph(edges = tibble(from = c(1, 1, 2, 2, 3), to = c(2, 3, 4, 5, 6)), nodes = tibble(node_id = 1:6))

# Extract the ego-network of node 1, including neighbors up to 2 hops
ego_network <- graph %>%
  focus(node_id == 1) %>%
  # Here, you can use more complex filtering based on edge weights, node attributes, or other criteria
  # For example, to limit to 2 hops:
  mutate(distance = shortest_path(to = node_id)) %>%
  filter(distance <= 2) %>%
  unfocus()
```

```{r subgraph}
library(tidygraph)
# Create a tidygraph object
graph <- tbl_graph(nodes = nodes, edges = edges)
plot(graph)
# Identify the target node (e.g., node with id 1)
target_node_id <- 1

# Extract child nodes
child_nodes <- graph %>%
  activate(edges) %>%
  filter(from == target_node_id) %>%
  pull(to)

# Filter the graph to include only the target node and its children
subgraph <- graph %>%
  filter(id %in% c(target_node_id, child_nodes))
# Visualize the subgraph (optional)
library(ggraph)
ggraph(subgraph, layout = "tree") +
  geom_node_point() +
  geom_edge_link() +
  geom_node_label(aes(label = name))
```
```{r}
# Create a simple graph
my_graph <- tbl_graph(edges = tibble(from = c(1, 1, 2, 2, 3), to = c(2, 3, 4, 5, 6)))

# Focus on node 1 and select its first 2 children
graph %>%
  focus(id == 1) %>%
  filter(igraph::edge_index() <= 2) %>%
  unfocus()
```
## Transform labels into 2 dataframes: indexes a
```{r exemplo split df graph into 2 dataframes}
# Assuming your dataframe is named "graph_data"
# DF_graph
graph_data <- data.frame(
  from = c("Amanda", "Bruno", "Carlos", "Daniel"),
  to = c("Bruno", "Carlos", "Daniel", "Amanda")
)

# Create a unique list of all nodes (vertices)
unique_nodes <- unique(c(graph_data$from, graph_data$to))

# Create a mapping between node names and indices
node_mapping <- data.frame(
  id = seq_along(unique_nodes),
  label = unique_nodes
)

# Replace node names with indices in the graph data
graph_data <- graph_data |>
  dplyr::mutate(
    from = purrr::map_chr(from, ~ node_mapping$id[node_mapping$label == .x]),
    to = purrr::map_chr(to, ~ node_mapping$id[node_mapping$label == .x])
  )

# Print the transformed graph data and node mapping
graph_data
node_mapping
```
```{r fun split_graph }
#' split a tidy graph (each line is grah with at least 2 nodes)

split_graph <- function(DF_graph) {
  # Create a unique list of all nodes (vertices)
  unique_nodes <- unique(c(DF_graph$from, DF_graph$to))

  # Create a mapping between node names and indices
  node_mapping <- data.frame(
    id = seq_along(unique_nodes),
    label = unique_nodes
  )

  # Replace node names with indices in the graph data
  graph_data <- DF_graph |>
    dplyr::mutate(
      from = purrr::map_chr(from, ~ node_mapping$id[node_mapping$label == .x] |> as.character()),
      to = purrr::map_chr(to, ~ node_mapping$id[node_mapping$label == .x] |> as.character()) |> as.character()
    )

  list_ <- list(node_mapping, graph_data)
  names(list_) <- s2v("node_mapping graph_data")
  list_
}


data.frame(
  from = c("Amanda", "Bruno", "Carlos", "Daniel"),
  to = c("Bruno", "Carlos", "Daniel", "Amanda")
) |> split_graph()
```
```{r ex gemini}
# Create the dataframe
df <- data.frame(
  col1 = c(1, 1, 1, 2, 2, 2),
  col2 = c(TRUE, FALSE, TRUE, TRUE, FALSE, FALSE),
  col3 = c(1, 2, 3, 4, 5, 6)
)

# Use `split` and `subset` to group and filter
result <- split(df$col3, df$col1)
result <- lapply(result, function(x) x[df$col2[match(x, df$col3)]])
result
```
```{r fun filtrando ppn em cada frase}
pos <- paste(txt[1:2], collapse = " ") |> spacyr::spacy_parse(, dependency = T)
pos |>
  group_seq_pos() |>
  pull(collapsed)
pos |>
  group_seq_pos() |>
  dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos)) |>
  select(pos, collapsed, noun_ppn)

filter_ppn <- function(pos_df, POS = c("NOUN", "PROPN")) {
  pos_df |>
    group_seq_pos() |>
    # dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos )) |>
    dplyr::mutate(noun_ppn = pos %in% POS) |>
    # select(sentence_id, pos, collapsed, noun_ppn) |>
    filter(noun_ppn) |>
    select(-noun_ppn) |>
    pull(collapsed) |>
    nothing()
}

#' extract proper name and nouns from POS DF
#' @params POS the POS to be extracted
filter_ppn <- function(pos_df, POS = c("NOUN", "PROPN")) {
  pos_df <- pos_df |>
    group_seq_pos() |>
    # dplyr::mutate(noun_ppn = grepl("NOUN|PROPN", pos )) |>
    dplyr::mutate(noun_ppn = pos %in% POS) |>
    # select(sentence_id, pos, collapsed, noun_ppn) |>
    filter(noun_ppn) |>
    select(-noun_ppn) |>
    nothing()

  result <- split(pos_df$collapsed, pos_df$sentence_id)
  # result <-
  # lapply(result, function(x) x[pos_df$col2[match(x, pos_df$col3)]])
  result
}

pos |> filter_ppn()
txtPT_trt_full_pos |> filter_ppn()

g_v <- filter_ppn(pos)[[1]] |>
  combn(2, simplify = F) |>
  unlist()
# separando os elementos pares dos impares do vetor em um DF
tibble(
  n1 = g_v[seq(1, length(g_v), by = 2)],
  n2 = g_v[seq(2, length(g_v), by = 2)]
)
# list_g  <-

lapply(filter_ppn(pos), \(x) {
  g_v <- combn(x, 2, simplify = F) |> unlist()

  tibble::tibble(
    n1 = g_v[seq(1, length(g_v), by = 2)],
    n2 = g_v[seq(2, length(g_v), by = 2)]
  )
}) |> bind_rows()
# lapply(list_g, \(x) { .[[x]][1] })
```
```{r fun extract_graph_pos}
extract_graph_pos <- function(pos_df, count = FALSE) {
  graph <- lapply(filter_ppn(pos_df), \(x) {
    g_v <- combn(x, 2, simplify = F) |> unlist()

    tibble::tibble( # id = x,
      n1 = g_v[seq(1, length(g_v), by = 2)],
      n2 = g_v[seq(2, length(g_v), by = 2)]
    )
  }) |>
    # mutate(id_sentence = x["sentence_id"],)
    bind_rows()

  if (count) {
    graph |> dplyr::count(n1, n2, sort = T)
  } else {
    graph
  }
}

extract_graph_pos(pos)
extract_graph_pos(pos, T)
as_tibble(txtPT_trt_full_pos) |> extract_graph_pos(F)
```
collapse by sequence 
```{r ex gemini}
cumsum(1:10)
cumprod(1:10)
cummin(c(3:1, 2:0, 4:2))
cummax(c(3:1, 2:0, 4:2))
data.frame(id=1:10, cum = cumsum(1:10))

# in R, I want to collapse a column if there is a specific sequence of elements from another column
library(dplyr)

# Sample data
df <- data.frame(
  id = 1:10,
  trigger_col = c("A", "B", "C", "A", "B", "C", "D", "A", "B", "C"),
  target_col = letters[1:10]
)

# Define the trigger sequence
trigger_seq <- c("A", "B", "C")

# Collapse the target column based on the trigger sequence
df %>% mutate(
    group_id = cumsum(lag(trigger_col, default = "") != data.table::shift(trigger_seq, n = length(trigger_seq), fill = ""))
  ) %>%
  group_by(group_id) %>%
  summarize(
    id = first(id),
    trigger_col = paste(trigger_col, collapse = ","),
    target_col = paste(target_col, collapse = ",")  # Adjust the collapse operation as needed
  )
```
```{r teste deepseek}
# in R, I want to collapse a column if there is a specific sequence of elements from another column
library(dplyr)

# Sample data
df <- data.frame(
  col1 = c("A", "B", "C", "D", "E", "F", "G", "H"),
  col2 = c(1, 2, 3, 4, 5, 6, 7, 8)
)

# Specific sequence to look for in col1
sequence <- c("C", "D", "E")

# Approach 1: Using mutate and if_else 
#df <- 
  df %>%
  mutate(collapsed_col2 = if_else(
    lag(col1, 2) %in% sequence[1] & 
      lag(col1, 1) %in% sequence[2] & 
      col1 %in% sequence[3],
    paste(lag(col2, 2), lag(col2, 1), col2, sep = "-"),
    as.character(col2)
  )) # %>% 
# Remove the original col2 if needed
#select(-col2)

print(df)
# Approach 2: Using group_by and summarize
# df <- 
df %>%
  mutate(group = cumsum(col1 == sequence[1] & lag(col1, 1) == sequence[2] & lag(col1, 2) == sequence[3])) %>%
  group_by(group) %>%
  summarize(collapsed_col2 = paste(col2, collapse = "-")) #%>% ungroup() #%>% select(-group)

print(df)
```
```{r testando adp adposition}
# https://universaldependencies.org/u/pos/ADP.html
teste <- "Estados Unidos da América foram de Vasco da Gama. A Justiça Estadual do Mato Grosso do Sul determinou que "  |> spacy_parse()
gt::gt(teste)
```






















I have a dataframe in R. One column is `entity = c(PER_B, NA ,NA, NA, PER_B, PER_I, PER_I )`. How to paste/conflate all the lines of the other column `tokens = c("Amanda","é", "amada", "por", "Joaquim", "de", "Souza")` so the new column became `new = c("Amanda","é", "amada","por", "Joaquim de Souza")`

## rollama
```{r rollama}
library(rollama)
"tinyllama"
```












