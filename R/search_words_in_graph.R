# functions to build graphs using entities co-ocurrence, from 1) text; 2) word of departure.
# Given a text, the functions here filters the sentences with the query, build the graph,
# find the N neighbours to expand

#' Tokenize and filter text by a query
#'
#' @description
#' from vector of texts, tokenize by sentence or paragraph and returns a
#' filtered list. It is useful to shrink the amount of text to be processed,
#' what can take a long time  for example, when running POS, or when expanding
#' the graph from their neighbors.
#'
#' @param txt vector of texts
#' @param query query to filter the text
#' @param ic ignore case (default TRUE)
#' @param by_sentence tokenize by sentence (default TRUE). If FALSE, tokenize by paragraph
#' @param unlist if TRUE (default FALSE), returns a vector instead of a list object.
#' @param msg if TRUE (default TRUE), returns a message if the query is not found.
#'
#' @export
#'
#' @examples
#' # loading data
#' data(package = "networds)
#' # using sample:
#' txt_wiki[2:3]
#' txt_wiki |> filter_by_query("York")
#' txt_wiki |> filter_by_query("Police")
filter_by_query <- function(
  txt,
  query,
  i_c = TRUE,
  by_sentence = TRUE,
  unlist = FALSE,
  msg = TRUE
) {
  # txt <- txt_wiki #|> paste(collapse = ". ");  query <- "Police"; i_c = TRUE
  # by_sentence = TRUE; unlist = FALSE; txt <- txt |> paste(collapse = ". ")

  if (by_sentence) {
    tokenized_txt <- txt |>
      tokenizers::tokenize_sentences()
  } else {
    tokenized_txt <- txt |>
      tokenizers::tokenize_paragraphs()
  }

  filtered_txt <- tokenized_txt |>
    lapply(\(x) grep(x = x, query, ignore.case = i_c, value = TRUE))

  length_list <- filtered_txt |>
    lapply(length) |>
    unlist() |>
    sum()

  if (length_list == 0) {
    if (msg) message('Query "', query, '" not found in given text.')
  } else {
    if (unlist) {
      unlist(filtered_txt)
    } else {
      filtered_txt
    }
  }
}


#' parse tokenized text into POS
#'
#' from a list of filtered text (from the function filter_by_query), tags the POS/ parse with spacyr.
#' It also renumber the doc_id and sentence_id. It runs spacyr::entity_extract() or spacyr::entity_consolidate() in background,
#' but also runs on lists with empty elements, and can returns a single dataframe (if `bind = TRUE`) renaming doc_id and sentence_id,
#'
#' @param txt a list of filtered text
#' @param bind if TRUE (default), returns a single dataframe. if FALSE, returns a List object
#' @param entities if TRUE (default), returns only the entities. if FALSE, returns consolidated entities
#'
#' @export
#' @examples
#' txt_wiki |>
#'   filter_by_query("Police") |>
#'   parsePOS()
#'
#' txt_wiki |>
#'   filter_by_query("Police") |>
#'   parsePOS(only_entities = FALSE)
parsePOS <- function(txt, bind = TRUE, only_entities = TRUE) {
  pos <- lapply(1:length(txt), \(x) {
    # check if list element is empty

    if (length(txt[[x]]) == 0) {
      txt[[x]]
    } else {
      parsed <- txt[[x]] |> spacyr::spacy_parse(entity = TRUE)

      if (only_entities) {
        spacyr::entity_extract(parsed)
      } else {
        spacyr::entity_consolidate(parsed) |>
          dplyr::mutate(
            sentence_id = gsub("text(.*)", "\\1", doc_id),
            doc_id = x,
            # paste0("text", x)
          )
      }
    }
  })

  if (bind) {
    non_empty <- lapply(pos, \(x) {
      length(x) > 0
    }) |>
      unlist() |>
      unname()

    pos[non_empty] |>
      dplyr::bind_rows()
  } else {
    pos
  }
}


#' Graph from word co-occurrence
#'
#' @description
#' given a dataframe with a column (list type) with POS, returns a list with three
#' elements: 1) "graphs": a tibble with the frequency of graphs;
#' 2) "isolated_nodes": a tibble with the isolated nodes, i.e. nodes without
#' any connection.
#' 3) "nodes": a tibble with the individual frequency of each node. If list
#' element has only one element, it is removed.
#'
#' @param df_cooccurrence a dataframe generated by get_pairs()
#' @param strip_rgx regex pattern to strip in the node text. Default: "^the_".
#' To erase nothing, use "".
#' @param freq if TRUE (default), returns a dataframe with frequency. If FALSE,
#' returns only the pairs without aggregation.
#'
#' @examples
#' pos <- txt_wiki |>
#'   filter_by_query("Police") |>
#'   parsePOS()
#'
#' entities_by_txt <- pos |>
#'   dplyr::group_by(doc_id) |>
#'   dplyr::summarise(entities = list(unique(entity)))
#'
#' graph_from_cooccurrence(entities_by_txt)
graph_from_cooccurrence <- function(
  df_cooccurrence,
  strip_rgx = "^the_",
  freq = TRUE
) {
  if (nrow(df_cooccurrence) == 0) stop("Empty dataframe.")

  # check colnames
  column_names <- colnames(df_cooccurrence)
  not_adequated_colnames <- !column_names %in% c("terms") |> any()

  if (not_adequated_colnames) {
    if(any(column_names %in% "entities")) {
      df_cooccurrence <- df_cooccurrence |>
        dplyr::rename(terms = entities) 
  } else {
    stop("Column 'terms' or 'entities' not found. Did it as generated with get_pairs() function?") }
  }

  node_list <- df_cooccurrence |>
    dplyr::pull(terms) |>
    as.list()

  # solitary nodes without edges: catching them
  lonely_nodes <- node_list |>
    lapply(length) |>
    unlist() == 1

  all_nodes_are_lonely <- all(lonely_nodes)

  if(all_nodes_are_lonely){  stop("All nodes are solitary. No graph pairs detected.")} 
  
  lonely_nodes <- node_list[lonely_nodes] |> unlist()

  # removing list elements that has length < 1
  list_length_correct <- node_list |>
    lapply(length) |>
    unlist() > 1

  node_list <- node_list[list_length_correct]

  # combining pairs
  comb_list <- node_list |>
    lapply(combn, 2, simplify = FALSE) |>
    unlist()

  if (strip_rgx != "") { comb_list <- gsub(x = comb_list, strip_rgx, "") }

  pairs <- tibble::tibble(
    n1 = comb_list[seq(1, length(comb_list), by = 2)],
    n2 = comb_list[seq(2, length(comb_list), by = 2)]
  )

  # all nodes
  all_nodes <- node_list |> unlist()

  # frequency
  if (freq) {
    edgesDF <- pairs |>
      dplyr::count(n1, n2) |>
      dplyr::arrange(-n) |>
      dplyr::rename(freq = n)

    isolated_nodes <-
      lonely_nodes |>
      plyr::count() |>
      dplyr::arrange(-freq) |>
      dplyr::rename(node = 1)
    # colnames(isolated_nodes)  <- c("node", "freq")

    all_nodes <-
      all_nodes |>
      plyr::count() |>
      dplyr::arrange(-freq) |>
      dplyr::rename(node = x) |>
      dplyr::as_tibble()
  } else {
    all_nodes <- all_nodes |>
      unique() |>
      dplyr::as_tibble()

    edgesDF <- pairs

    isolated_nodes <- lonely_nodes

  }

  # print result
  list(
    graphs = edgesDF,
    isolated_nodes = isolated_nodes,
    nodes = all_nodes
  )
}


#' Get graph (co-occurrence of entities)
#'
#' @description
#' from a POS dataframe (using filter_by_query() |> parsePOS()) get the pairs of entities
#'
#' @param pos a dataframe generated by filter_by_query() |> parsePOS()
#' @param loop if TRUE (default: FALSE), returns a loop (self reference) in the graph
#' @param freq if TRUE (default: TRUE), returns the count (frequency) of edges/co-occurences
#'
#' @export
#' @examples
#' x <- txt_wiki |> filter_by_query("Police")
#' x <- x |> parsePOS()
#' get_cooc_entities(x)
#'
#' # with loops /self-reference
#' get_cooc_entities(x, loop = TRUE)
#' get_cooc_entities(x, lower_case = TRUE)
get_cooc_entities <- function(
  pos,
  loop = FALSE,
  freq = TRUE,
  lower_case = FALSE
) {
  # pos <- txt_wiki |> filter_by_query("Police") |> parsePOS()

  # to erase loops (repeated value in both nodes, or self reference) in the graph
  entities_by_txt <-
    if (loop) {
      pos |>
        dplyr::group_by(doc_id) |>
        dplyr::summarise(entities = list(entity))
    } else {
      pos |>
        dplyr::group_by(doc_id) |>
        dplyr::summarise(entities = list(unique(entity)))
    }
  if (lower_case) {
    # entities_by_txt <- lapply(entities_by_txt, tolower)
    entities_by_txt <- entities_by_txt |>
      dplyr::mutate(entities = lapply(entities, tolower))
  }

  graph_from_cooccurrence(entities_by_txt, freq = freq)
}

# TODO add check parsePOS only_entities

#' get graph (co-occurrence of entities and/or grammar class)
#'
#' @description
#' from a POS dataframe (using `spacyr::spacy_parse()` or
#' `parsePOS(only_entities = FALSE)` ) get the pairs of co-occurrences.
#'
#' @param pos_df a POS dataframe
#' @param pos_cat the POS categories to be extracted. (Default: NOUN, PROPN and
#' ENTITY).
#' @param nodes the name of the text column to be used as nodes. options: 
#' "token" and "lemma" Default: "lemma".
#'
#' @export
#'
#' @examples
#' x <- txt_wiki |> filter_by_query("Police")
#' x <- x |> parsePOS(only_entities = FALSE)
#' get_cooc(x)
get_cooc <- function(
  pos_df,
  pos_cat = c("NOUN", "ENTITY", "PROPN"),
  nodes = "lemma"
) {
  col_names <- colnames(pos_df)

  if (!"pos" %in% col_names) {
    stop(
      '"pos" column not found. Did it was generated by
      "parsePOS(only_entities = FALSE)" or "spacyr::spacy_parse()"?'
    )
  }

  # pos_df  <- x |> parsePOS(only_entities = FALSE)
  pos2 <- pos_df |>
    dplyr::filter(pos %in% pos_cat) |>
    dplyr::group_by(doc_id, sentence_id) |>
    # dplyr::summarise(terms = list(token))
    # dplyr::summarise(terms = list(lemma))
    dplyr::summarise(terms = list(!!nodes))

  graph_from_cooccurrence(pos2)
}


#' Graph from text and query
#'
#' from text and a query, generates the graph dataframe of co-occurrence frequency
#'
#' @param text a vector of texts
#' @param query query to filter the text
#' @param ic ignore case (default TRUE)
#' @param by_sentence tokenize by sentence (default TRUE). If FALSE, tokenize by paragraph
#' @param loop if TRUE (default: FALSE), returns a loop (self reference) in the graph
#'
#' @export
#'
#' @examples
#' # loading data
#' data(package = "networds")
#' # sample
#' txt_wiki[2:3]
#' txt_wiki |> get_graph_from_txt("York")
#' txt_wiki |> get_graph_from_txt("Police")
get_graph_from_txt <- function(
  text,
  query,
  i_c = TRUE,
  by_sentence = TRUE,
  loop = FALSE
) {
  text |>
    filter_by_query(query = query, i_c = i_c, by_sentence = by_sentence) |>
    parsePOS() |>
    get_pairs(loop = loop)
}


