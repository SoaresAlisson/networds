[{"path":"/articles/02-Extract-entity-co-ocurrences-with-POS.html","id":"installing-spacyr","dir":"Articles","previous_headings":"","what":"Installing SpacyR","title":"02-Extract entity co-ocurrences with POS","text":"First , need install {spacyR} package. wrap around Spacy package Python, SpacyR deals boring parts creating exclusive python virtual environment. package extract NER (named entities) POS (part speech tagging). {networds} comes text sample. tutorial, use different parts text better visualization.","code":"install.packages(\"spacyr\")  # if you prefer, or maybe if the CRAN version is buggy, install the GitHub one: pak::pkg_install(\"quanteda/spacyr\")  # to Install spaCy and requirements (python env). With empty parameters, it will # install the default “en_core_web_sm” model. spacyr::spacy_install()  spacyr::spacy_initialize() data(package = \"networds\") # list the available dataset in package networds # Text_sample of the package # An example of text. Showing only the firsts lines txt_wiki[1:5] #> [1] \"Killing of Brian Thompson - Wikipedia\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            #> [2] \" \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                #> [3] \"Brian Thompson, the 50-year-old CEO of the American health insurance company UnitedHealthcare, was shot and killed in Midtown Manhattan, New York City, on December 4, 2024. The shooting occurred early in the morning outside an entrance to the New York Hilton Midtown hotel.[4] Thompson was in the city to attend an annual investors' meeting for UnitedHealth Group, the parent company of UnitedHealthcare. Prior to his death, he faced criticism for the company's rejection of insurance claims, and his family reported that he had received death threats in the past. The suspect, initially described as a white man wearing a mask, fled the scene.[1] On December 9, 2024, authorities arrested 26-year-old Luigi Mangione in Altoona, Pennsylvania, and charged him with Thompson's murder in a Manhattan court.[5][6][7]\"                                                                                                                                                                                                                                                                                                                                     #> [4] \"Authorities said Mangione was carrying a 3D-printed pistol and a 3D-printed suppressor consistent with those used in the attack, as well as a short handwritten letter to federal law enforcement (characterized as a manifesto) criticizing America's healthcare system, a U.S. passport, and multiple fraudulent IDs, including one with the same name the alleged shooter used to check into a hostel on the Upper West Side of Manhattan.[8][9][10] Authorities also said his fingerprints matched those that investigators found near the New York shooting scene.[11] Mangione was held without bail in Pennsylvania on charges of possession of an unlicensed firearm, forgery, and providing false New Jersey-resident identification to police.[12] Mangione also has an arrest warrant with five felony counts in New York, including second-degree murder.[13] Mangione's lawyer said he will plead not guilty to the charges.[12] Police believe that he was inspired by Ted Kaczynski's essay Industrial Society and Its Future (1995), and motivated by his personal views on health insurance.[14][15] They say an injury he suffered may have played a part.[16]\" #> [5] \"Online and social media reactions to the killing ranged from contempt and mockery toward Thompson and UnitedHealth Group, to sympathy and praise for the assailant. More broadly, social media users criticized the U.S. healthcare system, and many users characterized the killing as deserved or justified. These attitudes were related to anger over UnitedHealth's business practices and those of the United States health insurance industry in general – primarily the strategy to deny coverage to clients. In particular, Thompson's death was compared to the harm or death experienced by clients who were denied coverage by insurance companies. Some public officials expressed dismay and offered condolences to Thompson's family. Inquiries about protective services and security for CEOs and corporate executives surged following the killing. \"  POS <- txt_wiki |> spacyr::spacy_parse() #> successfully initialized (spaCy Version: 3.8.5, language model: en_core_web_sm) # parsing the POS tagging in it"},{"path":"/articles/02-Extract-entity-co-ocurrences-with-POS.html","id":"extracting-entities","dir":"Articles","previous_headings":"","what":"Extracting entities","title":"02-Extract entity co-ocurrences with POS","text":"package {spacyr} two functions useful section. conflate compound nouns, like “New” “York” “New_York”. first one, extract entities second one, conflates compound nouns preserve POS tags. functions used inside {networds}. Lets use package text example. function {networds}, possible give whole text input, search term/query. package tokenize sentences (paragraphs, specified parameters), perform POS tagging, extract graph. possible run whole process go step step, understand going . First, ’ll first option. text worked great, possible extract co-occurrences work, comes costs. can easily escalate lot size many times origial text size, take lot time computational costs. example, 345Mb text can become 15Gb POS tagged text. , another approach build co-occurrences wisely, departing specific words, processing text matters. function filter_by_query() tokenize text sentence default (use paragraph instead, use parameter by_sentence = FALSE). possible return vector instead list object next step POS tagging using parsePOS(). next step get graph using get_cooc_entities() Visualizing graph. can done function q_plot(), quicker plot, also less customization options.  better control features graph, plot_pos_graph() gives options. size dots shows frequency term. thickness edges shows often co occurrence nodes. text used small, huge differences visible. opted maintain words size matter .  viz function based {ggraph}, based {ggplot2}. possible customize even .  Plotting interactive graph (nodes can become crazy dance find best distance )  get graph entities nouns:","code":"POS |>   spacyr::entity_extract() |>   dplyr::as_tibble() # use tibble to better visualize #> # A tibble: 284 × 4 #>    doc_id sentence_id entity                      entity_type #>    <chr>        <int> <chr>                       <chr>       #>  1 text1            1 Brian_Thompson_-_Wikipedia  PERSON      #>  2 text3            1 Brian_Thompson              PERSON      #>  3 text3            1 American                    NORP        #>  4 text3            1 UnitedHealthcare            ORG         #>  5 text3            1 Midtown_Manhattan           GPE         #>  6 text3            1 New_York_City               GPE         #>  7 text3            2 the_New_York_Hilton_Midtown ORG         #>  8 text3            3 Thompson                    PERSON      #>  9 text3            3 UnitedHealth_Group          ORG         #> 10 text3            3 UnitedHealthcare            ORG         #> # ℹ 274 more rows POS |>   spacyr::entity_consolidate() |>   dplyr::as_tibble() # use tibble to better visualize #> # A tibble: 4,641 × 7 #>    doc_id sentence_id token_id token                     lemma pos   entity_type #>    <chr>        <int>    <dbl> <chr>                     <chr> <chr> <chr>       #>  1 text1            1        1 \"Killing\"                 \"Kil… PROPN \"\"          #>  2 text1            1        2 \"of\"                      \"of\"  ADP   \"\"          #>  3 text1            1        3 \"Brian_Thompson_-_Wikipe… \"Bri… ENTI… \"PERSON\"    #>  4 text2            1        1 \" \"                       \" \"   SPACE \"\"          #>  5 text3            1        1 \"Brian_Thompson\"          \"Bri… ENTI… \"PERSON\"    #>  6 text3            1        2 \",\"                       \",\"   PUNCT \"\"          #>  7 text3            1        3 \"the\"                     \"the\" DET   \"\"          #>  8 text3            1        4 \"50_-_year_-_old\"         \"50_… ENTI… \"DATE\"      #>  9 text3            1        5 \"CEO\"                     \"ceo\" NOUN  \"\"          #> 10 text3            1        6 \"of\"                      \"of\"  ADP   \"\"          #> # ℹ 4,631 more rows POS |> group_ppn() #> # A tibble: 4,975 × 8 #> # Groups:   name [1,650] #>    doc_id sentence_id token_id token       lemma       pos   entity     name     #>    <chr>        <int>    <int> <chr>       <chr>       <chr> <chr>      <chr>    #>  1 text1            1        1 \"Killing\"   \"Killing\"   PROPN \"\"         \"Killin… #>  2 text1            1        2 \"of\"        \"of\"        ADP   \"\"         \"of Bri… #>  3 text1            1        3 \"Brian\"     \"Brian\"     PROPN \"PERSON_B\" \"of Bri… #>  4 text1            1        4 \"Thompson\"  \"Thompson\"  PROPN \"PERSON_I\" \"of Bri… #>  5 text1            1        5 \"-\"         \"-\"         PUNCT \"PERSON_I\" \"- Wiki… #>  6 text1            1        6 \"Wikipedia\" \"Wikipedia\" PROPN \"PERSON_I\" \"- Wiki… #>  7 text2            1        1 \" \"         \" \"         SPACE \"\"         \"  Bria… #>  8 text3            1        1 \"Brian\"     \"Brian\"     PROPN \"PERSON_B\" \"  Bria… #>  9 text3            1        2 \"Thompson\"  \"Thompson\"  PROPN \"PERSON_I\" \"  Bria… #> 10 text3            1        3 \",\"         \",\"         PUNCT \"\"         \",\"      #> # ℹ 4,965 more rows # tokenizing in sentences and filtering three lines that contains the word \"police\" x <- txt_wiki[3:6] |> filter_by_query(\"Police\") x #> [[1]] #> character(0) #>  #> [[2]] #> [1] \"11] Mangione was held without bail in Pennsylvania on charges of possession of an unlicensed firearm, forgery, and providing false New Jersey-resident identification to police.[\" #> [2] \"12] Police believe that he was inspired by Ted Kaczynski's essay Industrial Society and Its Future (1995), and motivated by his personal views on health insurance.[\"              #>  #> [[3]] #> character(0) #>  #> [[4]] #> character(0) class(x) #> [1] \"list\" x <- txt_wiki[1:12] |> filter_by_query(\"Police\", unlist = TRUE) x #> [1] \"11] Mangione was held without bail in Pennsylvania on charges of possession of an unlicensed firearm, forgery, and providing false New Jersey-resident identification to police.[\" #> [2] \"12] Police believe that he was inspired by Ted Kaczynski's essay Industrial Society and Its Future (1995), and motivated by his personal views on health insurance.[\"              #> [3] \"39] According to the police, he then left the city from the George Washington Bridge Bus Station farther uptown in Upper Manhattan.[\" class(x) #> [1] \"character\" txt_wiki[1:12] |>   filter_by_query(\"Police\", unlist = TRUE) |>   parsePOS() #>   doc_id sentence_id                                   entity entity_type #> 1  text1           1                             Pennsylvania         GPE #> 2  text1           1                               New_Jersey         GPE #> 3  text1           1                         Ted_Kaczynski_'s      PERSON #> 4  text1           1        Industrial_Society_and_Its_Future         ORG #> 5  text1           2 the_George_Washington_Bridge_Bus_Station         ORG #> 6  text1           2                          Upper_Manhattan         LOC x <- txt_wiki[2:44] |>   filter_by_query(\"Police\") |>   parsePOS()  x |> dplyr::as_tibble() # use tibble to better visualize #> # A tibble: 26 × 4 #>    doc_id sentence_id entity                                   entity_type #>    <chr>        <int> <chr>                                    <chr>       #>  1 text1            1 Pennsylvania                             GPE         #>  2 text1            1 New_Jersey                               GPE         #>  3 text2            1 Ted_Kaczynski_'s                         PERSON      #>  4 text2            1 Industrial_Society_and_Its_Future        ORG         #>  5 text1            2 the_George_Washington_Bridge_Bus_Station ORG         #>  6 text1            2 Upper_Manhattan                          LOC         #>  7 text1            1 Central_Park                             LOC         #>  8 text1            1 New_York_City                            GPE         #>  9 text1            2 Mangione                                 ORG         #> 10 text1            2 the_San_Francisco_Police_Department      ORG         #> # ℹ 16 more rows  g <- get_cooc_entities(x)  g #> $graphs #> # A tibble: 88 × 3 #>    n1               n2                                 freq #>    <chr>            <chr>                             <int> #>  1 Ted_Kaczynski_'s Industrial_Society_and_Its_Future     2 #>  2 Altoona          Industrial_Society_and_Its_Future     1 #>  3 Altoona          McDonald                              1 #>  4 Altoona          Ted_Kaczynski_'s                      1 #>  5 Central_Park     Altoona                               1 #>  6 Central_Park     Industrial_Society_and_Its_Future     1 #>  7 Central_Park     Mangione                              1 #>  8 Central_Park     McDonald                              1 #>  9 Central_Park     New_York_City                         1 #> 10 Central_Park     San_Francisco                         1 #> # ℹ 78 more rows #>  #> $isolated_nodes #>       node freq #> 1 American    1 #>  #> $nodes #> # A tibble: 18 × 2 #>    node                                      freq #>    <chr>                                    <int> #>  1 Industrial_Society_and_Its_Future            2 #>  2 New_Jersey                                   2 #>  3 Ted_Kaczynski_'s                             2 #>  4 Altoona                                      1 #>  5 American                                     1 #>  6 Central_Park                                 1 #>  7 Joseph_Kenny                                 1 #>  8 Mangione                                     1 #>  9 Manhattan                                    1 #> 10 McDonald                                     1 #> 11 New_York                                     1 #> 12 New_York_City                                1 #> 13 NYPD                                         1 #> 14 Pennsylvania                                 1 #> 15 San_Francisco                                1 #> 16 the_George_Washington_Bridge_Bus_Station     1 #> 17 the_San_Francisco_Police_Department          1 #> 18 Upper_Manhattan                              1 g |> q_plot() graph_wiki <- txt_wiki[2:44] |>   filter_by_query(\"Police\") |>   parsePOS() |>   get_cooc_entities()  plot_pos_graph(graph_wiki) # TODO estava dando erro plot_pos_graph(graph_wiki,   font_size = 1.3,   edge_color = \"tomato\",   point_color = \"aquamarine4\" ) +   ggplot2::labs(     title = \"Wordnetwork of Nouns in a Wikipedia text\",     caption = \"The size of dots shows the frequency of the term.\"   ) graph_wiki$graphs |> interactive_graph() graph <- txt_wiki[2:44] |>   filter_by_query(\"Brian\") |>   parsePOS() |>   get_cooc_entities()  plot_pos_graph(graph) graph$graphs |> interactive_graph() graph_ppn <- filter_by_query(txt_wiki[2:44], \"Police\") |>   parsePOS(only_entities = FALSE) |>   dplyr::filter(entity_type != \"CARDINAL\") |> # to clean the graph   dplyr::mutate(token = gsub(\"Police\", \"police\", token)) |> # normalize the term \"police\"   get_cooc()  graph_ppn #> $graphs #> # A tibble: 401 × 3 #>    n1     n2          freq #>    <chr>  <chr>      <int> #>  1 police Mangione       3 #>  2 3D     Manhattan      2 #>  3 3D     New_Jersey     2 #>  4 3D     claim          2 #>  5 3D     driver         2 #>  6 3D     hostel         2 #>  7 3D     license        2 #>  8 3D     name           2 #>  9 3D     one            2 #> 10 3D     police         2 #> # ℹ 391 more rows #>  #> $isolated_nodes #> [1] node #> <0 rows> (or 0-length row.names) #>  #> $nodes #> # A tibble: 85 × 2 #>    node                               freq #>    <chr>                             <int> #>  1 police                               13 #>  2 Mangione                              6 #>  3 shooter                               4 #>  4 3D                                    2 #>  5 city                                  2 #>  6 Industrial_Society_and_Its_Future     2 #>  7 motive                                2 #>  8 New_Jersey                            2 #>  9 Pennsylvania                          2 #> 10 suspect                               2 #> # ℹ 75 more rows interactive_graph(graph_ppn$graphs) graph_ppn |> plot_pos_graph()"},{"path":"/articles/02-Extract-entity-co-ocurrences-with-POS.html","id":"sotu-example","dir":"Articles","previous_headings":"","what":"Sotu example","title":"02-Extract entity co-ocurrences with POS","text":"Using data {SOTU} package, United States Presidential State Union Addresses.","code":""},{"path":"/articles/02-Extract-entity-co-ocurrences-with-POS.html","id":"using-another-languages","dir":"Articles","previous_headings":"","what":"Using another languages","title":"02-Extract entity co-ocurrences with POS","text":"{networds} uses spacy tag words, user must initialize model language. default, spacy loads English model. model previously used, change model, necessary end loaded model spacyr::spacy_finalize() load new model. example, load Portuguese model, use spacyr::spacy_initialize(model = \"pt_core_news_lg\"). just started package library(networds), can simply run spacy_initialize(model = \"pt_core_news_lg\") console. download language model, check models available spacy.io/usage/models Ex.: installing portuguese, model available :","code":"modelsPT <- c(\"pt_core_news_sm\", \"pt_core_news_md\", \"pt_core_news_lg\")  # installing the bigger model spacyr::spacy_download_langmodel(modelsPT[3])"},{"path":"/articles/entities_and_relation_extraction.html","id":"entity-extraction-and-its-relations","dir":"Articles","previous_headings":"","what":"Entity extraction and its relations","title":"01 - Entity extraction with regex","text":"package {networds} provides set tools extract entities relations text. first tools uses one simples form: using rule based approach. contains rule based extraction algorithm rule based relation extraction algorithm.   Another similar package textnets, Chris Bail. capture proper names using UDpipe, plot word networks calculates centrality/betweeness words network. extract entities based Part speech tagging, see functions package, functions {networds} work well like NER, think , situations, can better job traditional joining unigram, bigrams, trigrams . rule based approach, simple use, need less dependencies runs also fast (maybe less slower). also requires lot post-cleaning, absolute control words extracted words rejected. Natural Language Processing, find proper names terms frequently appears together called “collocation”, e.g., find “United Kingdom”. can learn collocation statistical details r function article, also possible functions like quanteda.textstats::textstat_collocations(), TextForecast::get_collocations() identify , also require lot data cleaning, specially want proper names.","code":""},{"path":"/articles/entities_and_relation_extraction.html","id":"how-it-works","dir":"Articles","previous_headings":"","what":"How it works?","title":"01 - Entity extraction with regex","text":"function captures words : begins uppercase, followed uppercases, lowercase numbers, without white space can contain symbols like _, - .. way, words like “Covid-19” captured. user can specify connector, like “”“” , words like “United States America” also captured languages English Portuguese, extracts proper names. German, also extracts nouns. trade-, course. capture lot undesired words demand posterior cleaning, like: - contain sort built-classification. - “Obama Chief Staff Rahm Emanuel” captured one entity, wrong et al, maybe expect.","code":""},{"path":"/articles/entities_and_relation_extraction.html","id":"the-downsides","dir":"Articles","previous_headings":"","what":"The downsides","title":"01 - Entity extraction with regex","text":"capture: entities begin lowercase. delves words ambiguity. example, “” referring question World Health Organization? Washington person place? overthrow problemas, networds set functions works Part Speech tagging Named Entity Recognition. problems enumerated big problem case, take look next session “Extract entity co-ocurrences POS”.","code":""},{"path":"/articles/entities_and_relation_extraction.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"01 - Entity extraction with regex","text":"experience, approach section using regex works better certain types text others. well formatted text, like Books, formal articles can good option works well function. Text social media, lacks formalities language lot typos, lacking uppercase sometimes, written uppercase, approach work well.","code":""},{"path":"/articles/entities_and_relation_extraction.html","id":"using-networds-extracting-entities-with-regex","dir":"Articles","previous_headings":"","what":"Using networds: extracting entities with regex","title":"01 - Entity extraction with regex","text":"installing package (see Readme), load : , let’s extract proper names simple text: possible use languages, specifying parameter connectors using function connectors(lang). Checking connectors: Using languages: suppressWarnings(","code":"library(networds) \"John Does lives in New York in United States of America.\" |> extract_entity() #> [1] \"John Does\"                \"New York\"                 #> [3] \"United States of America\" connectors(\"eng\") #> [1] \"of\"     \"of the\" # or you can also use for english, to get the same result: connectors(\"en\") #> [1] \"of\"     \"of the\" # For portuguese connectors(\"pt\") #> [1] \"da\"  \"das\" \"de\"  \"do\"  \"dos\" # to get the same result: connectors(\"port\") #> [1] \"da\"  \"das\" \"de\"  \"do\"  \"dos\"  # by default, the functions uses the parameter \"misc\". meaning \"miscellaneous\". connectors(\"misc\") #> [1] \"of\"     \"the\"    \"of the\" \"von\"    \"van\"    \"del\" \"João Ninguém mora em São José do Rio Preto. Ele esteve antes em Sergipe\" |>   extract_entity(connect = connectors(\"pt\")) #> [1] \"João Ninguém\"          \"São José do Rio Preto\" \"Ele\"                   #> [4] \"Sergipe\"  vonNeumann_txt <- \"John von Neumann (/vɒn ˈnɔɪmən/ von NOY-mən; Hungarian: Neumann János Lajos [ˈnɒjmɒn ˈjaːnoʃ ˈlɒjoʃ]; December 28, 1903 – February 8, 1957) was a Hungarian and American mathematician, physicist, computer scientist and engineer\" vonNeumann_txt |> extract_entity() #> [1] \"John von Neumann\"    \"NOY-\"                \"Hungarian\"           #> [4] \"Neumann János Lajos\" \"December\"            \"February\"            #> [7] \"Hungarian\"           \"American\""},{"path":"/articles/entities_and_relation_extraction.html","id":"extracting-a-graph","dir":"Articles","previous_headings":"","what":"Extracting a graph","title":"01 - Entity extraction with regex","text":"possible extract graph extracted entities. First, happens tokenization sentence paragraph. , entities extracted using extract_entity(). data frame co-occurrence words sentences paragraph build. One parameters sw means “stopwords”. possible add vector stopwords. process can take run text/corpus big. , interested words, first , filter sentences/paragraphs desired words, , extract graph. Seeing another example, extracting Wikipedia article: now extracting graphs: plot wordcloud network necessary two parameters: original text dataframe/tibble returned dplyr::count(), three columns: node 1, node 2 weight/frequency.  different information graph: size words compound words means individual frequency word/compound word thickness links indicates often pair occur together. opted use approach two parameters data frame weights, well original text plotting networks matter readability, often requires select frequent. word individual frequency necessarily correlated ’s frequency graphs, function calculates individual frequency word. Looking frequency dataframe, user may want strip graphs, plot . function uses {ggraph} ggplot. , can change ggplot add another ones posteriori. plot interactive graph, possible use {networkD3}: Another text example.  plot interactive graph, possible use {networkD3}:","code":"vonNeumann_txt |> extract_graph() #> Tokenizing by sentences #> # A tibble: 27 × 2 #>    n1               n2                  #>    <chr>            <chr>               #>  1 John von Neumann NOY-                #>  2 John von Neumann Hungarian           #>  3 John von Neumann Neumann János Lajos #>  4 John von Neumann December            #>  5 John von Neumann February            #>  6 John von Neumann Hungarian           #>  7 John von Neumann American            #>  8 NOY-             Hungarian           #>  9 NOY-             Neumann János Lajos #> 10 NOY-             December            #> # ℹ 17 more rows my_sw <- c(stopwords::stopwords(   language = \"en\",   source = \"snowball\", simplify = TRUE ), \"lol\")  vonNeumann_txt |> extract_graph(sw = my_sw) #> Tokenizing by sentences #> # A tibble: 27 × 2 #>    n1               n2                  #>    <chr>            <chr>               #>  1 John von Neumann NOY-                #>  2 John von Neumann Hungarian           #>  3 John von Neumann Neumann János Lajos #>  4 John von Neumann December            #>  5 John von Neumann February            #>  6 John von Neumann Hungarian           #>  7 John von Neumann American            #>  8 NOY-             Hungarian           #>  9 NOY-             Neumann János Lajos #> 10 NOY-             December            #> # ℹ 17 more rows page <- \"https://en.wikipedia.org/wiki/GNU_General_Public_License\" |> rvest::read_html() text <- page |>   rvest::html_nodes(\"p\") |>   rvest::html_text()  # looking at the scraped text: text[1:2] # seeing the head of the text #> [1] \"\\n\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          #> [2] \"The GNU General Public Licenses (GNU GPL or simply GPL) are a series of widely used free software licenses, or copyleft licenses, that guarantee end users the freedoms to run, study, share, and modify the software.[7] The GPL was the first copyleft license for general use. It was originally written by Richard Stallman, the founder of the Free Software Foundation (FSF), for the GNU Project. The license grants the recipients of a computer program the rights of the Free Software Definition.[8] The licenses in the GPL series are all copyleft licenses, which means that any derivative work must be distributed under the same or equivalent license terms. It is more restrictive than the Lesser General Public License and even further distinct from the more widely-used permissive software licenses such as BSD, MIT, and Apache.\\n\" g <- text |> extract_graph(sw = my_sw) #> Tokenizing by sentences g #> # A tibble: 3,239 × 2 #>    n1                              n2                            #>    <chr>                           <chr>                         #>  1 The GNU General Public Licenses GNU GPL                       #>  2 The GNU General Public Licenses GPL                           #>  3 The GNU General Public Licenses The GPL                       #>  4 The GNU General Public Licenses Richard Stallman              #>  5 The GNU General Public Licenses Free Software Foundation      #>  6 The GNU General Public Licenses FSF                           #>  7 The GNU General Public Licenses GNU Project                   #>  8 The GNU General Public Licenses Free Software Definition      #>  9 The GNU General Public Licenses GPL                           #> 10 The GNU General Public Licenses Lesser General Public License #> # ℹ 3,229 more rows g_N <- g |> dplyr::count(n1, n2, sort = T) g_N #> # A tibble: 2,100 × 3 #>    n1           n2              n #>    <chr>        <chr>       <int> #>  1 GPLv3        GPLv2          24 #>  2 Artifex      Hancom         19 #>  3 Ghostscript  Hancom         19 #>  4 Linux        GPL            16 #>  5 GPL          FSF            14 #>  6 GPL-licensed GPL            14 #>  7 GPL          GPLv3          13 #>  8 Artifex      Ghostscript    12 #>  9 However      GPL            12 #> 10 FSF          GPL            11 #> # ℹ 2,090 more rows net_wordcloud(text, g_N) #> Warning: Unknown or uninitialised column: `n`. g_N |>   head(100) |> # to reduce the amount of nodes and edges in the graph   networkD3::simpleNetwork(     height = \"10px\", width = \"30px\",     linkDistance = 50,     fontSize = 16   ) page <- \"https://en.wikipedia.org/wiki/Hurricane_Milton\" |> rvest::read_html() text <- page |>   rvest::html_nodes(\"p\") |>   rvest::html_text()  text[1:2] # seeing the head of the tex #> [1] \"\\n\\n\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 #> [2] \"Hurricane Milton was an extremely powerful and destructive tropical cyclone which became the second-most intense Atlantic hurricane ever recorded over the Gulf of Mexico, behind only Hurricane Rita in 2005. Milton made landfall on the west coast of the U.S. state of Florida, less than two weeks after Hurricane Helene devastated the state's Big Bend region.[2] The thirteenth named storm, ninth hurricane, fourth major hurricane, and second Category 5 hurricane of the 2024 Atlantic hurricane season, Milton is the strongest tropical cyclone to occur worldwide in 2024 thus far.[3]\" g <- text |> extract_graph(sw = my_sw) #> Tokenizing by sentences # option 1: use counting the edge frequency g_N <- g |> dplyr::count(n1, n2, sort = T) # option 2: use count_graphs function, same results, but using data.table (quicker) and erasing loops g_N <- g |> count_graphs()  net_wordcloud(text, g_N, head_n = 50) #> Warning: Unknown or uninitialised column: `n`. g_N |>   head(100) |> # to reduce the amount of nodes and edges in the graph   networkD3::simpleNetwork(     height = \"10px\", width = \"30px\",     linkDistance = 50,     fontSize = 16   )"},{"path":"/articles/entities_and_relation_extraction.html","id":"using-state-of-the-union-data","dir":"Articles","previous_headings":"","what":"Using State of the Union data","title":"01 - Entity extraction with regex","text":"Using package SOTU, contains State Union Addresses. : Checking Obama speech first year first mandate plot ’ll use another function, plot_graph2. works differently net_wordcloud. word frequencies can vary significantly, differences text size can substantial. Therefore, instead adjusting text size, vary dot/node size, ensuring text remains consistently sized maintains readability.  Checking Trump speech first year first mandate  Now, comparing speeches certain topic","code":"\"is an annual message delivered by the president of the United States to a joint session of the United States Congress near the beginning of most calendar years on the current condition of the nation. The speech generally includes reports on the nation's budget, economy, news, agenda, progress, achievements and the president's priorities and legislative proposals.\" library(sotu) #  text examples of US presidents speeches  # checking the DF with the speeches tibble::as_tibble(sotu_meta) #> # A tibble: 240 × 6 #>        X president          year years_active party       sotu_type #>    <int> <chr>             <int> <chr>        <chr>       <chr>     #>  1     1 George Washington  1790 1789-1793    Nonpartisan speech    #>  2     2 George Washington  1790 1789-1793    Nonpartisan speech    #>  3     3 George Washington  1791 1789-1793    Nonpartisan speech    #>  4     4 George Washington  1792 1789-1793    Nonpartisan speech    #>  5     5 George Washington  1793 1793-1797    Nonpartisan speech    #>  6     6 George Washington  1794 1793-1797    Nonpartisan speech    #>  7     7 George Washington  1795 1793-1797    Nonpartisan speech    #>  8     8 George Washington  1796 1793-1797    Nonpartisan speech    #>  9     9 John Adams         1797 1797-1801    Federalist  speech    #> 10    10 John Adams         1798 1797-1801    Federalist  speech    #> # ℹ 230 more rows # checking what are the speeches of Obama sotu_meta |>   dplyr::filter(     grepl(\"Obama\", president, ignore.case = T),     grepl(\"2009\", years_active)   ) #>     X    president year years_active      party sotu_type #> 1 229 Barack Obama 2009    2009-2013 Democratic    speech #> 2 230 Barack Obama 2010    2009-2013 Democratic    speech #> 3 231 Barack Obama 2011    2009-2013 Democratic    speech #> 4 232 Barack Obama 2012    2009-2013 Democratic    speech  # Picking this speech of his first year text_sotu <- sotu_text[229] |>   paste(collapse = \" \") # turning the vector into a single element str(text_sotu) # first lines of the text #>  chr \"Madam Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States--she's around here \"| __truncated__  # As a matter of curiosity, checking the most frequent entities text_sotu |>   extract_entity(sw = my_sw) |>   plyr::count() |>   dplyr::arrange(-freq) |>   head(30) #>                           x freq #> 1                  American   25 #> 2                   America   16 #> 3                 Americans   14 #> 4                       Now   13 #> 5                  Congress    9 #> 6                    Nation    6 #> 7                   Chamber    5 #> 8                Government    4 #> 9                      Iraq    4 #> 10                Democrats    3 #> 11                 Medicare    3 #> 12                President    3 #> 13              Republicans    3 #> 14 United States of America    3 #> 15              Afghanistan    2 #> 16                  Already    2 #> 17                    Given    2 #> 18                      God    2 #> 19                 Laughter    2 #> 20          Social Security    2 #> 21                    Thank    2 #> 22              Wall Street    2 #> 23                     Well    2 #> 24                 Al Qaida    1 #> 25                    Along    1 #> 26        American Recovery    1 #> 27                    April    1 #> 28                  Cabinet    1 #> 29                     CEOs    1 #> 30                    China    1  sotu_g_Ob <- text_sotu |>   paste(collapse = \" \") |>   extract_graph(sw = my_sw) #> Tokenizing by sentences  count_graph_ob <- dplyr::count(sotu_g_Ob,   n1, n2,   sort = T )  count_graph_ob #> # A tibble: 5,307 × 3 #>    n1        n2            n #>    <chr>     <chr>     <int> #>  1 American  America     219 #>  2 Now       American    193 #>  3 America   American    181 #>  4 American  Americans   176 #>  5 Americans American    174 #>  6 Congress  American    146 #>  7 American  Now         132 #>  8 Now       America     131 #>  9 Americans America     116 #> 10 Now       Americans   110 #> # ℹ 5,297 more rows plot_graph2(   sotu_g_Ob,   count_graph_ob,   head_n = 70,   edge_color = \"blue\", edge_alpha = 0.1,   text_size = 10,   scale_graph = \"log2\" ) +   ggplot2::labs(title = \"Obama SOTU - First Year\") #> Warning in ggraph::geom_node_point(ggplot2::aes(size = #> eval(dplyr::sym(scale_graph))(freqPPN$freq)), : Ignoring unknown #> parameters: `repel` # Trump, first Mandate sotu_meta |>   dplyr::filter(grepl(\"Trump\", president, ignore.case = T)) #>     X    president year years_active      party sotu_type #> 1 237 Donald Trump 2017    2016-2020 Republican    speech #> 2 238 Donald Trump 2018    2016-2020 Republican    speech #> 3 239 Donald Trump 2019    2016-2020 Republican    speech #> 4 240 Donald Trump 2020    2016-2020 Republican    speech  sotu_g_Tr <- sotu_text[237] |>   paste(collapse = \" \") |>   extract_graph(sw = my_sw) #> Tokenizing by sentences  #  the most frequent entities sotu_g_Tr |>   extract_entity(sw = my_sw) |>   plyr::count() |>   dplyr::arrange(-freq) |>   head(30) #> Warning in stri_extract_all_regex(string, pattern, simplify = simplify, : #> argument is not an atomic vector; coercing #>                        x freq #> 1               American 9019 #> 2                America 8736 #> 3              Americans 4875 #> 4                 Nation 3619 #> 5          United States 2979 #> 6                  Thank 2331 #> 7               Congress 2004 #> 8                  Megan 2004 #> 9                   Ryan 2004 #> 10            Government 1675 #> 11             Obamacare 1675 #> 12              Laughter 1344 #> 13             Democrats 1011 #> 14               Finally 1011 #> 15                   God 1011 #> 16                 Jenna 1011 #> 17           Republicans 1011 #> 18                 Today 1011 #> 19               Tonight 1011 #> 20             According  676 #> 21              Applause  676 #> 22                Canada  676 #> 23               Denisha  676 #> 24 Department of Justice  676 #> 25                Jamiel  676 #> 26           Jamiel Shaw  676 #> 27         Jessica Davis  676 #> 28                Jewish  676 #> 29               Joining  676 #> 30               Justice  676  plot_graph2(   sotu_g_Tr,   dplyr::count(sotu_g_Tr, n1, n2, sort = T),   head_n = 70,   edge_color = \"red\",   edge_alpha = 0.3,   scale_graph = \"log2\",   text_size = 10, ) +   ggplot2::labs(title = \"Trump SOTU - First Year\") #> Warning in ggraph::geom_node_point(ggplot2::aes(size = #> eval(dplyr::sym(scale_graph))(freqPPN$freq)), : Ignoring unknown #> parameters: `repel` # a regex to capture some words/patterns term <- \"\\\\bChin|Beijing|Shanghai|\\\\bXi\\\\b|Jinping\" term_ <- \"China\"  # Get all Obama speeches of his first mandate text_sotu_Ob <- sotu_text[229:234] |>   filter_by_query(term)  sotu_g_Ob <- text_sotu_Ob |>   paste(collapse = \" \") |>   extract_graph(sw = my_sw) #> Tokenizing by sentences  g_Ob <- plot_graph2(   sotu_g_Ob,   dplyr::count(sotu_g_Ob, n1, n2, sort = T),   edge_color = \"blue\",   edge_alpha = 0.1,   text_size = 5,   scale_graph = \"log2\" ) +   # ggplot2::labs(title= paste(\"Obama about\", term))   ggplot2::labs(title = \"Obama\") #> Warning in ggraph::geom_node_point(ggplot2::aes(size = #> eval(dplyr::sym(scale_graph))(freqPPN$freq)), : Ignoring unknown #> parameters: `repel`   # Trump text_sotu_Tr <- sotu_text[237:240] |>   filter_by_query(term)  sotu_g_Tr <- text_sotu_Tr |>   paste(collapse = \" \") |>   extract_graph(sw = my_sw) #> Tokenizing by sentences  g_Tr <- plot_graph2(   sotu_g_Tr,   dplyr::count(sotu_g_Tr, n1, n2, sort = T),   edge_color = \"red\",   edge_alpha = 0.2,   text_size = 5,   scale_graph = \"log2\" ) +   # ggplot2::labs(title= paste(\"Trump about\", term))   ggplot2::labs(title = \"Trump\") #> Warning in ggraph::geom_node_point(ggplot2::aes(size = #> eval(dplyr::sym(scale_graph))(freqPPN$freq)), : Ignoring unknown #> parameters: `repel`  # Joining the graphs library(patchwork) (g_Ob + g_Tr) +   plot_annotation(     title =       \"Coocurrence of terms related to China\"   )"},{"path":"/articles/entities_and_relation_extraction.html","id":"substitutions-replacing-node-text","dir":"Articles","previous_headings":"","what":"Substitutions: replacing node text","title":"01 - Entity extraction with regex","text":"extracting graphs unstructured text, synonyms appear. replace text nodes dataframe, use function graph_subs:","code":"# a test tibble test_graph <- tibble::tibble(   n1 = c(\"A\", \"B\", \"A\", \"C\", \"B\", \"Ab\", \"A\", \"D\"),   n2 = c(\"B\", \"Ab\", \"B\", \"D\", \"C\", \"A\", \"C\", \"D\") # Includes a loop (D-D) )  # dataframe with substitutions DF_substitution <- tibble::tribble(   ~col1, ~col2,   \"B\", \"blah\",   \"C\", \"Capybara\" )  # Doing the substitutions test_graph |>   graph_subs(DF_substitution) #> # A tibble: 8 × 2 #>   n1       n2       #>   <chr>    <chr>    #> 1 A        blah     #> 2 blah     Ab       #> 3 A        blah     #> 4 Capybara D        #> 5 blah     Capybara #> 6 Ab       A        #> 7 A        Capybara #> 8 D        D"},{"path":"/articles/entities_and_relation_extraction.html","id":"final-remarks","dir":"Articles","previous_headings":"","what":"Final remarks","title":"01 - Entity extraction with regex","text":"mentioned, approach based text patterns (regex) limitations. advise test different head_n, increase number nodes, polluted, decrease . can helps see look text infer meaning. possible use grammar classification get precise results. See next session, “Extract entity co occurrences POS”.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Alisson Soares. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Soares (2025). networds: extract network text. R package version 0.1.0, https://github.com/SoaresAlisson/networds.","code":"@Manual{,   title = {networds: extract network from text},   author = {Alisson Soares},   year = {2025},   note = {R package version 0.1.0},   url = {https://github.com/SoaresAlisson/networds}, }"},{"path":[]},{"path":"/index.html","id":"networds---a-package-to-build-graphs-from-text","dir":"","previous_headings":"","what":"{networds} - a package to build graphs from text","title":"extract network from text","text":"PACKAGE NOW DEVELOPMENT","code":""},{"path":"/index.html","id":"extracting-co-occurences-and-relations-in-text","dir":"","previous_headings":"","what":"Extracting co-occurences and relations in text","title":"extract network from text","text":"package extract graphs build visualize text networks static dynamic graphs. extract graphs plain text using: Rule based: Regex extract proper names, build co-occurrence network (development) Extraction using Part Speech tagging proper names nouns co-occurrence extraction relations (verbs, cases) like {rsyntax} {semgram}, uses Universal Stanford Dependencies: cross-linguistic typology “propose improved taxonomy capture grammatical relations across languages, including morphologically rich ones” (development) Relation extraction using Large Language Models running locally {rollama}. method 1 quick easy understand explain, limitations. Method 2 3 still development powerful, can solve complex problems. One problems disambiguation, word can different meanings depending context. problem, can solved methods two three, possible called “anaphora resolution”, repeated reference entities different words.  example, phrase: “John Doe gave Mary flower loved ,” pronoun “” anaphor “Mary” “” anaphor “flower”. opposite case, pronoun precedes referent, called cataphora. working implementation feature reduce number redundant nodes.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"extract network from text","text":"can install development version networds GitHub :","code":"# install.packages(\"pak\") pak::pak(\"SoaresAlisson/networds\")"},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"extract network from text","text":"Ex. graph POS Obama Trump SOTU China Check vignettes: - 01 - Proper name extraction regex - [02 - Extract entity co-ocurrences POS]","code":""},{"path":"/index.html","id":"similar-projects","dir":"","previous_headings":"","what":"Similar Projects","title":"extract network from text","text":"textnet - “textNet set tools R language uses part--speech tagging dependency parsing generate semantic networks text data. compatible Universal Dependencies tested English-language text data”. textnets Chris Bail.","code":""},{"path":"/reference/collapse_adp.html","id":null,"dir":"Reference","previous_headings":"","what":"Collapse ADP — collapse_adp","title":"Collapse ADP — collapse_adp","text":"Collapse upos ADP, previous next pos NOUN PROPN. used pipeline extract proper names like \"United States America\"","code":""},{"path":"/reference/collapse_adp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collapse ADP — collapse_adp","text":"","code":"collapse_adp(pos_df)"},{"path":"/reference/collapse_adp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collapse ADP — collapse_adp","text":"pos_df POS dataframe generated spacy_parse","code":""},{"path":[]},{"path":"/reference/connectors.html","id":null,"dir":"Reference","previous_headings":"","what":"A lowercase connectors between two proper names — connectors","title":"A lowercase connectors between two proper names — connectors","text":"languages lowercase connector two proper names. function returns regex pattern language lowercase allowed connectors.","code":""},{"path":"/reference/connectors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A lowercase connectors between two proper names — connectors","text":"","code":"connectors(lang = \"pt\")"},{"path":"/reference/connectors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A lowercase connectors between two proper names — connectors","text":"lang language, can en, es, pt misc (many languages)","code":""},{"path":"/reference/connectors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A lowercase connectors between two proper names — connectors","text":"","code":"connectors(\"es\") #> [1] \"del\" connectors(\"pt\") #> [1] \"da\"  \"das\" \"de\"  \"do\"  \"dos\" connectors(\"port\") #> [1] \"da\"  \"das\" \"de\"  \"do\"  \"dos\" connectors(\"en\") #> [1] \"of\"     \"of the\" connectors(\"misc\") #> [1] \"of\"     \"the\"    \"of the\" \"von\"    \"van\"    \"del\""},{"path":"/reference/count_graphs.html","id":null,"dir":"Reference","previous_headings":"","what":"Count the graph frequency of co-occurences/Co-mentioning of triplets — count_graphs","title":"Count the graph frequency of co-occurences/Co-mentioning of triplets — count_graphs","text":"Count graph frequency co-occurences/Co-mentioning triplets","code":""},{"path":"/reference/count_graphs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count the graph frequency of co-occurences/Co-mentioning of triplets — count_graphs","text":"","code":"count_graphs(graph, loop = FALSE)"},{"path":"/reference/count_graphs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count the graph frequency of co-occurences/Co-mentioning of triplets — count_graphs","text":"graph graph co-occurrence loop TRUE, include loops. Default=FALSE","code":""},{"path":"/reference/count_graphs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Count the graph frequency of co-occurences/Co-mentioning of triplets — count_graphs","text":"","code":"DF <- data.frame(n1 = c(\"Sample1\", \"Sample1\", \"Sample2\", \"Sample2\"), n2 = c(\"A1\", \"Sample1\", \"B1\", \"B1\")) DF |> count_graphs() #> # A tibble: 2 × 3 #>   n1      n2        n #>   <chr>   <chr> <int> #> 1 Sample2 B1        2 #> 2 Sample1 A1        1"},{"path":"/reference/count_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"count a vector of elements — count_vec","title":"count a vector of elements — count_vec","text":"count vector elements, arrange frequency order, returns tibble","code":""},{"path":"/reference/count_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"count a vector of elements — count_vec","text":"","code":"count_vec(vec, sort_n = TRUE)"},{"path":"/reference/count_vec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"count a vector of elements — count_vec","text":"","code":"vec <- s2v(\"a a b c a b a z z z c d e\") #> Error in s2v(\"a a b c a b a z z z c d e\"): could not find function \"s2v\" vec |> count_vec() #> Error: object 'vec' not found"},{"path":"/reference/entity_list_2_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"from a vector of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs — entity_list_2_graph","title":"from a vector of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs — entity_list_2_graph","text":"vector entities (generated extract_entities2) returns tibble/dataframe co-occurence pairs","code":""},{"path":"/reference/entity_list_2_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"from a vector of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs — entity_list_2_graph","text":"","code":"entity_list_2_graph(entities)"},{"path":"/reference/entity_list_2_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"from a vector of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs — entity_list_2_graph","text":"entities vector entities generated extract_entities2","code":""},{"path":"/reference/entity_list_2_graph2.html","id":null,"dir":"Reference","previous_headings":"","what":"from a list of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs — entity_list_2_graph2","title":"from a list of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs — entity_list_2_graph2","text":"list entities (generated extract_entities2) returns tibble/dataframe co-occurence pairs","code":""},{"path":"/reference/entity_list_2_graph2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"from a list of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs — entity_list_2_graph2","text":"","code":"entity_list_2_graph2(entities_list, count = TRUE)"},{"path":"/reference/entity_list_2_graph2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"from a list of entities (generated by extract_entities2) returns a tibble/dataframe with co-occurence pairs — entity_list_2_graph2","text":"entities_list list object entities generated extract_entities2 count TRUE returns count co-occurences","code":""},{"path":"/reference/extract_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"extract entities from POS — extract_entities","title":"extract entities from POS — extract_entities","text":"extract entities POS","code":""},{"path":"/reference/extract_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extract entities from POS — extract_entities","text":"","code":"extract_entities(pos_df)"},{"path":"/reference/extract_entities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"extract entities from POS — extract_entities","text":"vector entities","code":""},{"path":"/reference/extract_entities2.html","id":null,"dir":"Reference","previous_headings":"","what":"extract_entity for each sentence — extract_entities2","title":"extract_entity for each sentence — extract_entities2","text":"extract_entity sentence","code":""},{"path":"/reference/extract_entities2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extract_entity for each sentence — extract_entities2","text":"","code":"extract_entities2(pos_df)"},{"path":"/reference/extract_entities2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"extract_entity for each sentence — extract_entities2","text":"list entities sentence","code":""},{"path":"/reference/extract_entities_l.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract entities in list format, by sentence — extract_entities_l","title":"Extract entities in list format, by sentence — extract_entities_l","text":"extract entities list, separated sentence.","code":""},{"path":"/reference/extract_entities_l.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract entities in list format, by sentence — extract_entities_l","text":"","code":"extract_entities_l(DF, rgx_entities = \"^(PER(SON)?|ORG|GPE)_\")"},{"path":"/reference/extract_entities_l.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract entities in list format, by sentence — extract_entities_l","text":"DF dataframe generated spacyr::spacy_parse","code":""},{"path":"/reference/extract_entities_l.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract entities in list format, by sentence — extract_entities_l","text":"vector","code":""},{"path":"/reference/extract_entities_l.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract entities in list format, by sentence — extract_entities_l","text":"","code":"t |> extract_entities() #> Error in extract_entities(t): could not find function \"extract_entities\""},{"path":"/reference/extract_entities_v.html","id":null,"dir":"Reference","previous_headings":"","what":"extract entities — extract_entities_v","title":"extract entities — extract_entities_v","text":"extract entities spacy entities.","code":""},{"path":"/reference/extract_entities_v.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extract entities — extract_entities_v","text":"","code":"extract_entities_v(DF, rgx_entities = \"^(PER(SON)?|ORG|GPE)_\")"},{"path":"/reference/extract_entities_v.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"extract entities — extract_entities_v","text":"DF dataframe generated spacyr::spacy_parse","code":""},{"path":"/reference/extract_entities_v.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"extract entities — extract_entities_v","text":"vector","code":""},{"path":"/reference/extract_entities_v.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"extract entities — extract_entities_v","text":"","code":"t |> extract_entities_v() #> Error in UseMethod(\"mutate\"): no applicable method for 'mutate' applied to an object of class \"function\""},{"path":"/reference/extract_entity.html","id":null,"dir":"Reference","previous_headings":"","what":"A rule based entity extractor extracts the entity from a text using regex. This regex captures all uppercase words, words that begin with upper case. If there is sequence of this patterns together, this function also captures. In the case of proper names with common lower case connectors like ","title":"A rule based entity extractor extracts the entity from a text using regex. This regex captures all uppercase words, words that begin with upper case. If there is sequence of this patterns together, this function also captures. In the case of proper names with common lower case connectors like ","text":"rule based entity extractor extracts entity text using regex. regex captures uppercase words, words begin upper case. sequence patterns together, function also captures. case proper names common lower case connectors like \"Wwwww Wwwww\" function also captures connector subsequent uppercase words.","code":""},{"path":"/reference/extract_entity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A rule based entity extractor extracts the entity from a text using regex. This regex captures all uppercase words, words that begin with upper case. If there is sequence of this patterns together, this function also captures. In the case of proper names with common lower case connectors like ","text":"","code":"extract_entity(text, connect = connectors(\"misc\"), sw = \"the\")"},{"path":"/reference/extract_entity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A rule based entity extractor extracts the entity from a text using regex. This regex captures all uppercase words, words that begin with upper case. If there is sequence of this patterns together, this function also captures. In the case of proper names with common lower case connectors like ","text":"text input text connect vector lowercase connectors. Use use , use function \"connector\" obtain patterns. sw vector stopwords","code":""},{"path":"/reference/extract_entity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A rule based entity extractor extracts the entity from a text using regex. This regex captures all uppercase words, words that begin with upper case. If there is sequence of this patterns together, this function also captures. In the case of proper names with common lower case connectors like ","text":"","code":"\"John Does lives in New York in United States of America.\" |> extract_entity() #> [1] \"John Does\"                \"New York\"                 #> [3] \"United States of America\" \"João Ninguém mora em São José do Rio Preto. Ele esteve antes em Sergipe\" |> extract_entity(connect = connectors(\"pt\")) #> [1] \"João Ninguém\"          \"São José do Rio Preto\" \"Ele\"                   #> [4] \"Sergipe\""},{"path":"/reference/extract_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a non directional graph based on co-occurrence in the token. It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph","title":"Extract a non directional graph based on co-occurrence in the token. It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph","text":"Extract non directional graph based co-occurrence token. extracts two entities mentioned token (sentence paragraph)","code":""},{"path":"/reference/extract_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a non directional graph based on co-occurrence in the token. It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph","text":"","code":"extract_graph(   text,   using = \"sentences\",   connect = connectors(\"misc\"),   sw = c(\"of\", \"the\"),   loop = FALSE )"},{"path":"/reference/extract_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract a non directional graph based on co-occurrence in the token. It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph","text":"text input text using sentence paragraph tokenize connect lowercase connectors, like \"von\" \"John von Neumann\". sw stopwords vector. loop TRUE, remove loops, node pointing .","code":""},{"path":"/reference/extract_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract a non directional graph based on co-occurrence in the token. It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph","text":"","code":"text <- \"John Does lives in New York in United States of America. He  is a passionate jazz musician, often playing in local clubs.\" extract_graph(text) #> Tokenizing by sentences #> # A tibble: 6 × 2 #>   n1                       n2                       #>   <chr>                    <chr>                    #> 1 John Does                New York                 #> 2 John Does                United States of America #> 3 John Does                He                       #> 4 New York                 United States of America #> 5 New York                 He                       #> 6 United States of America He"},{"path":"/reference/extract_graph_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a non directional graph based on co-occurrence in the token and returns a tibble It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph_df","title":"Extract a non directional graph based on co-occurrence in the token and returns a tibble It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph_df","text":"Extract non directional graph based co-occurrence token returns tibble extracts two entities mentioned token (sentence paragraph)","code":""},{"path":"/reference/extract_graph_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a non directional graph based on co-occurrence in the token and returns a tibble It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph_df","text":"","code":"extract_graph_df(   df,   column_id,   column_text,   using = \"sentences\",   connect = connectors(\"misc\"),   sw = c(\"of\", \"the\"),   loop = FALSE )"},{"path":"/reference/extract_graph_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract a non directional graph based on co-occurrence in the token and returns a tibble It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph_df","text":"df data frame two columns: text id column_id name column id column_text name column text extract graph using sentence paragraph tokenize connect lowercase connectors, like \"von\" \"John von Neumann\". sw stopwords vector. loop TRUE, remove loops, node pointing .","code":""},{"path":"/reference/extract_graph_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract a non directional graph based on co-occurrence in the token and returns a tibble It extracts only if two entities are mentioned in the same token (sentence or paragraph) — extract_graph_df","text":"","code":"# creating a dataframe with text and id DF <- data.frame(text = c(\"John Does lives in New York in United States of America. He  is a passionate jazz musician, often playing in local clubs.\", r\"(John Michael \"Ozzy\" Osbourne (3 December 1948 – 22 July 2025) was an English singer, songwriter, and media personality. He co-founded the pioneering heavy metal band Black Sabbath in 1968, and rose to prominence in the 1970s as their lead vocalist. During this time, he adopted the title \"Prince of Darkness\".[3][4] He performed on the band's first eight albums, most notably including Black Sabbath, Paranoid (both 1970) and Master of Reality (1971), before he was fired in 1979 due to his problems with alcohol and other drugs.)\")) |> dplyr::mutate(id = paste0(\"id_\", dplyr::row_number() )) extract_graph_df(DF, \"id\", \"text\") #> Error in extract_graph_df(DF, \"id\", \"text\"): object 'DF' not found"},{"path":"/reference/extract_graph_pos.html","id":null,"dir":"Reference","previous_headings":"","what":"extract graph of co-ocurrence from a POS dataframe — extract_graph_pos","title":"extract graph of co-ocurrence from a POS dataframe — extract_graph_pos","text":"extract graph co-ocurrence POS dataframe","code":""},{"path":"/reference/extract_graph_pos.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extract graph of co-ocurrence from a POS dataframe — extract_graph_pos","text":"","code":"extract_graph_pos(pos_df, POS = c(\"NOUN\", \"PROPN\"))"},{"path":"/reference/extract_graph_pos.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"extract graph of co-ocurrence from a POS dataframe — extract_graph_pos","text":"pos_df data.frame generated spacyr::spacy_parse POS POS extracted. Defaul: NOUN PROPN","code":""},{"path":"/reference/extract_graph_rgx.html","id":null,"dir":"Reference","previous_headings":"","what":"extract a graph from text, using custom regex pattern as nodes. — extract_graph_rgx","title":"extract a graph from text, using custom regex pattern as nodes. — extract_graph_rgx","text":"extract graph text, using custom regex pattern nodes.","code":""},{"path":"/reference/extract_graph_rgx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extract a graph from text, using custom regex pattern as nodes. — extract_graph_rgx","text":"","code":"extract_graph_rgx(   text,   pattern,   sw = gen_stopwords(\"en\"),   count_graphs = FALSE )"},{"path":"/reference/extract_graph_rgx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"extract a graph from text, using custom regex pattern as nodes. — extract_graph_rgx","text":"graph","code":""},{"path":"/reference/extract_nodes.html","id":null,"dir":"Reference","previous_headings":"","what":"extract nodes from a dataframe of two columns of nodes' — extract_nodes","title":"extract nodes from a dataframe of two columns of nodes' — extract_nodes","text":"extract nodes dataframe two columns nodes'","code":""},{"path":"/reference/extract_nodes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extract nodes from a dataframe of two columns of nodes' — extract_nodes","text":"","code":"extract_nodes(graph)"},{"path":"/reference/extract_relation.html","id":null,"dir":"Reference","previous_headings":"","what":"tokenize and selects only sentences/paragraphs with more than one entity per sentence or paragraph — extract_relation","title":"tokenize and selects only sentences/paragraphs with more than one entity per sentence or paragraph — extract_relation","text":"tokenize selects sentences/paragraphs one entity per sentence paragraph","code":""},{"path":"/reference/extract_relation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"tokenize and selects only sentences/paragraphs with more than one entity per sentence or paragraph — extract_relation","text":"","code":"extract_relation(   text,   using = \"sentences\",   connect = connectors(\"misc\"),   sw = gen_stopwords(\"en\") )"},{"path":"/reference/extract_relation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"tokenize and selects only sentences/paragraphs with more than one entity per sentence or paragraph — extract_relation","text":"text input text using sentence paragraph tokenize connect lowercase connectors, like \"von\" \"John von Neumann\". use pre built connectors use `connectors()“ sw stopwords vector. use pre built stopwords use `gen_stopwords()`","code":""},{"path":"/reference/extract_relation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"tokenize and selects only sentences/paragraphs with more than one entity per sentence or paragraph — extract_relation","text":"","code":"\"John Does lives in New York in United States of America.\" |> extract_relation() #> Tokenizing by sentences #> Error in gen_stopwords(\"en\"): could not find function \"gen_stopwords\" \"João Ninguém mora em São José do Rio Preto. Ele foi para o Rio de Janeiro.\" |> extract_relation(connector = connectors(\"pt\")) #> Error in extract_relation(\"João Ninguém mora em São José do Rio Preto. Ele foi para o Rio de Janeiro.\",     connector = connectors(\"pt\")): unused argument (connector = connectors(\"pt\"))"},{"path":"/reference/extract_triplets.html","id":null,"dir":"Reference","previous_headings":"","what":"from semgram output, join passive and active voices # VER great..../GR_analise — extract_triplets","title":"from semgram output, join passive and active voices # VER great..../GR_analise — extract_triplets","text":"semgram output, join passive active voices # VER great..../GR_analise","code":""},{"path":"/reference/extract_triplets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"from semgram output, join passive and active voices # VER great..../GR_analise — extract_triplets","text":"","code":"extract_triplets(semgram)"},{"path":"/reference/filter_by_query.html","id":null,"dir":"Reference","previous_headings":"","what":"tokenize and filter text by query — filter_by_query","title":"tokenize and filter text by query — filter_by_query","text":"vector texts, tokenize sentence paragraph returns filtered list","code":""},{"path":"/reference/filter_by_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"tokenize and filter text by query — filter_by_query","text":"","code":"filter_by_query(   txt,   query,   i_c = TRUE,   by_sentence = TRUE,   unlist = FALSE,   msg = TRUE )"},{"path":"/reference/filter_by_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"tokenize and filter text by query — filter_by_query","text":"txt vector texts query query filter text by_sentence tokenize sentence (default TRUE). FALSE, tokenize paragraph unlist TRUE (default FALSE), returns vector instead list object. msg TRUE (default TRUE), returns message query found. ic ignore case (default TRUE)","code":""},{"path":"/reference/filter_by_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"tokenize and filter text by query — filter_by_query","text":"","code":"# loading data data(package = \"txtnet\") # sample txt_wiki[2:3] #> [1] \" \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            #> [2] \"Brian Thompson, the 50-year-old CEO of the American health insurance company UnitedHealthcare, was shot and killed in Midtown Manhattan, New York City, on December 4, 2024. The shooting occurred early in the morning outside an entrance to the New York Hilton Midtown hotel.[4] Thompson was in the city to attend an annual investors' meeting for UnitedHealth Group, the parent company of UnitedHealthcare. Prior to his death, he faced criticism for the company's rejection of insurance claims, and his family reported that he had received death threats in the past. The suspect, initially described as a white man wearing a mask, fled the scene.[1] On December 9, 2024, authorities arrested 26-year-old Luigi Mangione in Altoona, Pennsylvania, and charged him with Thompson's murder in a Manhattan court.[5][6][7]\" txt_wiki |> filter_by_query(\"York\") #> [[1]] #> character(0) #>  #> [[2]] #> character(0) #>  #> [[3]] #> [1] \"Brian Thompson, the 50-year-old CEO of the American health insurance company UnitedHealthcare, was shot and killed in Midtown Manhattan, New York City, on December 4, 2024.\" #> [2] \"The shooting occurred early in the morning outside an entrance to the New York Hilton Midtown hotel.[\"                                                                        #>  #> [[4]] #> [1] \"8][9][10] Authorities also said his fingerprints matched those that investigators found near the New York shooting scene.[\" #> [2] \"12] Mangione also has an arrest warrant with five felony counts in New York, including second-degree murder.[\"              #>  #> [[5]] #> character(0) #>  #> [[6]] #> character(0) #>  #> [[7]] #> character(0) #>  #> [[8]] #> character(0) #>  #> [[9]] #> [1] \"The suspect arrived in New York City on November 24, 2024, on a Greyhound bus.\"                                                                                                   #> [2] \"27][22] He checked into the HI New York City Hostel on the Upper West Side of Manhattan on November 24, 2024, with a falsified New Jersey identification card and paid in cash.[\" #> [3] \"28] He stayed all but one night of the 10 days he was in New York City at the hostel, checking out on December 3, 2024.[\"                                                         #>  #> [[10]] #> [1] \"Thompson was in New York City for an annual UnitedHealth Group investors' meeting, having arrived in the city on December 2, 2024.[\" #> [2] \"EST (UTC−5), Thompson was walking along West 54th Street toward the New York Hilton Midtown hotel that was hosting the meeting.[\"    #>  #> [[11]] #> character(0) #>  #> [[12]] #> character(0) #>  #> [[13]] #> character(0) #>  #> [[14]] #> character(0) #>  #> [[15]] #> [1] \"The New York City Police Department offered a reward up to $10,000 for information about the shooter on December 4, 2024.[\" #>  #> [[16]] #> character(0) #>  #> [[17]] #> character(0) #>  #> [[18]] #> character(0) #>  #> [[19]] #> [1] \"71][72] Altoona is about 280 miles (450 km) west of New York City.[\" #>  #> [[20]] #> [1] \"80] He was denied bail for the second time on December 10, 2024, and through his Pennsylvania attorney, he indicated his intention to fight a prospective interstate extradition to New York.[\"         #> [2] \"6][81] Mangione hired Karen Friedman Agnifilo, former prosecutor at the Manhattan District Attorney's Office and former legal analyst with CNN, as his New York case defense attorney on December 13.[\" #>  #> [[21]] #> [1] \"10] According to The New York Times, the mention of CAD apparently relates to the process of 3D-printing the ghost gun's plastic frame.[\" #>  #> [[22]] #> character(0) #>  #> [[23]] #> [1] \"New York Police Chief of Detectives Joseph Kenny believes Mangione may have targeted them because of the company's size.[\" #>  #> [[24]] #> character(0) #>  #> [[25]] #> character(0) #>  #> [[26]] #> character(0) #>  #> [[27]] #> character(0) #>  #> [[28]] #> character(0) #>  #> [[29]] #> character(0) #>  #> [[30]] #> character(0) #>  #> [[31]] #> character(0) #>  #> [[32]] #> character(0) #>  #> [[33]] #> [1] \"Zeynep Tufekci, a professor of sociology and public affairs at Princeton University and New York Times columnist, said that the public reaction to Thompson's murder \\\"should ring all the alarm bells\\\" and resembled the reaction to the very high levels of corporate greed, exploitation, and economic inequality during the American Gilded Age, a period characterized by violent \\\"political movements that targeted corporate titans, politicians, judges and others\\\".[\" #>  #> [[34]] #> character(0) #>  #> [[35]] #> character(0) #>  #> [[36]] #> character(0) #>  #> [[37]] #> character(0) #>  #> [[38]] #> character(0) #>  #> [[39]] #> character(0) #>  #> [[40]] #> character(0) #>  #> [[41]] #> character(0) #>  #> [[42]] #> character(0) #>  #> [[43]] #> [1] \"Independent journalist Ken Klippenstein stated that numerous major media outlets refused to publish Mangione's alleged manifesto despite being in possession of it, writing \\\"My queries to The New York Times, CNN and ABC to explain their rationale for withholding the manifesto, while gladly quoting from it selectively, have not been answered.\\\"\" #> [2] \"Klippenstein also alleged that The New York Times directed their staff to \\\"dial back\\\" on showing photographs containing Mangione's face.[\"                                                                                                                                                                                                               #>  #> [[44]] #> character(0) #>  #> [[45]] #> character(0) #>  txt_wiki |> filter_by_query(\"Police\") #> [[1]] #> character(0) #>  #> [[2]] #> character(0) #>  #> [[3]] #> character(0) #>  #> [[4]] #> [1] \"11] Mangione was held without bail in Pennsylvania on charges of possession of an unlicensed firearm, forgery, and providing false New Jersey-resident identification to police.[\" #> [2] \"12] Police believe that he was inspired by Ted Kaczynski's essay Industrial Society and Its Future (1995), and motivated by his personal views on health insurance.[\"              #>  #> [[5]] #> character(0) #>  #> [[6]] #> character(0) #>  #> [[7]] #> character(0) #>  #> [[8]] #> character(0) #>  #> [[9]] #> character(0) #>  #> [[10]] #> character(0) #>  #> [[11]] #> character(0) #>  #> [[12]] #> [1] \"39] According to the police, he then left the city from the George Washington Bridge Bus Station farther uptown in Upper Manhattan.[\" #>  #> [[13]] #> [1] \"49] Accordingly, police stated they are investigating whether the words suggest the killer's motive.[\" #>  #> [[14]] #> [1] \"50] Police said they believed they found the shooter's backpack in Central Park on December 6, 2024.[\" #>  #> [[15]] #> [1] \"The New York City Police Department offered a reward up to $10,000 for information about the shooter on December 4, 2024.[\" #>  #> [[16]] #> [1] \"The shooter was described by police as a white man, approximately 6 ft 1 in (185 cm) tall, wearing a light brown or cream-colored hooded jacket, dark pants, and black sneakers with white soles.\" #> [2] \"31][39][57][58] Police said the suspect appeared to be proficient in the use of firearms[30] and was described as being \\\"extremely camera savvy.\\\"[\"                                              #>  #> [[17]] #> character(0) #>  #> [[18]] #> [1] \"69] Mangione's mother contacted the San Francisco Police Department, as she believed that Mangione lived in San Francisco and had a job in the area.[\" #>  #> [[19]] #> [1] \"Local police in Altoona, Pennsylvania, arrested Mangione on December 9, 2024, at a McDonald's restaurant in the city.\"                                                                                                                                                                          #> [2] \"An employee there called the police to say that a customer recognized the suspect from images released by the NYPD.[\"                                                                                                                                                                           #> [3] \"63] In his bag they found a 3D-printed gun and a 3D-printed suppressor, which the police claim are consistent with the weapon used in the shooting, and a falsified New Jersey driver's license with the same name as the one used by the alleged shooter to check into the Manhattan hostel.[\" #> [4] \"8][73][3][74]  The police also said that when they arrested Mangione, they found a three-page,[74] 262-word handwritten document about the American healthcare system, which they characterized as a manifesto.[\"                                                                               #>  #> [[20]] #> character(0) #>  #> [[21]] #> character(0) #>  #> [[22]] #> character(0) #>  #> [[23]] #> [1] \"85] Police believe the motive was related to an injury that Mangione had suffered that caused him to visit the emergency room in July 2023.\" #> [2] \"New York Police Chief of Detectives Joseph Kenny believes Mangione may have targeted them because of the company's size.[\"                   #>  #> [[24]] #> [1] \"Police believe that Mangione was inspired by Ted Kaczynski's Industrial Society and Its Future.[\" #>  #> [[25]] #> character(0) #>  #> [[26]] #> character(0) #>  #> [[27]] #> character(0) #>  #> [[28]] #> character(0) #>  #> [[29]] #> character(0) #>  #> [[30]] #> character(0) #>  #> [[31]] #> character(0) #>  #> [[32]] #> character(0) #>  #> [[33]] #> character(0) #>  #> [[34]] #> character(0) #>  #> [[35]] #> character(0) #>  #> [[36]] #> character(0) #>  #> [[37]] #> character(0) #>  #> [[38]] #> character(0) #>  #> [[39]] #> character(0) #>  #> [[40]] #> character(0) #>  #> [[41]] #> character(0) #>  #> [[42]] #> character(0) #>  #> [[43]] #> character(0) #>  #> [[44]] #> character(0) #>  #> [[45]] #> character(0) #>"},{"path":"/reference/filter_ego.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter a graph / create an ego graph — filter_ego","title":"Filter a graph / create an ego graph — filter_ego","text":"filter graph / create ego graph term number neighbors","code":""},{"path":"/reference/filter_ego.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter a graph / create an ego graph — filter_ego","text":"","code":"filter_ego(edges, nodes = NULL, filter_by, n_neighbours = 1)  filter_ego(edges, nodes = NULL, filter_by, n_neighbours = 1)"},{"path":"/reference/filter_ego.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter a graph / create an ego graph — filter_ego","text":"edges edge dataframe nodes node dataframe filter_by term filter ego graph n_neighbours number neighbors as_tbl TRUE, return tbl_graph, FALSE, return igraph object","code":""},{"path":"/reference/filter_ego.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter a graph / create an ego graph — filter_ego","text":"","code":"# creating sample data nodes <- data.frame(id = 1:5, name = LETTERS[1:5]) edges <- data.frame(from = c(1, 1, 2, 3, 4, 1, 6, 7), to = c(2, 3, 4, 5, 5, 4, 7, 5)) filter_ego(edges, nodes, filter_by = 1, n_neighbours = 1) #> Error in igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes): Some vertex names in edge list are not listed in vertex data frame filter_ego(edges, nodes, filter_by = 1, n_neighbours = 2) #> Error in igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes): Some vertex names in edge list are not listed in vertex data frame # creating sample data nodes <- data.frame(id = 1:5, name = LETTERS[1:5]) edges <- data.frame(from = c(1, 1, 2, 3, 4, 1, 6, 7), to = c(2, 3, 4, 5, 5, 4, 7, 5)) filter_ego(edges, nodes, filter_by = 1, n_neighbours = 1) #> Error in igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes): Some vertex names in edge list are not listed in vertex data frame filter_ego(edges, nodes, filter_by = \"jojo\", n_neighbours = 2) #> Error in igraph::graph_from_data_frame(edges, directed = TRUE, vertices = nodes): Some vertex names in edge list are not listed in vertex data frame"},{"path":"/reference/filter_ppn.html","id":null,"dir":"Reference","previous_headings":"","what":"extract proper name and nouns from POS DF — filter_ppn","title":"extract proper name and nouns from POS DF — filter_ppn","text":"extract proper name nouns POS DF","code":""},{"path":"/reference/filter_ppn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extract proper name and nouns from POS DF — filter_ppn","text":"","code":"filter_ppn(pos_df, POS = c(\"NOUN\", \"PROPN\"))"},{"path":"/reference/filter_ppn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"extract proper name and nouns from POS DF — filter_ppn","text":"pos_df data.frame generated spacyr::spacy_parse POS POS extracted. Default: NOUN PROPN","code":""},{"path":"/reference/filter_ppn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"extract proper name and nouns from POS DF — filter_ppn","text":"","code":"pos <- \"Mary Jane was the first to dance with Robert\" |> spacy_parse() #> Error in spacy_parse(\"Mary Jane was the first to dance with Robert\"): could not find function \"spacy_parse\" pos |> filter_ppn() #> Error: object 'pos' not found"},{"path":"/reference/g.html","id":null,"dir":"Reference","previous_headings":"","what":"A sample graph dataframe (tbl-graph) — g","title":"A sample graph dataframe (tbl-graph) — g","text":"sample dataset used illustrate functionality package.","code":""},{"path":"/reference/g.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A sample graph dataframe (tbl-graph) — g","text":"","code":"g"},{"path":"/reference/g.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A sample graph dataframe (tbl-graph) — g","text":"tibble graph, tidygraphs package nodes nodes names edges relationships nodes","code":""},{"path":"/reference/g.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A sample graph dataframe (tbl-graph) — g","text":"","code":"data(g) #> Warning: data set ‘g’ not found head(g) #> 6 x 6 sparse Matrix of class \"dgCMatrix\" #>         Alice Bob Charlie David John Mary #> Alice       .   1       1     .    .    . #> Bob         .   .       .     1    1    . #> Charlie     .   .       .     1    .    . #> David       .   .       .     .    .    . #> John        .   .       .     .    .    . #> Mary        .   .       .     .    1    ."},{"path":"/reference/get_cooc.html","id":null,"dir":"Reference","previous_headings":"","what":"get graph (co-ocurrence of entities and Nouns) — get_cooc","title":"get graph (co-ocurrence of entities and Nouns) — get_cooc","text":"POS dataframe (using `filter_by_query() |> parsePOS()` ) get pairs co-ocurrences.","code":""},{"path":"/reference/get_cooc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get graph (co-ocurrence of entities and Nouns) — get_cooc","text":"","code":"get_cooc(pos_df, pos_cat = c(\"NOUN\", \"ENTITY\"))"},{"path":"/reference/get_cooc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get graph (co-ocurrence of entities and Nouns) — get_cooc","text":"pos_df POS dataframe pos_cat POS categories extracted. Default: NOUN ENTITY","code":""},{"path":"/reference/get_cooc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get graph (co-ocurrence of entities and Nouns) — get_cooc","text":"","code":"x <- txt_wiki |> filter_by_query(\"Police\") x <- x |> parsePOS(only_entities = FALSE) #> successfully initialized (spaCy Version: 3.8.5, language model: en_core_web_sm) get_cooc(x) #> $graphs #> # A tibble: 505 × 3 #>    n1    n2          freq #>    <chr> <chr>      <int> #>  1 3D    Manhattan      2 #>  2 3D    New_Jersey     2 #>  3 3D    claim          2 #>  4 3D    driver         2 #>  5 3D    hostel         2 #>  6 3D    license        2 #>  7 3D    name           2 #>  8 3D    one            2 #>  9 3D    police         2 #> 10 3D    shooter        2 #> # ℹ 495 more rows #>  #> $isolated_nodes #> [1] node #> <0 rows> (or 0-length row.names) #>  #> $nodes #> # A tibble: 99 × 2 #>    node                               freq #>    <chr>                             <int> #>  1 police                                8 #>  2 Mangione                              6 #>  3 Police                                5 #>  4 shooter                               4 #>  5 3D                                    2 #>  6 Industrial_Society_and_Its_Future     2 #>  7 New_Jersey                            2 #>  8 Pennsylvania                          2 #>  9 Ted_Kaczynski_'s                      2 #> 10 city                                  2 #> # ℹ 89 more rows #>"},{"path":"/reference/get_cooc_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"get graph (co-ocurrence of entities) — get_cooc_entities","title":"get graph (co-ocurrence of entities) — get_cooc_entities","text":"POS dataframe (using filter_by_query() |> parsePOS()) get pairs entities","code":""},{"path":"/reference/get_cooc_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get graph (co-ocurrence of entities) — get_cooc_entities","text":"","code":"get_cooc_entities(pos, loop = FALSE, freq = TRUE, lower_case = FALSE)"},{"path":"/reference/get_cooc_entities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get graph (co-ocurrence of entities) — get_cooc_entities","text":"pos dataframe generated filter_by_query() |> parsePOS() loop TRUE (default: FALSE), returns loop (self reference) graph freq TRUE (default: TRUE), returns count (frequency) edges/co-occurences","code":""},{"path":"/reference/get_cooc_entities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get graph (co-ocurrence of entities) — get_cooc_entities","text":"","code":"x <- txt_wiki |> filter_by_query(\"Police\") x <- x |> parsePOS() get_cooc_entities(x) #> $graphs #> # A tibble: 88 × 3 #>    n1               n2                                 freq #>    <chr>            <chr>                             <int> #>  1 Ted_Kaczynski_'s Industrial_Society_and_Its_Future     2 #>  2 Altoona          Industrial_Society_and_Its_Future     1 #>  3 Altoona          McDonald                              1 #>  4 Altoona          Ted_Kaczynski_'s                      1 #>  5 Central_Park     Altoona                               1 #>  6 Central_Park     Industrial_Society_and_Its_Future     1 #>  7 Central_Park     Mangione                              1 #>  8 Central_Park     McDonald                              1 #>  9 Central_Park     New_York_City                         1 #> 10 Central_Park     San_Francisco                         1 #> # ℹ 78 more rows #>  #> $isolated_nodes #>       node freq #> 1 American    1 #>  #> $nodes #> # A tibble: 18 × 2 #>    node                                      freq #>    <chr>                                    <int> #>  1 Industrial_Society_and_Its_Future            2 #>  2 New_Jersey                                   2 #>  3 Ted_Kaczynski_'s                             2 #>  4 Altoona                                      1 #>  5 American                                     1 #>  6 Central_Park                                 1 #>  7 Joseph_Kenny                                 1 #>  8 Mangione                                     1 #>  9 Manhattan                                    1 #> 10 McDonald                                     1 #> 11 NYPD                                         1 #> 12 New_York                                     1 #> 13 New_York_City                                1 #> 14 Pennsylvania                                 1 #> 15 San_Francisco                                1 #> 16 Upper_Manhattan                              1 #> 17 the_George_Washington_Bridge_Bus_Station     1 #> 18 the_San_Francisco_Police_Department          1 #>  # with loops /self-reference get_cooc_entities(x, loop = TRUE) #> $graphs #> # A tibble: 103 × 3 #>    n1                                   n2                                 freq #>    <chr>                                <chr>                             <int> #>  1 Mangione                             Mangione                             10 #>  2 Pennsylvania                         Mangione                              8 #>  3 Central_Park                         Mangione                              5 #>  4 George_Washington_Bridge_Bus_Station Mangione                              5 #>  5 Mangione                             Industrial_Society_and_Its_Future     5 #>  6 Mangione                             Ted_Kaczynski_'s                      5 #>  7 New_Jersey                           Mangione                              5 #>  8 New_York_City                        Mangione                              5 #>  9 Upper_Manhattan                      Mangione                              5 #> 10 San_Francisco_Police_Department      Mangione                              4 #> # ℹ 93 more rows #>  #> $isolated_nodes #>       node freq #> 1 American    1 #>  #> $nodes #> # A tibble: 18 × 2 #>    node                                      freq #>    <chr>                                    <int> #>  1 Mangione                                     5 #>  2 Industrial_Society_and_Its_Future            2 #>  3 New_Jersey                                   2 #>  4 Pennsylvania                                 2 #>  5 Ted_Kaczynski_'s                             2 #>  6 Altoona                                      1 #>  7 American                                     1 #>  8 Central_Park                                 1 #>  9 Joseph_Kenny                                 1 #> 10 Manhattan                                    1 #> 11 McDonald                                     1 #> 12 NYPD                                         1 #> 13 New_York                                     1 #> 14 New_York_City                                1 #> 15 San_Francisco                                1 #> 16 Upper_Manhattan                              1 #> 17 the_George_Washington_Bridge_Bus_Station     1 #> 18 the_San_Francisco_Police_Department          1 #>  get_cooc_entities(x, lower_case = TRUE) #> $graphs #> # A tibble: 88 × 3 #>    n1               n2                                 freq #>    <chr>            <chr>                             <int> #>  1 ted_kaczynski_'s industrial_society_and_its_future     2 #>  2 altoona          industrial_society_and_its_future     1 #>  3 altoona          mcdonald                              1 #>  4 altoona          ted_kaczynski_'s                      1 #>  5 central_park     altoona                               1 #>  6 central_park     industrial_society_and_its_future     1 #>  7 central_park     mangione                              1 #>  8 central_park     mcdonald                              1 #>  9 central_park     new_york_city                         1 #> 10 central_park     san_francisco                         1 #> # ℹ 78 more rows #>  #> $isolated_nodes #>       node freq #> 1 american    1 #>  #> $nodes #> # A tibble: 18 × 2 #>    node                                      freq #>    <chr>                                    <int> #>  1 industrial_society_and_its_future            2 #>  2 new_jersey                                   2 #>  3 ted_kaczynski_'s                             2 #>  4 altoona                                      1 #>  5 american                                     1 #>  6 central_park                                 1 #>  7 joseph_kenny                                 1 #>  8 mangione                                     1 #>  9 manhattan                                    1 #> 10 mcdonald                                     1 #> 11 new_york                                     1 #> 12 new_york_city                                1 #> 13 nypd                                         1 #> 14 pennsylvania                                 1 #> 15 san_francisco                                1 #> 16 the_george_washington_bridge_bus_station     1 #> 17 the_san_francisco_police_department          1 #> 18 upper_manhattan                              1 #>"},{"path":"/reference/get_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"get entities from POS DF — get_entities","title":"get entities from POS DF — get_entities","text":"get entities POS DF","code":""},{"path":"/reference/get_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get entities from POS DF — get_entities","text":"","code":"get_entities(pos_df, as_list = TRUE, rgx_entities = \"^(PER(SON)?|ORG|GPE)_\")"},{"path":"/reference/get_entities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get entities from POS DF — get_entities","text":"pos_df pos data frame generated spacyr::spacy_parse()","code":""},{"path":"/reference/get_entities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get entities from POS DF — get_entities","text":"","code":"pos_txt <- sample_text |> spacyr::spacy_parse() #> Error: object 'sample_text' not found pos_txt  |> get_entities(as_list = FALSE) #> Error: object 'pos_txt' not found pos_txt  |> get_entities() #> Error: object 'pos_txt' not found"},{"path":"/reference/get_graph_from_txt.html","id":null,"dir":"Reference","previous_headings":"","what":"graph from text and query — get_graph_from_txt","title":"graph from text and query — get_graph_from_txt","text":"text query, generates graph dataframe co-ocurrence frequency","code":""},{"path":"/reference/get_graph_from_txt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"graph from text and query — get_graph_from_txt","text":"","code":"get_graph_from_txt(text, query, i_c = TRUE, by_sentence = TRUE, loop = FALSE)"},{"path":"/reference/get_graph_from_txt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"graph from text and query — get_graph_from_txt","text":"text vector texts query query filter text by_sentence tokenize sentence (default TRUE). FALSE, tokenize paragraph loop TRUE (default: FALSE), returns loop (self reference) graph ic ignore case (default TRUE)","code":""},{"path":"/reference/get_graph_from_txt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"graph from text and query — get_graph_from_txt","text":"","code":"# loading data data(package = \"txtnet\") # sample txt_wiki[2:3] #> [1] \" \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            #> [2] \"Brian Thompson, the 50-year-old CEO of the American health insurance company UnitedHealthcare, was shot and killed in Midtown Manhattan, New York City, on December 4, 2024. The shooting occurred early in the morning outside an entrance to the New York Hilton Midtown hotel.[4] Thompson was in the city to attend an annual investors' meeting for UnitedHealth Group, the parent company of UnitedHealthcare. Prior to his death, he faced criticism for the company's rejection of insurance claims, and his family reported that he had received death threats in the past. The suspect, initially described as a white man wearing a mask, fled the scene.[1] On December 9, 2024, authorities arrested 26-year-old Luigi Mangione in Altoona, Pennsylvania, and charged him with Thompson's murder in a Manhattan court.[5][6][7]\" txt_wiki |> get_graph_from_txt(\"York\") #> Error in get_pairs(parsePOS(filter_by_query(text, query = query, i_c = i_c,     by_sentence = by_sentence)), loop = loop): could not find function \"get_pairs\" txt_wiki |> get_graph_from_txt(\"Police\") #> Error in get_pairs(parsePOS(filter_by_query(text, query = query, i_c = i_c,     by_sentence = by_sentence)), loop = loop): could not find function \"get_pairs\""},{"path":"/reference/get_neighbors.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the neighbors of a site — get_neighbors","title":"Get the neighbors of a site — get_neighbors","text":"creates sub graph node reference Nth neighbors","code":""},{"path":"/reference/get_neighbors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the neighbors of a site — get_neighbors","text":"","code":"get_neighbors(graph, query, n = 1)"},{"path":"/reference/get_neighbors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the neighbors of a site — get_neighbors","text":"","code":"g #> IGRAPH 1195f6a DN-- 6 6 --  #> + attr: name (v/c) #> + edges from 1195f6a (vertex names): #> [1] Alice  ->Bob     Alice  ->Charlie Bob    ->David   Charlie->David   #> [5] Bob    ->John    Mary   ->John    g |> plot()  get_neighbors(g, \"Charlie\", 1) #>  #> Attaching package: ‘tidygraph’ #> The following object is masked from ‘package:stats’: #>  #>     filter #> # A tbl_graph: 3 nodes and 2 edges #> # #> # A rooted tree #> # #> # Node Data: 3 × 2 (active) #>   name    .tidygraph_node_index #>   <chr>                   <int> #> 1 Alice                       1 #> 2 Charlie                     3 #> 3 David                       4 #> # #> # Edge Data: 2 × 3 #>    from    to .tidygraph_edge_index #>   <int> <int>                 <int> #> 1     1     2                     2 #> 2     2     3                     4 get_neighbors(g, \"Charlie\", 2) |> plot()  get_neighbors(g, \"Alice\", 1) |> plot()  get_neighbors(g, \"Alice\", 2) |> plot()"},{"path":"/reference/get_node_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for the id of a node in a graph — get_node_id","title":"Search for the id of a node in a graph — get_node_id","text":"Search id node graph","code":""},{"path":"/reference/get_node_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for the id of a node in a graph — get_node_id","text":"","code":"get_node_id(graph, query)"},{"path":"/reference/get_node_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for the id of a node in a graph — get_node_id","text":"graph tbl_graph querie name node","code":""},{"path":"/reference/get_node_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for the id of a node in a graph — get_node_id","text":"","code":"# Building a graph dataframe # Node data frame: nodes <- data.frame(name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"John\", \"Mary\")) # Edge data frame edges <- data.frame(from = c(1, 1, 2, 3, 2, 6), to = c(2, 3, 4, 4, 5, 5)) g <- tidygraph::tbl_graph(nodes = nodes, edges = edges) get_node_id(g, \"Bob\") #> [1] 2"},{"path":"/reference/graph_from_coocurrence_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Graph from co-occurrence list — graph_from_coocurrence_list","title":"Graph from co-occurrence list — graph_from_coocurrence_list","text":"given dataframe column (list) POS, returns list three elements: 1) tibble frequency graphs; 2) tibble isolated nodes, .e. nodes withot connection. 3) tibble individual frequency node. list element one element, removed.","code":""},{"path":"/reference/graph_from_coocurrence_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graph from co-occurrence list — graph_from_coocurrence_list","text":"","code":"graph_from_coocurrence_list(coocurrence_list, strip_rgx = \"^the_\", freq = TRUE)"},{"path":"/reference/graph_from_coocurrence_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graph from co-occurrence list — graph_from_coocurrence_list","text":"coocurrence_list dataframe generated get_pairs() strip_rgx regex strip. Default: \"^the_\". erase nothing, use \"\". freq TRUE (default), returns dataframe frequency. FALSE, returns pairs","code":""},{"path":"/reference/graph_from_coocurrence_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Graph from co-occurrence list — graph_from_coocurrence_list","text":"","code":"pos <- txt_wiki |>   filter_by_query(\"Police\") |>   parsePOS() entities_by_txt <- pos |>   dplyr::group_by(doc_id) |>   dplyr::summarise(entities = list(unique(entity))) graph_from_coocurrence_list(entities_by_txt) #> Error in graph_from_coocurrence_list(entities_by_txt): could not find function \"graph_from_coocurrence_list\""},{"path":"/reference/graph_subs.html","id":null,"dir":"Reference","previous_headings":"","what":"Substitute node columns (columns 1 and 2) of a graph data frame with a dictionary of substitutions. — graph_subs","title":"Substitute node columns (columns 1 and 2) of a graph data frame with a dictionary of substitutions. — graph_subs","text":"Substitute node columns (columns 1 2) graph data frame dictionary substitutions.","code":""},{"path":"/reference/graph_subs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Substitute node columns (columns 1 and 2) of a graph data frame with a dictionary of substitutions. — graph_subs","text":"","code":"graph_subs(DF, df_subs)"},{"path":"/reference/graph_subs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Substitute node columns (columns 1 and 2) of a graph data frame with a dictionary of substitutions. — graph_subs","text":"DF graph data frame df_subs data frame substitutions made, first column substitute column 2 substitute . uses regular expressions.","code":""},{"path":"/reference/group_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"Group a sequence of entities in a POS dataframe — group_entities","title":"Group a sequence of entities in a POS dataframe — group_entities","text":"Group data.frame generated spacyr::spacy_parse, sequence entities","code":""},{"path":"/reference/group_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group a sequence of entities in a POS dataframe — group_entities","text":"","code":"group_entities(DF)"},{"path":"/reference/group_entities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group a sequence of entities in a POS dataframe — group_entities","text":"DF data.frame generated spacyr::spacy_parse. Crated spacy_parse()","code":""},{"path":"/reference/group_entities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group a sequence of entities in a POS dataframe — group_entities","text":"","code":"# example in English language t <- \"UnitedHealthcare boss Brian Thompson, 50, was fatally shot in the back on Wednesday morning outside the Hilton hotel in Midtown Manhattan... Investigators are using surveillance photos, bullet casings with cryptic messages written on them, and the suspect's movements to track him down. They are also working with the FBI and authorities in other states as the search expands beyond New York\" spacyr::spacy_initialize(model = \"en_core_web_lg\") #> spaCy is already initialized #> NULL spacyr::spacy_parse(t, dependency = T) |> group_entities() #> Error in UseMethod(\"group_by\"): no applicable method for 'group_by' applied to an object of class \"name\"  # example in Portuguese language spacyr::spacy_initialize(model = \"pt_core_news_lg\") #> spaCy is already initialized #> NULL \"Maria Jana ama John Smith e Maria é amada por Joaquim de Souza\" |>   spacyr::spacy_parse(dependency = T) |>   group_entities() #> Error in UseMethod(\"group_by\"): no applicable method for 'group_by' applied to an object of class \"name\""},{"path":"/reference/group_ppn.html","id":null,"dir":"Reference","previous_headings":"","what":"Group a POS of proper names — group_ppn","title":"Group a POS of proper names — group_ppn","text":"Group data.frame generated spacyr::spacy_parse(), sequence entity = PERS* / PERSON. works similar spacyr::entity_extract(), preserves dep_rel column","code":""},{"path":"/reference/group_ppn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group a POS of proper names — group_ppn","text":"","code":"group_ppn(DF)"},{"path":"/reference/group_ppn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group a POS of proper names — group_ppn","text":"DF data.frame generated spacyr::spacy_parse().","code":""},{"path":"/reference/group_ppn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group a POS of proper names — group_ppn","text":"","code":"# example in Portuguese language # spacy_finalize() # If spacy was previously initialized with another model. spacy_initialize(model = \"pt_core_news_lg\") #> Error in spacy_initialize(model = \"pt_core_news_lg\"): could not find function \"spacy_initialize\" \"Maria Jana ama John Smith e Maria é amada por Joaquim de Souza\" |>   spacyr::spacy_parse(dependency = T) |>   group_ppn() #> # A tibble: 13 × 10 #> # Groups:   name [2] #>    doc_id sentence_id token_id token   lemma  pos   head_token_id dep_rel entity #>    <chr>        <int>    <int> <chr>   <chr>  <chr>         <dbl> <chr>   <chr>  #>  1 text1            1        1 Maria   Maria  PROPN             2 compou… \"PERS… #>  2 text1            1        2 Jana    Jana   PROPN             3 compou… \"PERS… #>  3 text1            1        3 ama     ama    NOUN              3 ROOT    \"\"     #>  4 text1            1        4 John    John   PROPN             5 compou… \"PERS… #>  5 text1            1        5 Smith   Smith  PROPN             3 appos   \"PERS… #>  6 text1            1        6 e       e      PROPN            13 compou… \"\"     #>  7 text1            1        7 Maria   Maria  PROPN            13 nmod    \"NORP… #>  8 text1            1        8 é       é      PROPN            13 compou… \"\"     #>  9 text1            1        9 amada   amada  PROPN            13 compou… \"\"     #> 10 text1            1       10 por     por    PROPN            13 compou… \"\"     #> 11 text1            1       11 Joaquim Joaqu… PROPN            13 compou… \"PERS… #> 12 text1            1       12 de      de     PROPN            13 compou… \"PERS… #> 13 text1            1       13 Souza   Souza  PROPN             5 appos   \"PERS… #> # ℹ 1 more variable: name <chr>"},{"path":"/reference/group_seq_pos.html","id":null,"dir":"Reference","previous_headings":"","what":"collapse sequence of repeated POS into a single one — group_seq_pos","title":"collapse sequence of repeated POS into a single one — group_seq_pos","text":"collapse sequence repeated POS single one","code":""},{"path":"/reference/group_seq_pos.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"collapse sequence of repeated POS into a single one — group_seq_pos","text":"","code":"group_seq_pos(DF, POS = \"PROPN|NOUN\")"},{"path":"/reference/group_seq_pos.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"collapse sequence of repeated POS into a single one — group_seq_pos","text":"","code":"pos <- \"Mary Jane was the first to drink\" |> spacy_parse() #> Error in spacy_parse(\"Mary Jane was the first to drink\"): could not find function \"spacy_parse\" pos |> group_seq_pos() #> Error: object 'pos' not found"},{"path":"/reference/interactive_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"viz graph interactively — interactive_graph","title":"viz graph interactively — interactive_graph","text":"Visualize graphs interactively (package visNetwork). columns must named: \"\", \"label\", \"\" \"value\" (frequency triplet) 1st, 2nd 3rd columns taken .","code":""},{"path":"/reference/interactive_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"viz graph interactively — interactive_graph","text":"","code":"interactive_graph(graph_df, nodesIdSelection = TRUE, height = \"900px\")"},{"path":"/reference/interactive_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"viz graph interactively — interactive_graph","text":"graph_df dataframe graph data nodesIdSelection boolean value enable node selection. Default: TRUE.","code":""},{"path":"/reference/interactive_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"viz graph interactively — interactive_graph","text":"","code":"x <- ex_txt_wiki[2:44] |>   filter_by_query(\"Brian\") |>   parsePOS() #> Error: object 'ex_txt_wiki' not found g <- get_cooc_entities(x) #> Error: object 'x' not found g$edges |>   dplyr::rename(from = n1, to = n2) |>   viz_graph() #> Error in viz_graph(dplyr::rename(g$edges, from = n1, to = n2)): could not find function \"viz_graph\" g$edges |> viz_graph() #> Error in viz_graph(g$edges): could not find function \"viz_graph\""},{"path":"/reference/net_wordcloud.html","id":null,"dir":"Reference","previous_headings":"","what":"plot net wordcloud — net_wordcloud","title":"plot net wordcloud — net_wordcloud","text":"plot network co-occurrence terms, returned extract_graph dplyr::count(). size words compound words means individual frequency word/compound word. thickness links indicates often pair occur together.","code":""},{"path":"/reference/net_wordcloud.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plot net wordcloud — net_wordcloud","text":"","code":"net_wordcloud(text, df, head_n = 30, color = \"lightblue\")"},{"path":"/reference/net_wordcloud.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plot net wordcloud — net_wordcloud","text":"text original text used extract graph. necessary calculate individual frequency words. df dataframe co-occurrence, extracted `extract_graph()` `count(n1, n2)` head_n number nodes show - frequent","code":""},{"path":"/reference/net_wordcloud.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"plot net wordcloud — net_wordcloud","text":"","code":"# stopwords: my_sw <- c(stopwords::stopwords(language = \"en\", source = \"snowball\", simplify = TRUE), \"lol\")  txt_wiki |> # text available in the package   # because it is a vector, let's collapse it into a single element:   paste(collapse = \" \") |>   extract_graph(sw = my_sw) |>   networds::count_graphs() |> # counting the graphs   net_wordcloud(ex_txt_wiki, df = _) # plotting #> Tokenizing by sentences #> Error: object 'ex_txt_wiki' not found"},{"path":"/reference/parsePOS.html","id":null,"dir":"Reference","previous_headings":"","what":"parse tokenized text into POS — parsePOS","title":"parse tokenized text into POS — parsePOS","text":"list filtered text (function filter_by_query), tags POS/ parse spacyr. also renumber doc_id sentence_id. runs spacyr::entity_extract() spacyr::entity_consolidate() background, also runs os lists empty elements,  can returns single dataframe (`bind = TRUE`) renaming doc_id sentence_id,","code":""},{"path":"/reference/parsePOS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"parse tokenized text into POS — parsePOS","text":"","code":"parsePOS(txt, bind = TRUE, only_entities = TRUE)"},{"path":"/reference/parsePOS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"parse tokenized text into POS — parsePOS","text":"txt list filtered text bind TRUE (default), returns single dataframe. FALSE, returns List object entities TRUE (default), returns entities. FALSE, returns consolidated entities","code":""},{"path":"/reference/parsePOS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"parse tokenized text into POS — parsePOS","text":"","code":"txt_wiki |>   filter_by_query(\"Police\") |>   parsePOS() #>    doc_id sentence_id                                   entity entity_type #> 1   text1           1                             Pennsylvania         GPE #> 2   text1           1                               New_Jersey         GPE #> 3   text2           1                         Ted_Kaczynski_'s      PERSON #> 4   text2           1        Industrial_Society_and_Its_Future         ORG #> 5   text1           2 the_George_Washington_Bridge_Bus_Station         ORG #> 6   text1           2                          Upper_Manhattan         LOC #> 7   text1           1                             Central_Park         LOC #> 8   text1           1                            New_York_City         GPE #> 9   text1           2                                 Mangione         ORG #> 10  text1           2      the_San_Francisco_Police_Department         ORG #> 11  text1           2                                 Mangione         ORG #> 12  text1           2                            San_Francisco         GPE #> 13  text1           1                                  Altoona         GPE #> 14  text1           1                             Pennsylvania         GPE #> 15  text1           1                                 Mangione         ORG #> 16  text1           1                                 McDonald         ORG #> 17  text2           1                                     NYPD         ORG #> 18  text3           2                               New_Jersey         GPE #> 19  text3           2                                Manhattan         GPE #> 20  text4           1                                 American        NORP #> 21  text1           1                                 Mangione         ORG #> 22  text2           1                                 New_York         GPE #> 23  text2           1                             Joseph_Kenny      PERSON #> 24  text1           1                                 Mangione         ORG #> 25  text1           1                         Ted_Kaczynski_'s      PERSON #> 26  text1           1        Industrial_Society_and_Its_Future         ORG txt_wiki |>   filter_by_query(\"Police\") |>   parsePOS(only_entities = FALSE) #>     doc_id sentence_id token_id                                    token #> 1    text4           1        1                                       11 #> 2    text4           1        2                                        ] #> 3    text4           1        3                                 Mangione #> 4    text4           1        4                                      was #> 5    text4           1        5                                     held #> 6    text4           1        6                                  without #> 7    text4           1        7                                     bail #> 8    text4           1        8                                       in #> 9    text4           1        9                             Pennsylvania #> 10   text4           1       10                                       on #> 11   text4           1       11                                  charges #> 12   text4           1       12                                       of #> 13   text4           1       13                               possession #> 14   text4           1       14                                       of #> 15   text4           1       15                                       an #> 16   text4           1       16                               unlicensed #> 17   text4           1       17                                  firearm #> 18   text4           1       18                                        , #> 19   text4           1       19                                  forgery #> 20   text4           1       20                                        , #> 21   text4           1       21                                      and #> 22   text4           1       22                                providing #> 23   text4           1       23                                    false #> 24   text4           1       24                               New_Jersey #> 25   text4           1       25                                        - #> 26   text4           1       26                                 resident #> 27   text4           1       27                           identification #> 28   text4           1       28                                       to #> 29   text4           1       29                                   police #> 30   text4           1       30                                        . #> 31   text4           1        1                                        [ #> 32   text4           2        1                                       12 #> 33   text4           2        2                                        ] #> 34   text4           2        3                                   Police #> 35   text4           2        4                                  believe #> 36   text4           2        5                                     that #> 37   text4           2        6                                       he #> 38   text4           2        7                                      was #> 39   text4           2        8                                 inspired #> 40   text4           2        9                                       by #> 41   text4           2       10                         Ted_Kaczynski_'s #> 42   text4           2       11                                    essay #> 43   text4           2       12        Industrial_Society_and_Its_Future #> 44   text4           2       13                                        ( #> 45   text4           2       14                                     1995 #> 46   text4           2       15                                        ) #> 47   text4           2       16                                        , #> 48   text4           2       17                                      and #> 49   text4           2       18                                motivated #> 50   text4           2       19                                       by #> 51   text4           2       20                                      his #> 52   text4           2       21                                 personal #> 53   text4           2       22                                    views #> 54   text4           2       23                                       on #> 55   text4           2       24                                   health #> 56   text4           2       25                                insurance #> 57   text4           2       26                                        . #> 58   text4           2        1                                        [ #> 59  text12           1        1                                       39 #> 60  text12           1        2                                        ] #> 61  text12           1        1                                According #> 62  text12           1        2                                       to #> 63  text12           1        3                                      the #> 64  text12           1        4                                   police #> 65  text12           1        5                                        , #> 66  text12           1        6                                       he #> 67  text12           1        7                                     then #> 68  text12           1        8                                     left #> 69  text12           1        9                                      the #> 70  text12           1       10                                     city #> 71  text12           1       11                                     from #> 72  text12           1       12 the_George_Washington_Bridge_Bus_Station #> 73  text12           1       13                                  farther #> 74  text12           1       14                                   uptown #> 75  text12           1       15                                       in #> 76  text12           1       16                          Upper_Manhattan #> 77  text12           1       17                                        . #> 78  text12           1        1                                        [ #> 79  text13           1        1                                       49 #> 80  text13           1        2                                        ] #> 81  text13           1        1                              Accordingly #> 82  text13           1        2                                        , #> 83  text13           1        3                                   police #> 84  text13           1        4                                   stated #> 85  text13           1        5                                     they #> 86  text13           1        6                                      are #> 87  text13           1        7                            investigating #> 88  text13           1        8                                  whether #> 89  text13           1        9                                      the #> 90  text13           1       10                                    words #> 91  text13           1       11                                  suggest #> 92  text13           1       12                                      the #> 93  text13           1       13                                   killer #> 94  text13           1       14                                       's #> 95  text13           1       15                                   motive #> 96  text13           1       16                                        . #> 97  text13           1        1                                        [ #> 98  text14           1        1                                       50 #> 99  text14           1        2                                        ] #> 100 text14           1        3                                   Police #> 101 text14           1        4                                     said #> 102 text14           1        5                                     they #> 103 text14           1        6                                 believed #> 104 text14           1        7                                     they #> 105 text14           1        8                                    found #> 106 text14           1        9                                      the #> 107 text14           1       10                                  shooter #> 108 text14           1       11                                       's #> 109 text14           1       12                                 backpack #> 110 text14           1       13                                       in #> 111 text14           1       14                             Central_Park #> 112 text14           1       15                                       on #> 113 text14           1       16                        December_6_,_2024 #> 114 text14           1       17                                        . #> 115 text14           1        1                                        [ #> 116 text15           1        1                                      The #> 117 text15           1        2                            New_York_City #> 118 text15           1        3                                   Police #> 119 text15           1        4                               Department #> 120 text15           1        5                                  offered #> 121 text15           1        6                                        a #> 122 text15           1        7                                   reward #> 123 text15           1        8                           up_to_$_10,000 #> 124 text15           1        9                                      for #> 125 text15           1       10                              information #> 126 text15           1       11                                    about #> 127 text15           1       12                                      the #> 128 text15           1       13                                  shooter #> 129 text15           1       14                                       on #> 130 text15           1       15                        December_4_,_2024 #> 131 text15           1       16                                        . #> 132 text15           1        1                                        [ #> 133 text16           1        1                                      The #> 134 text16           1        2                                  shooter #> 135 text16           1        3                                      was #> 136 text16           1        4                                described #> 137 text16           1        5                                       by #> 138 text16           1        6                                   police #> 139 text16           1        7                                       as #> 140 text16           1        8                                        a #> 141 text16           1        9                                    white #> 142 text16           1       10                                      man #> 143 text16           1       11                                        , #> 144 text16           1       12                          approximately_6 #> 145 text16           1       13                                       ft #> 146 text16           1       14                                        1 #> 147 text16           1       15                                       in #> 148 text16           1       16                                        ( #> 149 text16           1       17                                   185_cm #> 150 text16           1       18                                        ) #> 151 text16           1       19                                     tall #> 152 text16           1       20                                        , #> 153 text16           1       21                                  wearing #> 154 text16           1       22                                        a #> 155 text16           1       23                                    light #> 156 text16           1       24                                    brown #> 157 text16           1       25                                       or #> 158 text16           1       26                                    cream #> 159 text16           1       27                                        - #> 160 text16           1       28                                  colored #> 161 text16           1       29                                   hooded #> 162 text16           1       30                                   jacket #> 163 text16           1       31                                        , #> 164 text16           1       32                                     dark #> 165 text16           1       33                                    pants #> 166 text16           1       34                                        , #> 167 text16           1       35                                      and #> 168 text16           1       36                                    black #> 169 text16           1       37                                 sneakers #> 170 text16           1       38                                     with #> 171 text16           1       39                                    white #> 172 text16           1       40                                    soles #> 173 text16           1       41                                        . #> 174 text16           2        1                           31][39][57][58 #> 175 text16           2        2                                        ] #> 176 text16           2        3                                   Police #> 177 text16           2        4                                     said #> 178 text16           2        5                                      the #> 179 text16           2        6                                  suspect #> 180 text16           2        7                                 appeared #> 181 text16           2        8                                       to #> 182 text16           2        9                                       be #> 183 text16           2       10                               proficient #> 184 text16           2       11                                       in #> 185 text16           2       12                                      the #> 186 text16           2       13                                      use #> 187 text16           2       14                                       of #> 188 text16           2       15                              firearms[30 #> 189 text16           2       16                                        ] #> 190 text16           2       17                                      and #> 191 text16           2       18                                      was #> 192 text16           2       19                                described #> 193 text16           2       20                                       as #> 194 text16           2       21                                    being #> 195 text16           2       22                                        \" #> 196 text16           2       23                                extremely #> 197 text16           2       24                                   camera #> 198 text16           2       25                                    savvy #> 199 text16           2       26                                        . #> 200 text16           2       27                                        \" #> 201 text16           2        1                                        [ #> 202 text18           1        1                                       69 #> 203 text18           1        2                                        ] #> 204 text18           1        1                                 Mangione #> 205 text18           1        2                                       's #> 206 text18           1        3                                   mother #> 207 text18           1        4                                contacted #> 208 text18           1        5      the_San_Francisco_Police_Department #> 209 text18           1        6                                        , #> 210 text18           1        7                                       as #> 211 text18           1        8                                      she #> 212 text18           1        9                                 believed #> 213 text18           1       10                                     that #> 214 text18           1       11                                 Mangione #> 215 text18           1       12                                    lived #> 216 text18           1       13                                       in #> 217 text18           1       14                            San_Francisco #> 218 text18           1       15                                      and #> 219 text18           1       16                                      had #> 220 text18           1       17                                        a #> 221 text18           1       18                                      job #> 222 text18           1       19                                       in #> 223 text18           1       20                                      the #> 224 text18           1       21                                     area #> 225 text18           1       22                                        . #> 226 text18           1        1                                        [ #> 227 text19           1        1                                    Local #> 228 text19           1        2                                   police #> 229 text19           1        3                                       in #> 230 text19           1        4                                  Altoona #> 231 text19           1        5                                        , #> 232 text19           1        6                             Pennsylvania #> 233 text19           1        7                                        , #> 234 text19           1        8                                 arrested #> 235 text19           1        9                                 Mangione #> 236 text19           1       10                                       on #> 237 text19           1       11                        December_9_,_2024 #> 238 text19           1       12                                        , #> 239 text19           1       13                                       at #> 240 text19           1       14                                        a #> 241 text19           1       15                                 McDonald #> 242 text19           1       16                                       's #> 243 text19           1       17                               restaurant #> 244 text19           1       18                                       in #> 245 text19           1       19                                      the #> 246 text19           1       20                                     city #> 247 text19           1       21                                        . #> 248 text19           2        1                                       An #> 249 text19           2        2                                 employee #> 250 text19           2        3                                    there #> 251 text19           2        4                                   called #> 252 text19           2        5                                      the #> 253 text19           2        6                                   police #> 254 text19           2        7                                       to #> 255 text19           2        8                                      say #> 256 text19           2        9                                     that #> 257 text19           2       10                                        a #> 258 text19           2       11                                 customer #> 259 text19           2       12                               recognized #> 260 text19           2       13                                      the #> 261 text19           2       14                                  suspect #> 262 text19           2       15                                     from #> 263 text19           2       16                                   images #> 264 text19           2       17                                 released #> 265 text19           2       18                                       by #> 266 text19           2       19                                      the #> 267 text19           2       20                                     NYPD #> 268 text19           2       21                                        . #> 269 text19           2        1                                        [ #> 270 text19           3        1                                       63 #> 271 text19           3        2                                        ] #> 272 text19           3        1                                       In #> 273 text19           3        2                                      his #> 274 text19           3        3                                      bag #> 275 text19           3        4                                     they #> 276 text19           3        5                                    found #> 277 text19           3        6                                        a #> 278 text19           3        7                                       3D #> 279 text19           3        8                                        - #> 280 text19           3        9                                  printed #> 281 text19           3       10                                      gun #> 282 text19           3       11                                      and #> 283 text19           3       12                                        a #> 284 text19           3       13                                       3D #> 285 text19           3       14                                        - #> 286 text19           3       15                                  printed #> 287 text19           3       16                               suppressor #> 288 text19           3       17                                        , #> 289 text19           3       18                                    which #> 290 text19           3       19                                      the #> 291 text19           3       20                                   police #> 292 text19           3       21                                    claim #> 293 text19           3       22                                      are #> 294 text19           3       23                               consistent #> 295 text19           3       24                                     with #> 296 text19           3       25                                      the #> 297 text19           3       26                                   weapon #> 298 text19           3       27                                     used #> 299 text19           3       28                                       in #> 300 text19           3       29                                      the #> 301 text19           3       30                                 shooting #> 302 text19           3       31                                        , #> 303 text19           3       32                                      and #> 304 text19           3       33                                        a #> 305 text19           3       34                                falsified #> 306 text19           3       35                               New_Jersey #> 307 text19           3       36                                   driver #> 308 text19           3       37                                       's #> 309 text19           3       38                                  license #> 310 text19           3       39                                     with #> 311 text19           3       40                                      the #> 312 text19           3       41                                     same #> 313 text19           3       42                                     name #> 314 text19           3       43                                       as #> 315 text19           3       44                                      the #> 316 text19           3       45                                      one #> 317 text19           3       46                                     used #> 318 text19           3       47                                       by #> 319 text19           3       48                                      the #> 320 text19           3       49                                  alleged #> 321 text19           3       50                                  shooter #> 322 text19           3       51                                       to #> 323 text19           3       52                                    check #> 324 text19           3       53                                     into #> 325 text19           3       54                                      the #> 326 text19           3       55                                Manhattan #> 327 text19           3       56                                   hostel #> 328 text19           3       57                                        . #> 329 text19           3        1                                        [ #> 330 text19           4        1                             8][73][3][74 #> 331 text19           4        2                                        ] #> 332 text19           4        3                                          #> 333 text19           4        4                                      The #> 334 text19           4        5                                   police #> 335 text19           4        6                                     also #> 336 text19           4        7                                     said #> 337 text19           4        8                                     that #> 338 text19           4        9                                     when #> 339 text19           4       10                                     they #> 340 text19           4       11                                 arrested #> 341 text19           4       12                                 Mangione #> 342 text19           4       13                                        , #> 343 text19           4       14                                     they #> 344 text19           4       15                                    found #> 345 text19           4       16                                        a #> 346 text19           4       17                       three_-_page,[74_] #> 347 text19           4       18                                      262 #> 348 text19           4       19                                        - #> 349 text19           4       20                                     word #> 350 text19           4       21                              handwritten #> 351 text19           4       22                                 document #> 352 text19           4       23                                    about #> 353 text19           4       24                                      the #> 354 text19           4       25                                 American #> 355 text19           4       26                               healthcare #> 356 text19           4       27                                   system #> 357 text19           4       28                                        , #> 358 text19           4       29                                    which #> 359 text19           4       30                                     they #> 360 text19           4       31                            characterized #> 361 text19           4       32                                       as #> 362 text19           4       33                                        a #> 363 text19           4       34                                manifesto #> 364 text19           4       35                                        . #> 365 text19           4        1                                        [ #> 366 text23           1        1                                       85 #> 367 text23           1        2                                        ] #> 368 text23           1        3                                   Police #> 369 text23           1        4                                  believe #> 370 text23           1        5                                      the #> 371 text23           1        6                                   motive #> 372 text23           1        7                                      was #> 373 text23           1        8                                  related #> 374 text23           1        9                                       to #> 375 text23           1       10                                       an #> 376 text23           1       11                                   injury #> 377 text23           1       12                                     that #> 378 text23           1       13                                 Mangione #> 379 text23           1       14                                      had #> 380 text23           1       15                                 suffered #> 381 text23           1       16                                     that #> 382 text23           1       17                                   caused #> 383 text23           1       18                                      him #> 384 text23           1       19                                       to #> 385 text23           1       20                                    visit #> 386 text23           1       21                                      the #> 387 text23           1       22                                emergency #> 388 text23           1       23                                     room #> 389 text23           1       24                                       in #> 390 text23           1       25                                July_2023 #> 391 text23           1       26                                        . #> 392 text23           2        1                                 New_York #> 393 text23           2        2                                   Police #> 394 text23           2        3                                    Chief #> 395 text23           2        4                                       of #> 396 text23           2        5                               Detectives #> 397 text23           2        6                             Joseph_Kenny #> 398 text23           2        7                                 believes #> 399 text23           2        8                                 Mangione #> 400 text23           2        9                                      may #> 401 text23           2       10                                     have #> 402 text23           2       11                                 targeted #> 403 text23           2       12                                     them #> 404 text23           2       13                                  because #> 405 text23           2       14                                       of #> 406 text23           2       15                                      the #> 407 text23           2       16                                  company #> 408 text23           2       17                                       's #> 409 text23           2       18                                     size #> 410 text23           2       19                                        . #> 411 text23           2        1                                        [ #> 412 text24           1        1                                   Police #> 413 text24           1        2                                  believe #> 414 text24           1        3                                     that #> 415 text24           1        4                                 Mangione #> 416 text24           1        5                                      was #> 417 text24           1        6                                 inspired #> 418 text24           1        7                                       by #> 419 text24           1        8                         Ted_Kaczynski_'s #> 420 text24           1        9        Industrial_Society_and_Its_Future #> 421 text24           1       10                                        . #> 422 text24           1        1                                        [ #>                                        lemma    pos entity_type #> 1                                         11 ENTITY    CARDINAL #> 2                                          ]  PUNCT             #> 3                                   mangione   NOUN             #> 4                                         be    AUX             #> 5                                       hold   VERB             #> 6                                    without    ADP             #> 7                                       bail   NOUN             #> 8                                         in    ADP             #> 9                               Pennsylvania ENTITY         GPE #> 10                                        on    ADP             #> 11                                    charge   NOUN             #> 12                                        of    ADP             #> 13                                possession   NOUN             #> 14                                        of    ADP             #> 15                                        an    DET             #> 16                                unlicensed    ADJ             #> 17                                   firearm   NOUN             #> 18                                         ,  PUNCT             #> 19                                   forgery   NOUN             #> 20                                         ,  PUNCT             #> 21                                       and  CCONJ             #> 22                                   provide   VERB             #> 23                                     false    ADJ             #> 24                                New_Jersey ENTITY         GPE #> 25                                         -  PUNCT             #> 26                                  resident   NOUN             #> 27                            identification   NOUN             #> 28                                        to    ADP             #> 29                                    police   NOUN             #> 30                                         .  PUNCT             #> 31                                         [      X             #> 32                                        12 ENTITY    CARDINAL #> 33                                         ]  PUNCT             #> 34                                    Police   NOUN             #> 35                                   believe   VERB             #> 36                                      that  SCONJ             #> 37                                        he   PRON             #> 38                                        be    AUX             #> 39                                   inspire   VERB             #> 40                                        by    ADP             #> 41                          Ted_Kaczynski_'s ENTITY      PERSON #> 42                                     essay    ADJ             #> 43         Industrial_Society_and_its_future ENTITY         ORG #> 44                                         (  PUNCT             #> 45                                      1995 ENTITY        DATE #> 46                                         )  PUNCT             #> 47                                         ,  PUNCT             #> 48                                       and  CCONJ             #> 49                                  motivate   VERB             #> 50                                        by    ADP             #> 51                                       his   PRON             #> 52                                  personal    ADJ             #> 53                                      view   NOUN             #> 54                                        on    ADP             #> 55                                    health   NOUN             #> 56                                 insurance   NOUN             #> 57                                         .  PUNCT             #> 58                                         [      X             #> 59                                        39 ENTITY    CARDINAL #> 60                                         ]  PUNCT             #> 61                                    accord   VERB             #> 62                                        to    ADP             #> 63                                       the    DET             #> 64                                    police   NOUN             #> 65                                         ,  PUNCT             #> 66                                        he   PRON             #> 67                                      then    ADV             #> 68                                     leave   VERB             #> 69                                       the    DET             #> 70                                      city   NOUN             #> 71                                      from    ADP             #> 72  the_George_Washington_Bridge_Bus_Station ENTITY         ORG #> 73                                       far    ADV             #> 74                                    uptown   VERB             #> 75                                        in    ADP             #> 76                           Upper_Manhattan ENTITY         LOC #> 77                                         .  PUNCT             #> 78                                         [      X             #> 79                                        49 ENTITY    CARDINAL #> 80                                         ]  PUNCT             #> 81                               accordingly    ADV             #> 82                                         ,  PUNCT             #> 83                                    police   NOUN             #> 84                                     state   VERB             #> 85                                      they   PRON             #> 86                                        be    AUX             #> 87                               investigate   VERB             #> 88                                   whether  SCONJ             #> 89                                       the    DET             #> 90                                      word   NOUN             #> 91                                   suggest   VERB             #> 92                                       the    DET             #> 93                                    killer   NOUN             #> 94                                        's   PART             #> 95                                    motive   NOUN             #> 96                                         .  PUNCT             #> 97                                         [      X             #> 98                                        50 ENTITY    CARDINAL #> 99                                         ]  PUNCT             #> 100                                   Police   NOUN             #> 101                                      say   VERB             #> 102                                     they   PRON             #> 103                                  believe   VERB             #> 104                                     they   PRON             #> 105                                     find   VERB             #> 106                                      the    DET             #> 107                                  shooter   NOUN             #> 108                                       's   PART             #> 109                                 backpack   NOUN             #> 110                                       in    ADP             #> 111                             Central_Park ENTITY         LOC #> 112                                       on    ADP             #> 113                        December_6_,_2024 ENTITY        DATE #> 114                                        .  PUNCT             #> 115                                        [      X             #> 116                                      the    DET             #> 117                            New_York_City ENTITY         GPE #> 118                                   Police  PROPN             #> 119                               Department  PROPN             #> 120                                    offer   VERB             #> 121                                        a    DET             #> 122                                   reward   NOUN             #> 123                           up_to_$_10,000 ENTITY       MONEY #> 124                                      for    ADP             #> 125                              information   NOUN             #> 126                                    about    ADP             #> 127                                      the    DET             #> 128                                  shooter   NOUN             #> 129                                       on    ADP             #> 130                        December_4_,_2024 ENTITY        DATE #> 131                                        .  PUNCT             #> 132                                        [      X             #> 133                                      the    DET             #> 134                                  shooter   NOUN             #> 135                                       be    AUX             #> 136                                 describe   VERB             #> 137                                       by    ADP             #> 138                                   police   NOUN             #> 139                                       as    ADP             #> 140                                        a    DET             #> 141                                    white    ADJ             #> 142                                      man   NOUN             #> 143                                        ,  PUNCT             #> 144                          approximately_6 ENTITY    CARDINAL #> 145                                       ft   NOUN             #> 146                                        1    NUM             #> 147                                       in    ADP             #> 148                                        (  PUNCT             #> 149                                   185_cm ENTITY    QUANTITY #> 150                                        )  PUNCT             #> 151                                     tall    ADJ             #> 152                                        ,  PUNCT             #> 153                                     wear   VERB             #> 154                                        a    DET             #> 155                                    light    ADJ             #> 156                                    brown    ADJ             #> 157                                       or  CCONJ             #> 158                                    cream   NOUN             #> 159                                        -  PUNCT             #> 160                                    color   VERB             #> 161                                   hooded    ADJ             #> 162                                   jacket   NOUN             #> 163                                        ,  PUNCT             #> 164                                     dark    ADJ             #> 165                                     pant   NOUN             #> 166                                        ,  PUNCT             #> 167                                      and  CCONJ             #> 168                                    black    ADJ             #> 169                                  sneaker   NOUN             #> 170                                     with    ADP             #> 171                                    white    ADJ             #> 172                                      sol   NOUN             #> 173                                        .  PUNCT             #> 174                           31][39][57][58 ENTITY    CARDINAL #> 175                                        ]  PUNCT             #> 176                                   Police   NOUN             #> 177                                      say   VERB             #> 178                                      the    DET             #> 179                                  suspect   NOUN             #> 180                                   appear   VERB             #> 181                                       to   PART             #> 182                                       be    AUX             #> 183                               proficient    ADJ             #> 184                                       in    ADP             #> 185                                      the    DET             #> 186                                      use   NOUN             #> 187                                       of    ADP             #> 188                              firearms[30   NOUN             #> 189                                        ]  PUNCT             #> 190                                      and  CCONJ             #> 191                                       be    AUX             #> 192                                 describe   VERB             #> 193                                       as    ADP             #> 194                                       be    AUX             #> 195                                        \"  PUNCT             #> 196                                extremely    ADV             #> 197                                   camera   NOUN             #> 198                                    savvy   NOUN             #> 199                                        .  PUNCT             #> 200                                        \"  PUNCT             #> 201                                        [      X             #> 202                                       69 ENTITY    CARDINAL #> 203                                        ]  PUNCT             #> 204                                 Mangione ENTITY         ORG #> 205                                       's   PART             #> 206                                   mother   NOUN             #> 207                                  contact   VERB             #> 208      the_San_Francisco_Police_Department ENTITY         ORG #> 209                                        ,  PUNCT             #> 210                                       as  SCONJ             #> 211                                      she   PRON             #> 212                                  believe   VERB             #> 213                                     that  SCONJ             #> 214                                 Mangione ENTITY         ORG #> 215                                     live   VERB             #> 216                                       in    ADP             #> 217                            San_Francisco ENTITY         GPE #> 218                                      and  CCONJ             #> 219                                     have   VERB             #> 220                                        a    DET             #> 221                                      job   NOUN             #> 222                                       in    ADP             #> 223                                      the    DET             #> 224                                     area   NOUN             #> 225                                        .  PUNCT             #> 226                                        [      X             #> 227                                    local    ADJ             #> 228                                   police   NOUN             #> 229                                       in    ADP             #> 230                                  Altoona ENTITY         GPE #> 231                                        ,  PUNCT             #> 232                             Pennsylvania ENTITY         GPE #> 233                                        ,  PUNCT             #> 234                                   arrest   VERB             #> 235                                 Mangione ENTITY         ORG #> 236                                       on    ADP             #> 237                        December_9_,_2024 ENTITY        DATE #> 238                                        ,  PUNCT             #> 239                                       at    ADP             #> 240                                        a    DET             #> 241                                 McDonald ENTITY         ORG #> 242                                       's   PART             #> 243                               restaurant   NOUN             #> 244                                       in    ADP             #> 245                                      the    DET             #> 246                                     city   NOUN             #> 247                                        .  PUNCT             #> 248                                       an    DET             #> 249                                 employee   NOUN             #> 250                                    there    ADV             #> 251                                     call   VERB             #> 252                                      the    DET             #> 253                                   police   NOUN             #> 254                                       to   PART             #> 255                                      say   VERB             #> 256                                     that  SCONJ             #> 257                                        a    DET             #> 258                                 customer   NOUN             #> 259                                recognize   VERB             #> 260                                      the    DET             #> 261                                  suspect   NOUN             #> 262                                     from    ADP             #> 263                                    image   NOUN             #> 264                                  release   VERB             #> 265                                       by    ADP             #> 266                                      the    DET             #> 267                                     NYPD ENTITY         ORG #> 268                                        .  PUNCT             #> 269                                        [      X             #> 270                                       63 ENTITY    CARDINAL #> 271                                        ]  PUNCT             #> 272                                       in    ADP             #> 273                                      his   PRON             #> 274                                      bag   NOUN             #> 275                                     they   PRON             #> 276                                     find   VERB             #> 277                                        a    DET             #> 278                                       3d   NOUN             #> 279                                        -  PUNCT             #> 280                                    print   VERB             #> 281                                      gun   NOUN             #> 282                                      and  CCONJ             #> 283                                        a    DET             #> 284                                       3d   NOUN             #> 285                                        -  PUNCT             #> 286                                    print   VERB             #> 287                               suppressor   NOUN             #> 288                                        ,  PUNCT             #> 289                                    which   PRON             #> 290                                      the    DET             #> 291                                   police   NOUN             #> 292                                    claim   NOUN             #> 293                                       be    AUX             #> 294                               consistent    ADJ             #> 295                                     with    ADP             #> 296                                      the    DET             #> 297                                   weapon   NOUN             #> 298                                      use   VERB             #> 299                                       in    ADP             #> 300                                      the    DET             #> 301                                 shooting   NOUN             #> 302                                        ,  PUNCT             #> 303                                      and  CCONJ             #> 304                                        a    DET             #> 305                                  falsify   VERB             #> 306                               New_Jersey ENTITY         GPE #> 307                                   driver   NOUN             #> 308                                       's   PART             #> 309                                  license   NOUN             #> 310                                     with    ADP             #> 311                                      the    DET             #> 312                                     same    ADJ             #> 313                                     name   NOUN             #> 314                                       as    ADP             #> 315                                      the    DET             #> 316                                      one   NOUN             #> 317                                      use   VERB             #> 318                                       by    ADP             #> 319                                      the    DET             #> 320                                  alleged    ADJ             #> 321                                  shooter   NOUN             #> 322                                       to   PART             #> 323                                    check   VERB             #> 324                                     into    ADP             #> 325                                      the    DET             #> 326                                Manhattan ENTITY         GPE #> 327                                   hostel   NOUN             #> 328                                        .  PUNCT             #> 329                                        [      X             #> 330                             8][73][3][74 ENTITY    CARDINAL #> 331                                        ]  PUNCT             #> 332                                           SPACE             #> 333                                      the    DET             #> 334                                   police   NOUN             #> 335                                     also    ADV             #> 336                                      say   VERB             #> 337                                     that  SCONJ             #> 338                                     when  SCONJ             #> 339                                     they   PRON             #> 340                                   arrest   VERB             #> 341                                 Mangione  PROPN             #> 342                                        ,  PUNCT             #> 343                                     they   PRON             #> 344                                     find   VERB             #> 345                                        a    DET             #> 346                       three_-_page,[74_] ENTITY    CARDINAL #> 347                                      262 ENTITY    CARDINAL #> 348                                        -  PUNCT             #> 349                                     word   NOUN             #> 350                              handwritten    ADJ             #> 351                                 document   NOUN             #> 352                                    about    ADP             #> 353                                      the    DET             #> 354                                 american ENTITY        NORP #> 355                               healthcare   NOUN             #> 356                                   system   NOUN             #> 357                                        ,  PUNCT             #> 358                                    which   PRON             #> 359                                     they   PRON             #> 360                             characterize   VERB             #> 361                                       as    ADP             #> 362                                        a    DET             #> 363                                manifesto   NOUN             #> 364                                        .  PUNCT             #> 365                                        [      X             #> 366                                       85 ENTITY    CARDINAL #> 367                                        ]  PUNCT             #> 368                                   Police   NOUN             #> 369                                  believe   VERB             #> 370                                      the    DET             #> 371                                   motive   NOUN             #> 372                                       be    AUX             #> 373                                   relate   VERB             #> 374                                       to    ADP             #> 375                                       an    DET             #> 376                                   injury   NOUN             #> 377                                     that   PRON             #> 378                                 Mangione ENTITY         ORG #> 379                                     have    AUX             #> 380                                   suffer   VERB             #> 381                                     that   PRON             #> 382                                    cause   VERB             #> 383                                       he   PRON             #> 384                                       to   PART             #> 385                                    visit   VERB             #> 386                                      the    DET             #> 387                                emergency   NOUN             #> 388                                     room   NOUN             #> 389                                       in    ADP             #> 390                                July_2023 ENTITY        DATE #> 391                                        .  PUNCT             #> 392                                 New_York ENTITY         GPE #> 393                                   Police  PROPN             #> 394                                    Chief  PROPN             #> 395                                       of    ADP             #> 396                               Detectives  PROPN             #> 397                             Joseph_Kenny ENTITY      PERSON #> 398                                  believe   VERB             #> 399                                 Mangione  PROPN             #> 400                                      may    AUX             #> 401                                     have    AUX             #> 402                                   target   VERB             #> 403                                     they   PRON             #> 404                                  because  SCONJ             #> 405                                       of    ADP             #> 406                                      the    DET             #> 407                                  company   NOUN             #> 408                                       's   PART             #> 409                                     size   NOUN             #> 410                                        .  PUNCT             #> 411                                        [      X             #> 412                                   Police   NOUN             #> 413                                  believe   VERB             #> 414                                     that  SCONJ             #> 415                                 Mangione ENTITY         ORG #> 416                                       be    AUX             #> 417                                  inspire   VERB             #> 418                                       by    ADP             #> 419                         Ted_Kaczynski_'s ENTITY      PERSON #> 420        Industrial_Society_and_its_Future ENTITY         ORG #> 421                                        .  PUNCT             #> 422                                        [      X"},{"path":"/reference/plot_graph2.html","id":null,"dir":"Reference","previous_headings":"","what":"weighted word graph viz — plot_graph2","title":"weighted word graph viz — plot_graph2","text":"Plot network co-ocurrence terms. word frequencies can vary significantly, differences text size can substantial. Therefore, instead adjusting text size, vary dot/node size, ensuring text remains consistently sized maintains readability. also possible normalize result log.","code":""},{"path":"/reference/plot_graph2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"weighted word graph viz — plot_graph2","text":"","code":"plot_graph2(   text,   df,   head_n = 30,   edge_color = \"lightblue\",   edge_alpha = 0.5,   node_alpha = 0.5,   text_color = \"black\",   text_size = 1,   scale_graph = \"scale_values\" )"},{"path":"/reference/plot_graph2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"weighted word graph viz — plot_graph2","text":"text input text df dataframe co-occurrence, extracted `extract_graph()` `count(n1, n2)` head_n number nodes show - frequent edge_color color edges edge_alpha transparency edges. Values 0 1. node_alpha transparency nodes text_color color text nodes text_size font size nodes scale_graph name function normalize result. Sometime, range numbers wide graph becomes unreadable. Applying function normalize result can improve readability, example using `scale_graph = \"log2\"`, `\"log10\"`","code":""},{"path":"/reference/plot_graph2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"weighted word graph viz — plot_graph2","text":"plot graph co-occurrence terms, returned extract_graph","code":""},{"path":"/reference/plot_graph2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"weighted word graph viz — plot_graph2","text":"","code":"# plot_graph(txt, df = graph_count, head_n = 50, scale_graph = \"log2\")"},{"path":"/reference/plot_pos_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"plot static graph from POS list — plot_pos_graph","title":"plot static graph from POS list — plot_pos_graph","text":"plot graph co-occurrence terms, returned `get_cooc_entities()`. function uses ggraph, color options can work expected due package incompatibilities. plot_pos_graph based ggraph, based ggplot2. means possible customize graph ggplot2 functions, like `labs()`.","code":""},{"path":"/reference/plot_pos_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plot static graph from POS list — plot_pos_graph","text":"","code":"plot_pos_graph(   pos_list,   n_head = 30,   edge_color = \"lightblue\",   edge_alpha = 0.1,   font_size = 2,   font_color = \"black\",   point_fill = \"firebrick4\",   point_alpha = 0.3,   point_color = \"firebrick4\",   graph_layout = \"graphopt\" )"},{"path":"/reference/plot_pos_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plot static graph from POS list — plot_pos_graph","text":"pos_list list POS, returned `get_cooc_entities()` n_head maximum number edges show. freq column must ordered edge_color color edges edge_alpha transparency edges. Values 0 1. font_size integer. font size nodes. Values 1 2. font_color color nodes. point_fill color nodes point_alpha transparency nodes point_color color nodes graph_layout layout graph","code":""},{"path":"/reference/plot_pos_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"plot static graph from POS list — plot_pos_graph","text":"","code":"gr <- ex_txt_wiki[2:44] |>   filter_by_query(\"Police\") |>   parsePOS() #> Error: object 'ex_txt_wiki' not found gr <- gr |> get_cooc_entities() #> Error: object 'gr' not found plot_pos_graph(gr) #> Error in eval(expr, envir) : object 'gr' not found #> Warning: restarting interrupted promise evaluation #> Error in eval(expr, envir) : object 'gr' not found #> Warning: restarting interrupted promise evaluation #> Error in eval(expr, envir) : object 'gr' not found #> Error in plot_pos_graph(gr): object 'head_edges' not found  gr <- ex_txt_wiki[2:44] |>   filter_by_query(\"Brian\") |>   parsePOS() #> Error: object 'ex_txt_wiki' not found gr <- gr |> get_cooc_entities() #> Error: object 'gr' not found plot_pos_graph(gr) #> Error in eval(expr, envir) : object 'gr' not found #> Warning: restarting interrupted promise evaluation #> Error in eval(expr, envir) : object 'gr' not found #> Warning: restarting interrupted promise evaluation #> Error in eval(expr, envir) : object 'gr' not found #> Error in plot_pos_graph(gr): object 'head_edges' not found"},{"path":"/reference/q_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"quick word graph — q_plot","title":"quick word graph — q_plot","text":"Plot network co-ocurrence terms. options, see `net_wordcloud()`.","code":""},{"path":"/reference/q_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"quick word graph — q_plot","text":"","code":"q_plot(graph_list, color = \"lightblue\")"},{"path":"/reference/q_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"quick word graph — q_plot","text":"color color edges. Default: \"lightblue\". edge_df dataframe co-occurrence, extracted `extract_graph()`","code":""},{"path":"/reference/q_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"quick word graph — q_plot","text":"","code":"library(txtnet) #>  #> Attaching package: ‘txtnet’ #> The following objects are masked from ‘package:networds’: #>  #>     connectors, count_vec, entity_list_2_graph2, extract_entities_l, #>     extract_entities_v, extract_entity, extract_graph, #>     extract_graph_pos, extract_relation, filter_by_query, filter_ego, #>     filter_ppn, g, get_cooc, get_cooc_entities, get_entities, #>     get_graph_from_txt, get_neighbors, get_node_id, group_entities, #>     group_ppn, group_seq_pos, interactive_graph, net_wordcloud, #>     parsePOS, plot_graph2, plot_pos_graph, q_plot, rename_cols, #>     split_graph, subs_ppn  g <- ex_txt_wiki[2:44] |>   filter_by_query(\"Police\") |>   parsePOS() #> Error: object 'ex_txt_wiki' not found g <- get_cooc_entities(g) #> Error in group_by(d_tmp, ..., .add = add): Must group by variables found in `.data`. #> ✖ Column `doc_id` is not found. q_plot(g) #> Error in tidygraph::as_tbl_graph(graph_list$edges): No support for <NULL> objects #> Caused by error in `UseMethod()`: #> ! no applicable method for 'as.igraph' applied to an object of class \"NULL\""},{"path":"/reference/rename_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"rename cols from count rename cols froum count to use with other functions, renaming to ","title":"rename cols from count rename cols froum count to use with other functions, renaming to ","text":"rename cols count rename cols froum count use functions, renaming \"\", \"\" \"value\" n (frequency.)'","code":""},{"path":"/reference/rename_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"rename cols from count rename cols froum count to use with other functions, renaming to ","text":"","code":"rename_cols(df)"},{"path":"/reference/split_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"split a tidy graph (each line is graph with at least 2 nodes) into two dataframes within a list: one with de nodes and its indexes, a second dataframe with de edges. — split_graph","title":"split a tidy graph (each line is graph with at least 2 nodes) into two dataframes within a list: one with de nodes and its indexes, a second dataframe with de edges. — split_graph","text":"split tidy graph (line graph least 2 nodes) two dataframes within list: one de nodes indexes, second dataframe de edges.","code":""},{"path":"/reference/split_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"split a tidy graph (each line is graph with at least 2 nodes) into two dataframes within a list: one with de nodes and its indexes, a second dataframe with de edges. — split_graph","text":"","code":"split_graph(DF_graph)"},{"path":"/reference/split_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"split a tidy graph (each line is graph with at least 2 nodes) into two dataframes within a list: one with de nodes and its indexes, a second dataframe with de edges. — split_graph","text":"","code":"DF <- data.frame(from = c(\"Amanda\", \"Bruno\", \"Carlos\", \"Daniel\"), to = c(\"Bruno\", \"Carlos\", \"Daniel\", \"Amanda\"))  split_graph(DF) #> $node_index #> # A tibble: 4 × 2 #>      id label  #>   <int> <chr>  #> 1     1 Amanda #> 2     2 Bruno  #> 3     3 Carlos #> 4     4 Daniel #>  #> $edges #> # A tibble: 4 × 2 #>   from  to    #>   <chr> <chr> #> 1 1     2     #> 2 2     3     #> 3 3     4     #> 4 4     1     #>"},{"path":"/reference/subs_ppn.html","id":null,"dir":"Reference","previous_headings":"","what":"Substitute proper names/entities spaces with underscore in the text. — subs_ppn","title":"Substitute proper names/entities spaces with underscore in the text. — subs_ppn","text":"given text vector entities, substitutes spaces underscores, entities identified.","code":""},{"path":"/reference/subs_ppn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Substitute proper names/entities spaces with underscore in the text. — subs_ppn","text":"","code":"subs_ppn(text, entities, method = \"normal\")"},{"path":"/reference/subs_ppn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Substitute proper names/entities spaces with underscore in the text. — subs_ppn","text":"text input text entities input vector, exported `extract_entity()`","code":""},{"path":"/reference/subs_ppn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Substitute proper names/entities spaces with underscore in the text. — subs_ppn","text":"","code":"texto_teste <- \"José da Silva e Fulano de Tal foram, bla Maria Silva. E depois disso, bla Joaquim José da Silva Xavier no STF\" ppn <- texto_teste |> extract_entity(connectors(\"pt\"), sw = gen_stopwords(\"pt\")) #> Error in gen_stopwords(\"pt\"): could not find function \"gen_stopwords\" texto_teste |> subs_ppn(ppn) #> Error: object 'ppn' not found texto_teste |> subs_ppn(ppn, method = \"loop\") #> Error: object 'ppn' not found text <- texto_teste |> subs_ppn(ppn) #> Error: object 'ppn' not found texd #> Error: object 'texd' not found"},{"path":"/reference/text_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"A Text Sample — text_sample","title":"A Text Sample — text_sample","text":"sample text text https://www.bbc.com/news/articles/c7ve36zg0e5o","code":""},{"path":"/reference/text_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A Text Sample — text_sample","text":"","code":"text_sample"},{"path":"/reference/text_sample.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A Text Sample — text_sample","text":"object class character length 1.","code":""},{"path":"/reference/text_sample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A Text Sample — text_sample","text":"","code":"data(package = \"txtnet\") head(text_sample) #> [1] \"A man is being questioned about the fatal shooting of a healthcare insurance boss in New York last week, officials have told the BBC's US partner CBS News.\\nUnitedHealthcare boss Brian Thompson, 50, was fatally shot in the back on Wednesday morning last week outside the Hilton hotel in Midtown Manhattan.\\nPolice say Thompson was targeted in a pre-planned killing, for which they do not yet have a motive. Nor have officers revealed a name of the suspect.\\nIt is not clear if the person being questioned is the suspect who authorities have been searching for.\\nInvestigators have been using surveillance photos, bullet casings with cryptic messages written on them, and the suspect's movements to track him down. They are also working with the FBI and authorities in other states...\\nThe manhunt for a suspect who gunned down a healthcare chief executive in New York is now in its third day, with police chasing several different leads.\\nUnitedHealthcare boss Brian Thompson, 50, was fatally shot in the back on Wednesday morning outside the Hilton hotel in Midtown Manhattan.\\nPolice say Thompson was targeted in a pre-planned killing, for which they do not yet have a motive.\\nInvestigators are using surveillance photos, bullet casings with cryptic messages written on them, and the suspect's movements to track him down. They are also working with the FBI and authorities in other states as the search expands beyond New York.\""},{"path":"/reference/txt_wiki.html","id":null,"dir":"Reference","previous_headings":"","what":"A Text Sample from wikipedia — txt_wiki","title":"A Text Sample from wikipedia — txt_wiki","text":"vector sample text Wikipedia","code":""},{"path":"/reference/txt_wiki.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A Text Sample from wikipedia — txt_wiki","text":"","code":"txt_wiki"},{"path":"/reference/txt_wiki.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A Text Sample from wikipedia — txt_wiki","text":"vector text 43 elements.","code":""},{"path":"/reference/txt_wiki.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"A Text Sample from wikipedia — txt_wiki","text":"<https://en.wikipedia.org/wiki/Killing_of_Brian_Thompson>","code":""},{"path":"/reference/txt_wiki.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A Text Sample from wikipedia — txt_wiki","text":"","code":"data(ex_txt_wiki) str(ex_txt_wiki) #> Error: object 'ex_txt_wiki' not found"}]
