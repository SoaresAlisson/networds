---
title: "02-Extract entity co-ocurrences with POS"
check_title: FALSE
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{entities_and_relation_extraction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
Loading the library

```{r setup library}
library(networds)
library(spacyr)
```
```{r browser, echo=FALSE}
options(browser = "firefox")
```


Extracting proper names or entities with regex are quick, but has its limitations. 
In this section, we will use the Part of Speech (POS) tags to extract the entities, proper names, noun phrases and its co-occurrences to generate graphs.
In R, to tag words with POS, the main packages are {UDPipe} and {SpacyR}.

## Installing SpacyR
First of all, we need to install the {[spacyR](https://spacyr.quanteda.io/)} package.
It is a wrap around the Spacy package in Python, and SpacyR deals with the boring parts of creating an exclusive python virtual environment.
This package will extract the NER (named entities) and POS (part of speech tagging).

```{r install spacyR, eval = FALSE}
install.packages("spacyr")

# if you prefer, or maybe if the CRAN version is buggy, install the GitHub one:
pak::pkg_install("quanteda/spacyr")

# to Install spaCy and requirements (python env). With empty parameters, it will
# install the default “en_core_web_sm” model.
spacyr::spacy_install()

spacyr::spacy_initialize()
```
```{r spacyR init, echo = FALSE, eval=FALSE}
py_venvs <- list.files("~/.virtualenvs", full.names = TRUE)
py_venvs <- py_venvs[grepl("spacy", py_venvs)]
spacyr::spacy_initialize(python_executable = paste0("~/.virtualenvs/", py_venvs, "/bin/python"))
```

The {networds} comes with this text sample. In this tutorial, we will use different parts of the text to a better visualization.

```{r sample_txt}
data(package = "networds") # list the available dataset in package networds
```
```{r sample_txt head}
# Text_sample of the package
# An example of text. Showing only the firsts lines
txt_wiki[1:5]

POS <- txt_wiki |> spacyr::spacy_parse()
# parsing the POS tagging in it
```

## Extracting entities

The package {spacyr} has two functions useful in this section.
Both of them conflate compound nouns, like "New" and "York" into "New_York".
The first one, extract only the entities

```{r spacyr::entity_extract}
POS |>
  spacyr::entity_extract() |>
  dplyr::as_tibble() # use tibble to better visualize
```

The second one, conflates the compound nouns and preserve the other POS tags.

```{r spacyr::entity_consolidate}
POS |>
  spacyr::entity_consolidate() |>
  dplyr::as_tibble() # use tibble to better visualize
```

This functions is used inside {networds}.
Lets use the package text example. 
<!-- You can use whatever you want. -->

```{r }
POS |> group_ppn()
```

With the function in {networds}, is possible to give the whole text as input, search for a term/query. The package will tokenize in sentences (or in paragraphs, if specified in parameters), perform the POS tagging, and extract the graph. Is possible to run the whole process or go step by step, to understand what is going on. First, we'll do the first option.

If the text worked is not great, is possible to extract the all the co-occurrences and then work, but it comes with costs. 
It can easily escalate a lot in size in many times the origial text size, take a lot of time and computational costs.
For example, 345Mb of text can become 15Gb of POS tagged text.
So, another approach is to build the co-occurrences more wisely, departing from specific words, then processing only the text that matters.

The function `filter_by_query()` tokenize the text by sentence by default (to use paragraph instead, use the parameter  `by_sentence = FALSE`).

```{r filter_by_query}
# tokenizing in sentences and filtering three lines that contains the word "police"
x <- txt_wiki[3:6] |> filter_by_query("Police")
x
class(x)
```

Is possible to return a vector instead of a list object

```{r filter_by_query unlist}
x <- txt_wiki[1:12] |> filter_by_query("Police", unlist = TRUE)
x
class(x)
```

The next step is the POS tagging using `parsePOS()`. 

```{r parsePOS}
txt_wiki[1:12] |>
  filter_by_query("Police", unlist = TRUE) |>
  parsePOS()
```

The next step is to get the graph using `get_cooc_entities()`

```{r wiki get_cooc_entities, eval=T, echo=T}
x <- txt_wiki[2:44] |>
  filter_by_query("Police") |>
  parsePOS()

x |> dplyr::as_tibble() # use tibble to better visualize

g <- get_cooc_entities(x)

g
```


Visualizing the graph. It can be done with the function `q_plot()`, the quicker plot, but also with less customization options.

```{r q_plot}
g |> q_plot()
```

To a better control over the features of the graph, `plot_pos_graph()` gives more options.
The size of dots shows the frequency of term. The thickness of edges shows how often is the co occurrence of nodes.
The text used is very small, so there is no huge differences visible.
We opted to maintain the words in the same size as matter of.


```{r plot_pos_graph}
graph_wiki <- txt_wiki[2:44] |>
  filter_by_query("Police") |>
  parsePOS() |>
  get_cooc_entities()

plot_pos_graph(graph_wiki) # TODO estava dando erro
```

This viz function is based on {ggraph}, that is based on {ggplot2}. So it is possible to customize it even more.

```{r plot_pos_graph + ggplot}
plot_pos_graph(graph_wiki,
  font_size = 1.3,
  edge_color = "tomato",
  point_color = "aquamarine4"
) +
  ggplot2::labs(
    title = "Wordnetwork of Nouns in a Wikipedia text",
    caption = "The size of dots shows the frequency of the term."
  )
```


Plotting an interactive graph
(the nodes can become in a crazy dance to find the best distance between themselves)
<!-- era graph_wiki$edges |> viz_graph() -->

```{r viz_graph police, eval=T}
graph_wiki$graphs |> interactive_graph()
```


```{r Brian, eval=T, echo=T}
graph <- txt_wiki[2:44] |>
  filter_by_query("Brian") |>
  parsePOS() |>
  get_cooc_entities()

plot_pos_graph(graph)
```


```{r Brian viz_graph, eval=T, echo=T}
graph$graphs |> interactive_graph()
```

To get the graph of entities and nouns:

```{r police -cardinal + viz_graph}
graph_ppn <- filter_by_query(txt_wiki[2:44], "Police") |>
  parsePOS(only_entities = FALSE) |>
  dplyr::filter(entity_type != "CARDINAL") |> # to clean the graph
  dplyr::mutate(token = gsub("Police", "police", token)) |> # normalize the term "police"
  get_cooc()

graph_ppn
```


```{r police -cardinal + viz_graph 2}
interactive_graph(graph_ppn$graphs)
```



```{r police -cardinal + plot_pos_graph}
graph_ppn |> plot_pos_graph()
```

## Sotu example

Using data from [{SOTU} package](https://cran.r-project.org/web/packages/sotu/index.html),  United States Presidential State of the Union Addresses.

```{r, eval=FALSE, echo=FALSE}
library(sotu)
# looking at availabe datasets in the package
data(package = "sotu")
sotu_text[237:240] |> filter_by_query("China")
filter_by_query(sotu_text[237:240], "\\bXi\\b", unlist = TRUE)
filter_by_query(sotu_text[237:240], "bob") # test

# Trump sentences from SOTU talking about "Xi"
filter_by_query(sotu_text[237:240], "\\Xi\\b") |> parsePOS()
# Trump sentences from SOTU talking about "China"
filter_by_query(sotu_text[237:240], "China")[2] |> parsePOS()
# Trump sentences from SOTU talking about "China"     d
filter_by_query(sotu_text[237:240], "China") |> parsePOS()
filter_by_query(sotu_text[237:240], "China") |> parsePOS(entities = FALSE)

filter_by_query(sotu_text[237:240], "China") |>
  parsePOS() |>
  get_pairs()

filter_by_query(sotu_text[237:240], "China") |>
  parsePOS() |>
  get_pairs(loop = TRUE)
```

## Using another languages

How {networds} uses spacy to tag the words, the user must initialize the model for the language.
By default, spacy loads the English model. If the model was previously used, to change the model, it is necessary to end the loaded model with `spacyr::spacy_finalize()` and then load the new model. 
For example, to load the Portuguese model, use with `spacyr::spacy_initialize(model = "pt_core_news_lg")`.
If you just started the package with `library(networds)`, you can simply run  `spacy_initialize(model = "pt_core_news_lg")` in the console.


To download other language model, check models available at [spacy.io/usage/models](https://spacy.io/usage/models)
Ex.: installing portuguese, the model available are:

```{r install spacy models PT, eval = FALSE}
modelsPT <- c("pt_core_news_sm", "pt_core_news_md", "pt_core_news_lg")

# installing the bigger model
spacyr::spacy_download_langmodel(modelsPT[3])
```

```{r, eval=FALSE, echo=FALSE}
# if you initialized spacyR with another model, first finalize it
spacyr::spacy_finalize()

# load the model of another language
spacy_initialize(model = "pt_core_news_lg")

"Maria Jana ama John Smith e Maria é amada por Joaquim de Souza" |>
  spacyr::spacy_parse(dependency = T) |>
  group_ppn()
```

