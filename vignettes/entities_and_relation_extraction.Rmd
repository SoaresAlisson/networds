---
title: "01 - Entity extraction with regex"
output: rmarkdown::html_vignette
knitr:
  opts_chunk:
    # cache: true
    cache: false 
vignette: >
  %\VignetteIndexEntry{entities_and_relation_extraction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE,
  # cache = FALSE,
  # dimensiongs for figure
  fig.width = 5,
  fig.height = 3,
  dpi = 150
)
library(txtnet)
```


##  Entity extraction and its relations

The package {txtnet} provides a set of tools to extract entities and relations from text. 
The first of this tools uses one of the simples form: by using the rule based approach. It contains a rule based extraction algorithm and a rule based relation extraction algorithm.
<!-- For more advanced NER, there is other packages such as  -->
<!-- [{crfsuite}](https://cran.r-project.org/web/packages/crfsuite/index.html), {[UDpipe](https://cran.r-project.org/web/packages/udpipe/)} and {[spacyR](https://spacyr.quanteda.io/)}. -->
Another similar package is [textnets](https://github.com/cbail/textnets), from Chris Bail. It capture the proper names using UDpipe, plot word networks and calculates the centrality/betweeness of words in the network.
To extract entities based on Part of speech tagging, see other functions of this package,

<!-- Carley 1993, MAP analysis -->

The functions of {txtnet} do not work so well like the NER, but we think that, in some situations, they can be better job than traditional joining unigram, bigrams, trigrams and so on.
Because it is a rule based approach, it is very simple to use, need less dependencies and runs also fast (or maybe less slower). It will also requires a lot of post-cleaning, but you have the absolute control over which words are extracted and what words are rejected.

In Natural Language Processing, to find proper names or terms that frequently appears together is called "collocation", e.g., to find "United Kingdom". You can learn more what is collocation and its statistical details and r function in this [article](https://sigil.r-forge.r-project.org/materials/04_collocation_analysis.pdf), and it is also possible to functions like [`quanteda.textstats::textstat_collocations()`](https://quanteda.io/reference/textstat_collocations.html), [`TextForecast::get_collocations()`](https://cran.r-project.org/web/packages/TextForecast/) to identify them, but it also will require a lot of data cleaning, specially if you want is proper names.

## How it works?  
The function captures all words that:   
- begins with uppercase,   
- followed by other uppercases, lowercase or numbers, without white space  
- it can contain symbols like `_`, `-` or `.`. In this way, words like "Covid-19" are captured.
- the user can specify a connector, like "of"or "of the" so, words like "United States of America"  are also captured

In languages such as English and Portuguese, it extracts proper names. In German, it also extracts nouns.
There is some trade-off, of course.
It will capture a lot of undesired words and will demand posterior cleaning, like:  
- It does not contain any sort of built-in classification.  
- "Obama Chief of Staff Rahm Emanuel" will be captured as one entity, what is not wrong et al, but maybe not what was expect.

## The downsides

it will not capture:  

- entities that begin with lowercase. 
<!-- To do that, take a look at other already mentioned packages, like spacyr and UDpipe. -->
- it does not delves with words ambiguity. For example, is "WHO" referring to question or World Health Organization? Washington is a person or a place?

To overthrow this problemas, txtnet has a set of other functions that works with Part of Speech tagging and Named Entity Recognition. 
If the problems enumerated are a big problem to your case, take a look at the next session "Extract entity co-ocurrences with POS".

## Conclusion 
In my experience, the approach of this section using regex works better to certain types of text than others. Very well formatted text, like Books, formal articles can be a good option that works well with this function. Text from social media, because it lacks formalities of the language and have a lot of typos, lacking uppercase sometimes, or written all in uppercase, this approach will not work so well.

## Using txtnet: extracting entities with regex

After installing the package (see Readme), load it:

```{r ll libs}
library(txtnet)
```

So, let's extract some proper names from a simple text:

```{r NER}
"John Does lives in New York in United States of America." |> extract_entity()
```

Or it is possible to use other languages, specifying the parameter `connectors` using the function `connectors(lang)`. Checking the connectors:

```{r connectors}
connectors("eng")
# or you can also use for english, to get the same result:
connectors("en")
# For portuguese
connectors("pt")
# to get the same result:
connectors("port")

# by default, the functions uses the parameter "misc". meaning "miscellaneous".
connectors("misc")
```

Using with other languages:
suppressWarnings(
```{r entities pt}
"João Ninguém mora em São José do Rio Preto. Ele esteve antes em Sergipe" |>
  extract_entity(connect = connectors("pt"))

vonNeumann_txt <- "John von Neumann (/vɒn ˈnɔɪmən/ von NOY-mən; Hungarian: Neumann János Lajos [ˈnɒjmɒn ˈjaːnoʃ ˈlɒjoʃ]; December 28, 1903 – February 8, 1957) was a Hungarian and American mathematician, physicist, computer scientist and engineer"
vonNeumann_txt |> extract_entity()
```




## Extracting a graph
It is possible to extract a graph from the extracted entities. First, happens the tokenization by sentence or paragraph.
Than, the entities are extracted using `extract_entity()`. Than a data frame with the co-occurrence of words in sentences or paragraph is build. 

```{r vonNeumann graph}
vonNeumann_txt |> extract_graph()
```

One of the parameters is `sw` that means "stopwords". It is possible to add a vector stopwords.

```{r vonNeumann graph stopwords}
my_sw <- c(stopwords::stopwords(
  language = "en",
  source = "snowball", simplify = TRUE
), "lol")

vonNeumann_txt |> extract_graph(sw = my_sw)
```

This process can take a while to run if the text/corpus is big. So, if you are interested only in some words, so first of all, filter the sentences/paragraphs with the desired words, and after that, extract the graph.
Seeing another example, extracting from a Wikipedia article:

```{r read_html, echo=FALSE, eval=T}
# pagina <- "https://en.wikipedia.org/wiki/GNU_General_Public_License" |> rvest::read_html()
# writeLines( as.character(pagina), "../inst/wiki_GNU.html" )
# page <- readLines("../inst/wiki_GNU.html")
# page <- rvest::read_html("inst/wiki_GNU.html")
page <- rvest::read_html("../inst/wiki_GNU.html")
```

```{r, echo=T, eval=FALSE}
page <- "https://en.wikipedia.org/wiki/GNU_General_Public_License" |> rvest::read_html()
```

```{r ex wikipedia scrape GNU, eval=TRUE}
text <- page |>
  rvest::html_nodes("p") |>
  rvest::html_text()

# looking at the scraped text:
text[1:2] # seeing the head of the text
```

And now extracting the graphs:

```{r ex wikipedia plot GNU}
g <- text |> extract_graph(sw = my_sw)
g
g_N <- g |> dplyr::count(n1, n2, sort = T)
g_N
```

To plot the wordcloud network it is necessary two parameters: the original text and the dataframe/tibble as returned by `dplyr::count()`, with three columns: node 1, node 2 and the weight/frequency. 

```{r ploting the graph}
net_wordcloud(text, g_N)
```

There are different information in the graph:

- The size of words and compound words means the individual frequency of each word/compound word 
- The thickness of the links indicates how often the pair occur together.

We opted to use this approach of the two parameters of data frame with weights, as well as with the original text because plotting such networks as a matter of readability, often requires to select only the most frequent.
A word individual frequency is not necessarily correlated with it's frequency in graphs, so the function calculates the individual frequency of the word.
Looking at the frequency dataframe, the user may want to strip some graphs, and then plot it.

This function uses {[ggraph](https://ggraph.data-imaginist.com/reference/)} and ggplot. So, you can change some ggplot or add another ones _a posteriori_.

To plot an interactive graph, it is possible to use {[networkD3](https://christophergandrud.github.io/networkD3/)}:

```{r networkD3}
g_N |>
  head(100) |> # to reduce the amount of nodes and edges in the graph
  networkD3::simpleNetwork(
    height = "10px", width = "30px",
    linkDistance = 50,
    fontSize = 16
  )
```
<!-- g_N |> interactive_graph() -->

Another text example.
<!-- "https://en.wikipedia.org/wiki/Julia_Silge" -->

```{r read_html hurricane, echo=TRUE, eval=FALSE}
page <- "https://en.wikipedia.org/wiki/Hurricane_Milton" |> rvest::read_html()
```

```{r read_html wiki_Hurricane_Milton,  echo=FALSE, eval=T}
# pagina <- "https://en.wikipedia.org/wiki/Hurricane_Milton" |> rvest::read_html()
# writeLines( as.character(pagina), "../inst/wiki_Hurricane_Milton.html" )
# page <- readLines("../inst/wiki_GNU.html")
page <- rvest::read_html("../inst/wiki_Hurricane_Milton.html")
```
```{r ex2 wiki Hurricane}
text <- page |>
  rvest::html_nodes("p") |>
  rvest::html_text()

text[1:2] # seeing the head of the tex
```

```{r ex2 wiki Hurricane2}
g <- text |> extract_graph(sw = my_sw)
# option 1: use counting the edge frequency
g_N <- g |> dplyr::count(n1, n2, sort = T)
# option 2: use count_graph function, same results
g_N <- g |> count_graph()

net_wordcloud(text, g_N, head_n = 50)
```

To plot an interactive graph, it is possible to use {networkD3}:

```{r networkD3 hurricane}
g_N |>
  head(100) |> # to reduce the amount of nodes and edges in the graph
  networkD3::simpleNetwork(
    height = "10px", width = "30px",
    linkDistance = 50,
    fontSize = 16
  )
```


## Using State of the Union data


Using the package [SOTU](https://github.com/taylor-arnold/sotu), that contains the [State of the Union Addresses](https://en.wikipedia.org/wiki/State_of_the_Union).
It :

    "is an annual message delivered by the president of the United States to a joint session of the United States Congress near the beginning of most calendar years on the current condition of the nation. The speech generally includes reports on the nation's budget, economy, news, agenda, progress, achievements and the president's priorities and legislative proposals." 


```{r sotu}
library(sotu) #  text examples of US presidents speeches

# checking the DF with the speeches
tibble::as_tibble(sotu_meta)
```

Checking Obama speech of the first year of his first mandate

```{r sotu obama}
# checking what are the speeches of Obama
sotu_meta |>
  dplyr::filter(
    grepl("Obama", president, ignore.case = T),
    grepl("2009", years_active)
  )

# Picking this speech of his first year
text_sotu <- sotu_text[229] |>
  paste(collapse = " ") # turning the vector into a single element
str(text_sotu) # first lines of the text

# As a matter of curiosity, checking the most frequent entities
text_sotu |>
  extract_entity(sw = my_sw) |>
  plyr::count() |>
  dplyr::arrange(-freq) |>
  head(30)

sotu_g_Ob <- text_sotu |>
  paste(collapse = " ") |>
  extract_graph(sw = my_sw)

count_graph_ob <- dplyr::count(sotu_g_Ob,
  n1, n2,
  sort = T
)

count_graph_ob
```

To plot we'll use another function, plot_graph2.
It works differently than net_wordcloud.
Because word frequencies can vary significantly, differences in text size can be substantial. Therefore, instead of adjusting text size, we vary the dot/node size, ensuring the text remains consistently sized and maintains readability.

```{r plot}
plot_graph2(
  sotu_g_Ob,
  count_graph_ob,
  head_n = 70,
  edge_color = "blue", edge_alpha = 0.1,
  text_size = 10,
  scale_graph = "log2"
) +
  ggplot2::labs(title = "Obama SOTU - First Year")
```

Checking Trump speech of the first year of his first mandate

```{r sotu Trump}
# Trump, first Mandate
sotu_meta |>
  dplyr::filter(grepl("Trump", president, ignore.case = T))

sotu_g_Tr <- sotu_text[237] |>
  paste(collapse = " ") |>
  extract_graph(sw = my_sw)

#  the most frequent entities
sotu_g_Tr |>
  extract_entity(sw = my_sw) |>
  plyr::count() |>
  dplyr::arrange(-freq) |>
  head(30)

plot_graph2(
  sotu_g_Tr,
  dplyr::count(sotu_g_Tr, n1, n2, sort = T),
  head_n = 70,
  edge_color = "red",
  edge_alpha = 0.3,
  scale_graph = "log2",
  text_size = 10,
) +
  ggplot2::labs(title = "Trump SOTU - First Year")
```

Now, comparing speeches on a certain topic

```{r sotu obama trump graph}
# a regex to capture some words/patterns
term <- "\\bChin|Beijing|\\bXi\\b|Jinping"
term_ <- "China"

# Get all Obama speeches of his first mandate
text_sotu_Ob <- sotu_text[229:234] |>
  filter_by_query(term)

sotu_g_Ob <- text_sotu_Ob |>
  paste(collapse = " ") |>
  extract_graph(sw = my_sw)

g_Ob <- plot_graph2(
  sotu_g_Ob,
  dplyr::count(sotu_g_Ob, n1, n2, sort = T),
  edge_color = "blue",
  edge_alpha = 0.1,
  text_size = 5,
  scale_graph = "log2"
) +
  # ggplot2::labs(title= paste("Obama about", term))
  ggplot2::labs(title = "Obama")


# Trump
text_sotu_Tr <- sotu_text[237:240] |>
  filter_by_query(term)

sotu_g_Tr <- text_sotu_Tr |>
  paste(collapse = " ") |>
  extract_graph(sw = my_sw)

g_Tr <- plot_graph2(
  sotu_g_Tr,
  dplyr::count(sotu_g_Tr, n1, n2, sort = T),
  edge_color = "red",
  edge_alpha = 0.2,
  text_size = 5,
  scale_graph = "log2"
) +
  # ggplot2::labs(title= paste("Trump about", term))
  ggplot2::labs(title = "Trump")

# Joining the graphs
library(patchwork)
(g_Ob + g_Tr) +
  plot_annotation(
    title =
      "Coocurrence of terms related to China"
  )
```

## Final remarks
As mentioned, this approach based on text patterns (regex) has its limitations. 
We advise to test different head_n, to increase the number of nodes, and if it is too polluted, decrease it.
It can helps to see what to look at the text to infer meaning.
Is possible to use grammar classification to get more precise results. See next session, "Extract entity co-ocurrences with POS".

