---
title: "02-Extract entity co-ocurrences with POS"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{entities_and_relation_extraction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
Loading the library

```{r setup library}
library(txtnet)
library(spacyr)
```
```{r browser, echo=FALSE}
options(browser="firefox")
```


Extracting proper names or entities with regex are cool, but has its limitations. 
In this section, we will use the Part of Speech (POS) tags to extract the entities, proper names, noun phrases and its co-occurrences to generate graphs.
In R, to tag words with POS, the main packages are {UDPipe} and {SpacyR}.

The {txtnet} comes with this text sample. In this tutorial, we will use different parts of the text to a better visualization.

```{r sample_txt}
data(package = "txtnet") # list the available dataset in package txtnet
```
```{r sample_txt head}
# Text_sample of the package
# An example of text. Showing only the firsts lines
txt_wiki[1:5]

# parsing the POS tagging in it
POS <- txt_wiki |> spacyr::spacy_parse()
```

## Extracting entities

The package {spacyr} has two functions useful in this section.
Both of them conflate compound nouns, like "New" and "York" into "New_York".
The first one, extract only the entities

```{r spacyr::entity_extract}
POS |> spacyr::entity_extract()
```

The second one, conflates the compound nouns and preserve the other POS tags.

```{r spacyr::entity_consolidate}
POS |> spacyr::entity_consolidate()
```

This functions is used inside {txtnet}.
Lets use the package text example. You can use whatever you want.


With the function in {txtnet}, is possible to give the whole text as input, search for a term/query. The package will tokenize in sentences (or in paragraphs, if specified in parameters), perform the POS tagging, and extract the graph. Is possible to run the whole process or go step by step, to understand what is going on. First, we'll do the first option.

If the text worked is not great, is possible to extract the all the co-occurrences and then work, but it comes with costs. 
It can easly escalate a lot in size in many times the origial text size, take a lot of time and computational costs.
For example, 345Mb of text can become 15Gb of POS tagged text.
So, another approach is to build the co-occurrences more wisely, departing from specific words, then processing only the text that matters.

The function `filter_by_query()` tokenize the text by sentence by default (to use paragraph instead, use the parameter  `by_sentence = FALSE`).

```{r filter_by_query}
# tokenizing in sentences and filtering three lines that contains the word "police"
x <- txt_wiki[3:6] |> filter_by_query("Police")
x
class(x)
```

Is possible to return a vector instead of a list object

```{r filter_by_query unlist}
x <- txt_wiki[1:12] |> filter_by_query("Police", unlist=TRUE)
x
class(x)
```

The next step is the POS tagging using `parsePOS()`. 

```{r parsePOS}
txt_wiki[1:12] |> 
  filter_by_query("Police", unlist=TRUE) |>
  parsePOS() 
```

The next step is to get the graph using `get_cooc_entities()`

```{r wiki get_cooc_entities, eval=T, echo=T}
x <- txt_wiki[2:44] |> 
  filter_by_query("Police") |>
  parsePOS()

x

g <- get_cooc_entities(x)

g
```


Visualizing the graph. It can be done with the function `q_plot()`, the quicker plot, but also with less customization options.

```{r q_plot}
g  |> q_plot()
```

To a better control over the features of the graph, `plot_pos_graph()` gives more options.
The size of dots shows the fequency of term. The thickness of edges shows how often is the oc-ocurrence of nodes.
The text used is very small, so there is no huge differences visible.
We opted to maintain the words in the same size as matter of.


```{r plot_pos_graph}
graph_wiki <- txt_wiki[2:44] |> 
  filter_by_query("Police") |> 
  parsePOS() |>
  get_cooc_entities()  
  
plot_pos_graph(graph_wiki)
```

This viz function is based on {ggraph}, that is based on {ggplot2}. So it is possible to customize it even more.

```{r plot_pos_graph + ggplot}
plot_pos_graph(graph_wiki, 
               font_size=1.3,
               edge_color = "tomato", 
               point_color = "aquamarine4") +
  ggplot2::labs(title = "Wordnetwork of Nouns in a Wikipedia text",
                caption = "The size of dots shows the frequency of the term.")
```


Ploting an interactive graph
(the nodes can become in a crazy dance to find the best distance between themselves)

```{r viz_graph police, eval=T}
graph_wiki$edges |> viz_graph()
```




```{r Brian, eval=T, echo=T}
graph <- txt_wiki[2:44] |> 
  filter_by_query("Brian") |> 
  parsePOS() |>
  get_cooc_entities() 

plot_pos_graph(graph)
```


```{r Brian viz_graph, eval=T, echo=T}
graph$edges |> viz_graph()
```

To get the graph of entities and nouns:

```{r police -cardinal + viz_graph}
graph_ppn <- filter_by_query(txt_wiki[2:44], "Police") |> 
  parsePOS(only_entities=FALSE)  |> 
  dplyr::filter(entity_type != "CARDINAL") |> # to clean the graph
  dplyr::mutate(token = gsub("Police", "police", token)) |> # normalize the term "police"
  get_cooc()

graph_ppn
```


```{r police -cardinal + viz_graph}
viz_graph(graph_ppn$edges)
```



```{r police -cardinal + plot_pos_graph}
graph_ppn |> plot_pos_graph()
```

## Sotu example

Using data from [{SOTU} package](https://cran.r-project.org/web/packages/sotu/index.html),  United States Presidential State of the Union Addresses.

```{r, eval=FALSE, echo=FALSE}
library(sotu)
# looking at availabe datasets in the package
data(package = "sotu")
sotu_text[237:240] |> filter_by_query("China")
filter_by_query(sotu_text[237:240], "\\bXi\\b", unlist = TRUE)
filter_by_query(sotu_text[237:240], "bob") # test

# Trump sentences from SOTU talking about "Xi"
filter_by_query(sotu_text[237:240], "\\Xi\\b") |> parsePOS()               
# Trump sentences from SOTU talking about "China"                          
filter_by_query(sotu_text[237:240], "China")[2] |> parsePOS()              
# Trump sentences from SOTU talking about "China"     d                    
filter_by_query(sotu_text[237:240], "China") |> parsePOS()                 
filter_by_query(sotu_text[237:240], "China") |> parsePOS(entities = FALSE) 

filter_by_query(sotu_text[237:240], "China") |> 
  parsePOS() |>                                 
  get_pairs()

filter_by_query(sotu_text[237:240], "China") |> 
  parsePOS() |>                                 
  get_pairs(loop = TRUE)                        

```

## Using another languages

How {txtnet} uses spacy to tag the words, the user must initialize the model for the language.
By default, spacy loads the English model. If the model was previously used, to change the model, it is necessary to end the loaded model with `spacyr::spacy_finalize()` and then load the new model. 
For example, to load the Portuguese model, use with `spacyr::spacy_initialize(model = "pt_core_news_lg")`.
If you just started the package with `library(txtnet)`, you can simply run  `spacy_initialize(model = "pt_core_news_lg")` in the console.


